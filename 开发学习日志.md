# 12-14
# 2 Byte-Pair Encoding (BPE) tokenizer
## 2.1 The Unicode Standard

å’Œpdfé‡Œå†™çš„ä¸€æ · unicode defineäº†15000+çš„character   
s æ˜¯code point 115
```python
>>> z = ord('s')
115

>>> chr(115)
's'
```
æ­¤å¤„å¯è§ord() : 
  convert ä¸€ä¸ª single Unicode character ä¸€ä¸ªunicodeçš„å­—ç¬¦
  å˜æˆ its integer representation ä»–çš„æ•´æ•°è¡¨ç¤º 
  è¿™ä¸ªinteger æŒ‡çš„å°±æ˜¯ Unicode code point
chr():
  æŠŠä¸€ä¸ª æ•´æ•°è¡¨è¾¾
  (è¿™ä¸ªæ•´æ•°å¿…é¡»æ˜¯ä¸€ä¸ª valid Unicode code point) 
  è½¬æ¢æˆå¯¹åº”çš„ single Unicode character
  (è¿”å›å€¼åœ¨ Python é‡Œæ˜¯ä¸€ä¸ª é•¿åº¦ä¸º 1 çš„å­—ç¬¦ä¸²)

print(chr(0))
- è¾“å‡ºçš„æ˜¯ å­—ç¬¦æœ¬èº«
- è¯¥å­—ç¬¦æ˜¯ NULLï¼ˆä¸å¯æ‰“å°ï¼‰
- æ‰€ä»¥ç»ˆç«¯ çœ‹èµ·æ¥ä»€ä¹ˆéƒ½æ²¡æœ‰

repr(chr(0))
- è¾“å‡ºçš„æ˜¯ escaped representation
- ä½¿ç”¨çš„æ˜¯ åå…­è¿›åˆ¶å½¢å¼ \x00
- è¿™æ˜¯ä¸ºäº† è®©äººç±»å’Œè°ƒè¯•å™¨å¯è¯»

NULLæœ¬èº«è¿æ‰“å° éƒ½ä¸å ä½ç½®
```python
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```
## 2.2 Unicode Encodings

å› ä¸ºç›´æ¥ç”¨unicodeä¸ç°å®
150k çš„item å¾ˆå¤šéƒ½ç”¨ä¸åˆ°
å¾ˆå¤šå­—ç¬¦åŠå…¶ç½•è§
vocab ä¼šå¾ˆç¨€ç–

æˆ‘ä»¬ç›´æ¥ç”¨unicode encoding UTF-8
å› ä¸º
ä¸ç”¨ç»†çœ‹ ä½†æ˜¯å¤§æ¦‚æ˜¯è¿™æ ·åšçš„ 


[[../../ç†è§£/utf-8.md]]
UTF-8 çš„ç›®çš„ä¸æ˜¯å‹ç¼©å­—ç¬¦æ•°é‡ï¼Œè€Œæ˜¯æŠŠä»»æ„ Unicode å­—ç¬¦æ˜ å°„æˆç”±æœ‰é™çš„ byteï¼ˆ0â€“255ï¼‰ç»„æˆçš„åºåˆ—ï¼Œä»è€Œç”¨å¯å˜é•¿åº¦ç¼–ç è¡¨ç¤ºæ‰€æœ‰å­—ç¬¦ï¼Œå¹¶ä¿è¯å‘åå’Œå‘å‰å…¼å®¹ã€‚


### Problem(unicode2):


(b)
The function incorrectly decodes each byte separately, but UTF-8 characters may span multiple bytes, causing multi-byte characters such as "ã“" to be decoded incorrectly.

## 2.4 BPE Tokenizer Training
(æ€»ç»“ç‰ˆ ç°åœ¨ä¸è§£é‡Š)
raw text 
 â†’ special token handling
 â†’ pre-tokenizer (regex)
 â†’ UTF-8 bytes
 â†’ BPE merges
 â†’ token IDs

### Pre-tokenization 

Pre-tokenizer çš„ä½œç”¨ä¸æ˜¯â€œå†³å®šæœ€ç»ˆ tokenâ€ï¼Œè€Œæ˜¯æŠŠæ–‡æœ¬åˆ‡æˆâ€œåˆç†çš„å°å—â€ï¼Œè®© BPE åœ¨è¿™äº›å—å†…éƒ¨ç»Ÿè®¡å’Œ mergeã€‚
```python
>>> # requires `regex` package
>>> import regex as re
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```
å¯ä»¥çœ‹å‡º pre-rtokenizationæ˜¯å…ˆå°†å¥å­å˜æˆä¸€ä¸ªä¸ªè¯å’Œç¬¦å·å’Œ


è¸©çš„å‘ æ­£åˆ™è¡¨è¾¾å¼ regex å†™äº†åˆ†æ®µ
åªèƒ½ä¸€è¡Œ
å…·ä½“çœ‹ ./ç†è§£/pre-tokenizer-regex

### ä½¿ç”¨re.finditer è€Œä¸æ˜¯ re.findall
```python
>>> re.findall(PAT, "some text that i'll pre-tokenize")
>>>['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']

for m in re.finditer(PAT2, english_words, flags=re.VERBOSE):
    print(m.group(0))

The original sentence using PAT2 'some text that i'll pre-tokenize'
some
 text
 that
 i
\'ll
 pre
-
tokenize
```
#### re.findall
å¯è§ re.finall(pattern,text)
ä¼šç”Ÿæˆä¸€ä¸ªlist

#### re.finditer
ä½†æ˜¯re.finditer(pattern,text,flag = re.VERBOSE) æ˜¯ä¸€ä¸ªscanner
å¾—ç”¨ for m in ä¸Šè¿°è¿™ä¸ª
ç„¶åæ¯ä¸ªscançš„ éƒ½å¾—ç”¨ m.group(0)

#### åˆ›å»ºå­—å…¸
```python
counts = {}
```
ç›´æ¥ç”Ÿæˆå­—å…¸
å¦‚æœä½¿ç”¨ç›´æ¥æŒ‰ç…§whitespaceåˆ†çš„è¯
æ˜¯text_list = text.split(" ")
æ³¨æ„ç©ºæ ¼ ä»¥åŠè¿”å›çš„æ˜¯list

### 2.4A python é‡Œæ€ä¹ˆåˆ›å»ºå’ŒæŸ¥çœ‹`bytes`
1. é¦–å…ˆbæ˜¯ä¸€ä¸ªbytes å¯¹è±¡ 
  b = b'low'
  bytes æ˜¯ Python çš„ä¸€ä¸ªå†…å»ºç±»å‹

2. å…¶æ¬¡ bytesæ˜¯ä¸€ä¸ª ä¸å¯å˜çš„å­—èŠ‚åºåˆ— 
  æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 0~255 çš„ intï¼ˆ8-bitï¼‰
  
  å› ä¸ºæ˜¯å­—èŠ‚åºåˆ—ï¼ˆsequenceï¼‰
  æ‰€ä»¥å¯ä»¥éå†
  éå†æ—¶è¿”å›çš„ä¹Ÿæ˜¯ int
```python
listï¼ˆbï¼‰
#[108,111,119]  æ¯ä¸ª int æ˜¯è¯¥å­—èŠ‚çš„â€œæ•°å€¼â€ âœ” å–å€¼èŒƒå›´åˆšå¥½æ˜¯ 0~255
    # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 8-bit çš„å­—èŠ‚
    # Python ç”¨ 0~255 çš„ int æ¥è¡¨ç¤ºè¿™ä¸ªå­—èŠ‚çš„å€¼
```

3. bytes(intå€¼) æ˜¯åˆ›å»ºä¸€ä¸ª é•¿åº¦ä¸º n çš„ bytes å¯¹è±¡
    æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯ 0ï¼ˆ\x00ï¼‰

4. ä½†æ˜¯å¦‚æœæ˜¯ bytes([n]) 
    åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ bytes å¯¹è±¡
    ğŸ‘‰ è¿™ä¸ªå­—èŠ‚çš„å€¼æ˜¯ n
    âœ” [n] æ˜¯ä¸€ä¸ª iterable
    âœ” è¡¨ç¤ºâ€œæˆ‘è¦ä¸€ä¸ªå€¼ä¸º n çš„å­—èŠ‚â€

5.ç´¢å¼•è¿”å› int åˆ‡ç‰‡è¿”å› bytes


##### 1 `b"..."` literal
```python
b"abc"          # åªèƒ½ç›´æ¥å†™ ASCII å­—ç¬¦
b"\xe4\xb8\xad" # å¯ä»¥ç”¨åå…­è¿›åˆ¶è½¬ä¹‰å†™ä»»æ„å­—èŠ‚
```

##### 2 `str.encode(...)`ï¼šä»æ–‡æœ¬åˆ°å­—èŠ‚
```python
s = "ä¸­æ–‡"
bs = s.encode("utf-8")
# bs æ˜¯ bytes
```
 
##### 3 `bytes.decode(...)` ä»å­—èŠ‚åˆ°æ–‡æœ¬
```python
bs = b"\xe4\xb8\xad\xe6\x96\x87"
s = bs.decode("utf-8")
```


##### 4 `bytes([x,y,z])` ç”¨æ•´æ•°åˆ—è¡¨æ„é€ 
```python
bytes([65, 66, 67])  # b'ABC'
```

##### 5 bytes çš„â€œå…ƒç´ â€æ˜¯ä»€ä¹ˆ
```python
b = b"ABC"
b[0]      # 65ï¼ˆintï¼‰
b[0:2]    # b'AB'ï¼ˆbytesï¼‰
list(b)   # [65, 66, 67]
```



#### 2.4B è¯»å–æ–‡ä»¶
https://chatgpt.com/g/g-p-693f75d2365c8191baf9aaa7038e3595-cs336xiao-xi-jie/c/6948a752-5e40-832e-bb7f-f6299b8b04be
##### 1. open() ä¸ä¸Šä¸‹æ–‡ç®¡ç†å™¨ with

ä¸€å®šä¼˜å…ˆç”¨ with open(...) as f:ï¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€å¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®é‡Šæ”¾èµ„æºã€‚
ä¸ç”¨ with å°±è¦æ‰‹åŠ¨ f.close()ï¼Œå¾ˆå®¹æ˜“å¿˜ã€‚

```python
from pathlib import Path
path = Path("data.txt")
with path.open("r", encoding="utf-8") as f:
    text = f.read()
```

1ï¸âƒ£ ä½ è¦è§£å†³çš„å®é™…é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ BPE è®­ç»ƒå‰ï¼Œè¦æ±‚ï¼š
special tokensï¼ˆå¦‚ <|endoftext|>ï¼‰ä¸èƒ½å‚ä¸ pre-tokenization

ä¸èƒ½è·¨ special token åš merge
æ‰€ä»¥æµç¨‹æ˜¯ï¼š

å…ˆæŒ‰ special tokens åˆ‡æ–‡æœ¬ â†’ æ¯ä¸€æ®µå•ç‹¬åš regex pre-tokenization

1. ä¸ç”¨æ­£åˆ™ï¼ˆå¯¹æ¯”ç”¨ï¼‰
  ```python
    text = "A<X>B<X>C"
    print(text.split("<X>"))
  ```

2.  å¿…é¡»ã€Œè¿æ¥ä¸¤ä¸ªå¯åŒ¹é…çš„ä¸œè¥¿ã€

    æ­£åˆ™é‡Œçš„åŸºæœ¬å½¢å¼æ˜¯ï¼š
    A | B
    æ„æ€æ˜¯ï¼š
    åŒ¹é… A æˆ– B
    æ‰€ä»¥ä½ å¿…é¡»ç»™å®ƒå·¦æ“ä½œæ•°å’Œå³æ“ä½œæ•°ï¼š
    <X> | <Y>

3. join åšçš„äº‹ï¼ˆéå¸¸ç²¾ç¡®ï¼‰
    "|".join(["<X>", "<Y>", "<Z>"])
    ç»“æœï¼š
    "<X>|<Y>|<Z>"
    join éœ€è¦ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ æ‰€ä»¥generator expressionå’Œ list comprehensionéƒ½è¡Œ
```python
"|".join(re.escape(tok) for tok in special_tokens)
"|".join([re.escape(tok) for tok in special_tokens])
```
```python
(re.escape(tok) for tok in special_tokens)
```
å®ƒäº§ç”Ÿçš„æ˜¯ï¼š
ä¸€ä¸ª æƒ°æ€§ iterable
æ¯æ¬¡ join éœ€è¦ä¸‹ä¸€ä¸ªå…ƒç´ æ—¶æ‰è®¡ç®—

4. re.escape
    æ˜¯æŠŠç‰¹æ®Šçš„ç¬¦å· ä¾‹å¦‚ 
    special token é€šå¸¸é•¿è¿™æ ·ï¼š
    <|endoftext|>
    ä½†åœ¨æ­£åˆ™é‡Œï¼š
    |
    <
    >
    éƒ½æœ‰ç‰¹æ®Šå«ä¹‰
    ğŸ‘‰ å¦‚æœä½ ç›´æ¥ç”¨ï¼Œä¼šè¢« regex è¯¯è§£
    âŒ é”™è¯¯ç¤ºä¾‹
    ```python
    pattern = "|".join(["<|endoftext|>"])
    ```

### 2.4C sum
`sum(iterable, start=0) `
ç­‰ä»·äºï¼š

```python
total = start
for x in iterable:
    total = total + x
return total
```

1) æ±‚æ•°å­—å’Œï¼ˆæœ€å¸¸è§ï¼‰

```python
nums = [1, 2, 3]
total = sum(nums)          # 6
total2 = sum(nums, 10)     # 16  (ä» 10 å¼€å§‹åŠ )
```

2) å¯¹ç”Ÿæˆå™¨æ±‚å’Œï¼ˆçœå†…å­˜ï¼‰
```python
total = sum(i*i for i in range(10))
```

3) åˆå¹¶â€œæ”¯æŒ + çš„å¯¹è±¡â€ï¼Œç”¨ start æŒ‡å®šåˆå§‹å€¼
```python
from collections import Counter
counters = [Counter("ab"), Counter("bc")]
merged = sum(counters, Counter())
```

#####

### 2.4D Counter 
1) ç›´æ¥ç»Ÿè®¡é¢‘æ¬¡
```python
from collections import Counter
cnt = Counter("banana")
# Counter({'a': 3, 'n': 2, 'b': 1})
```

2) é€æ­¥ç´¯åŠ ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
cnt = Counter()
for tok in tokens:
    cnt[tok] += 1
```

3) æ‰¹é‡æ›´æ–°
```python
cnt.update(tokens)              # tokens æ˜¯ iterable
cnt.update({"a": 2, "b": 1})    # ä¹Ÿå¯ä»¥æ˜¯ mapping
```

4) top-k é«˜é¢‘
```python
cnt.most_common(5)
```

5) åˆå¹¶/åŠ å‡ï¼ˆè®¡æ•°è¯­ä¹‰ï¼‰
```python
c1 = Counter(a=2, b=1)
c2 = Counter(a=1, b=5, c=1)

c1 + c2   # è®¡æ•°ç›¸åŠ 
c1 - c2   # è®¡æ•°ç›¸å‡ï¼ˆä¼šè¿‡æ»¤ <=0ï¼‰
c1 & c2   # æ¯ä¸ªé”®å– min
c1 | c2   # æ¯ä¸ªé”®å– max
```
6) ä¸å­˜åœ¨çš„é”®é»˜è®¤æ˜¯ 0
```python
cnt = Counter()
cnt["missing"]    # 0
```

### 2.5E Counter vs dictï¼šå¸¸è§ç”¨æ³•å¯¹æ¯”ï¼ˆä½ å†™ä½œä¸šæœ€å¸¸ç¢°åˆ°ï¼‰
A) è®¡æ•°ï¼ˆtoken -> æ¬¡æ•°ï¼‰

âœ… ç”¨ Counter

```python
cnt = Counter()
cnt[tok] += 1
```

ç”¨ dict ä¹Ÿèƒ½åšï¼Œä½†æ›´å•°å—¦ï¼š
```python
d = {}
d[tok] = d.get(tok, 0) + 1
```
B) æ˜ å°„/ç´¢å¼•ï¼ˆtoken -> id, id -> tokenï¼‰

âœ… ç”¨ dictï¼ˆå³ä½¿ value æ˜¯æ•°å­—ä¹Ÿä¸€æ ·ï¼‰

```python
token2id = {"<pad>": 0, "<unk>": 1}
id2token = {v: k for k, v in token2id.items()}
```

C) åˆå¹¶å¤šä¸ªç»Ÿè®¡ç»“æœ

âœ… Counter.update / +=ï¼ˆæ¨èï¼‰

```python
total = Counter()
for c in sub_counters:
    total.update(c)     # æˆ– total += c
```

ä¹Ÿå¯ä»¥ç”¨ sum(sub_counters, Counter())ï¼ˆèƒ½ç”¨ä½†é€šå¸¸æ…¢ç‚¹ï¼‰

D) â€œä¸å­˜åœ¨â€è¦ä¸è¦ç®— 0

è¦å½“ 0ï¼šCounter æ›´è‡ªç„¶

è¦ä¸¥æ ¼åŒºåˆ†ä¸å­˜åœ¨ vs å€¼ä¸º 0ï¼šdict æ›´æ˜ç¡®



### 2.4E å¹¶è¡Œ multiprocessing

#### map + sum  

```python
with Pool(num_processes) as pool:
sub_counters = pool.map(parallel_worker, worker_parameters)
```

ä¸¤ä¸ªç»„æˆå…ƒç´ 
ä¸€ä¸ªæ˜¯worker function
    æ‹¿åˆ°çš„æ˜¯y ä½œä¸ºiterableä¸­çš„ä¸€ä»½ä½œä¸ºå‚æ•°
    

ä¸€ä¸ªæ˜¯å¹¶è¡Œçš„caller
    pool.map()
      ä¼šæŠŠpool.map(x,y)
      yä¸­æ¯ä¸€ä¸ªå…ƒç´  
      ä½œä¸ºå®å‚ä¼ ç»™x 
      å³taskï¼ˆpiece of yï¼‰
      
```python
def task1(x:str)->list[str]:
    # print(x) # æ‹¿åˆ°çš„æ˜¯ string
    token_list = x.split(" ")
    # print(token_list) # è¿”å›çš„æ˜¯list
    return token_list


if __name__ == "__main__":

    with Pool(4) as pool:
        results = pool.map(task1, list_text)# è¿™ä¸ªresultsæ˜¯ä¸€ä¸ªlist

    bytes_counts = sum(sub_counters, Counter()) # è¿™é‡Œå¯è§ sum æŠŠè¿”å›çš„è¿™ä¸ªiterable ç»™ä¸€æ¬¡æ€§æ‰«æå½’å¹¶
```
##### åœ¨åšä»€ä¹ˆ
map ä¼š ç­‰æ‰€æœ‰ worker éƒ½å®Œæˆ
æ‰€æœ‰ Counter ä¸€æ¬¡æ€§è¿”å›æˆä¸€ä¸ª list
æœ€åç”¨ sum æŠŠå®ƒä»¬åˆå¹¶

##### ä¼˜ç‚¹
âœ… ä»£ç æç®€
å¾ˆâ€œå‡½æ•°å¼â€ï¼Œä¸€çœ¼å°±æ‡‚
âœ… åˆå¹¶é€»è¾‘å°‘
sum å†…éƒ¨æ˜¯çº¿æ€§ mergeï¼Œæ¬¡æ•° = len(sub_counters)

##### è‡´å‘½ç¼ºç‚¹ï¼ˆåœ¨ BPE é‡Œéå¸¸æ˜æ˜¾ï¼‰
âŒå†…å­˜å³°å€¼çˆ†ç‚¸
    åŒæ—¶å­˜åœ¨ï¼š
    æ‰€æœ‰ worker çš„ Counter
    æœ€ç»ˆåˆå¹¶ç”¨çš„ Counter
    åœ¨ TinyStories / OWT ä¸Šï¼š
    â— è¿™æ˜¯æœ€å®¹æ˜“ OOM / swap / å¡æ­» çš„å†™æ³•

âŒ æ²¡æœ‰æµå¼åé¦ˆ
    tqdm åªèƒ½åŒ… mapï¼Œä½†ç»“æœæ²¡å›æ¥ä¹‹å‰ä½ å•¥ä¹Ÿå¹²ä¸äº†
    çœ‹èµ·æ¥åƒâ€œå¡ä½äº†â€

#### imap_unordered + å¢é‡ç´¯åŠ ï¼ˆæµå¼ï¼‰
from multiprocessing import Pool

```python

bytes_counts = Counter()
with Pool(num_processes) as pool:
    for sub in pool.imap_unordered(
        parallel_worker,
        worker_parameters,
        chunksize=1,
    ):
        bytes_counts += sub
'''
chunk å¾ˆé‡ â†’ chunksize å°
chunk å¾ˆè½» â†’ chunksize å¤§
'''

```
##### åœ¨åšä»€ä¹ˆ
parallel_worker ä¸€ä¸ªä»»åŠ¡ â†’ ä¸€ä¸ª Counter
worker ç®—å®Œå°±ç«‹åˆ»è¿”å›
ä¸»è¿›ç¨‹ è¾¹æ”¶åˆ°ã€è¾¹ç´¯åŠ  åˆ° bytes_counts
é¡ºåº ä¸ä¿è¯ï¼ˆunorderedï¼‰

##### ä¼˜ç‚¹
âœ… å†…å­˜å ç”¨ä½ï¼ˆå…³é”®ï¼‰
    ä¸»è¿›ç¨‹é‡Œæ°¸è¿œåªå¤šä¸€ä¸ª sub Counter
    éå¸¸é€‚åˆ TinyStories / OWT è¿™ç§ Counter æå¤§ çš„åœºæ™¯
âœ… å»¶è¿Ÿä½
    ç¬¬ä¸€ä¸ª worker ä¸€ç®—å®Œä½ å°±å¼€å§‹ç´¯åŠ 
    tqdm èƒ½å®æ—¶åŠ¨ï¼Œæ–¹ä¾¿åˆ¤æ–­â€œæ˜¯ä¸æ˜¯å¡æ­»äº†â€
âœ… æ›´ç¨³
    å¦‚æœæŸä¸ª chunk ç‰¹åˆ«å¤§ï¼Œä¸ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ç»“æœå †åœ¨å†…å­˜é‡Œ

##### ç¼ºç‚¹
âŒ Python å±‚ += æ¬¡æ•°å¤š
    æ¯ä¸ª sub éƒ½è¦åšä¸€æ¬¡ Counter merge
    å¦‚æœ worker æ•°é‡ Ã— chunk æ•°é‡ç‰¹åˆ«å¤§ï¼Œä¼šæœ‰ä¸€äº› Python å¼€é”€


### Compute BPE merges è®¡ç®—BPE merges

BPE ç®—æ³• 
1. iteratively æ•°æ¯ä¸€å¯¹bytes å¹¶ä¸” è¯†åˆ«æœ€é«˜çš„ä¸€å¯¹
2. ç»Ÿè®¡å‡ºç°æœ€å¤šçš„ä¸€å¯¹ 
3. åˆå¹¶ ä» 'a' 'b' å˜æˆab
4. æŠŠåˆå¹¶çš„åŠ å…¥vocabulary 
5. æœ€åçš„vocabularyä¼šæ˜¯256ä¸ª åˆå§‹çš„ åŠ ä¸Š bpeåœ¨è®­ç»ƒä¸­èåˆçš„
6. ä¸è€ƒè™‘è·¨pre-tokençš„è¾¹ç•Œ 
7. å¦‚æœå‡ºç°tieçš„æƒ…å†µ ä½¿ç”¨lexicographically larger
 ï¼ˆä»€ä¹ˆæ˜¯lexicographically larger å¾…è¡¥å®Œ ï¼‰
```python
max([(b'l', b'o'), (b'o', b'w')]) == (b'o', b'w')
```

#### ä½¿ç”¨tqdm
##### 1ï¸âƒ£ åŒ…è£¹ä»»ä½•å¯è¿­ä»£å¯¹è±¡

```python
from tqdm import tqdm
for x in tqdm(data):
    ...
```

ğŸ‘‰ åŸåˆ™ï¼šå‡¡æ˜¯â€œäººä¼šç­‰â€çš„å¾ªç¯ï¼Œå°±è¯¥æœ‰ tqdm
    ï¼ˆtokenize / merge / train / scan file / pool.imapï¼‰

##### 2ï¸âƒ£ æ˜ç¡® totalï¼ˆéå¸¸é‡è¦ï¼‰
```python
for sub in tqdm(pool.imap_unordered(fn, items), total=len(items)):
```

å¦‚æœæ²¡æœ‰ totalï¼š
    ETA ä¸å‡†
    ç™¾åˆ†æ¯”ä¸æ˜¾ç¤º
    çœ‹èµ·æ¥åƒâ€œå¡ä½äº†â€ï¼ˆä½ é‡åˆ°è¿‡ï¼‰

##### 3ï¸âƒ£ ç»™ tqdm èµ·ä¸€ä¸ªâ€œè§£é‡Šå‹åå­—â€

```python
tqdm(..., desc="Pretokenize chunks")
tqdm(..., desc="BPE merges", unit="merge")
tqdm(..., desc="Counting byte pairs")
```

âŒ ä¸è¦ï¼š
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024

âœ… è¦ï¼š
    Pretokenize chunks:  35%|â–ˆâ–ˆâ–ˆâ–Œ | 350/1000
    desc = ä½ åœ¨ debug æ—¶çš„â€œå¿ƒç†å®‰å…¨æ„Ÿâ€

4ï¸âƒ£ unit æ˜¯ underrated çš„ç¥å™¨
```python
tqdm(..., unit="chunk")
tqdm(..., unit="merge")
tqdm(..., unit="MB")
```

æ¯”å¦‚ BPE mergeï¼š
```python
for _ in trange(num_merges, desc="BPE merges", unit="merge"):
```
ä¼šæ˜¾ç¤ºï¼š
    BPE merges:  1234/10000 merge




##### 4ï¸âƒ£ unit æ˜¯ underrated çš„ç¥å™¨
tqdm(..., unit="chunk")
tqdm(..., unit="merge")
tqdm(..., unit="MB")


æ¯”å¦‚ BPE mergeï¼š

for _ in trange(num_merges, desc="BPE merges", unit="merge"):
    ...


ä¼šæ˜¾ç¤ºï¼š

BPE merges:  1234/10000 merge

##### 5ï¸âƒ£ imap / imap_unordered + tqdmï¼ˆä½ æ­£åœ¨ç”¨çš„ï¼‰

```python
with Pool(num_processes) as pool:
    for sub in tqdm(
        pool.imap_unordered(worker, params, chunksize=1),
        total=len(params),
        desc="Pretokenize chunks",
    ):
        bytes_counts += sub
```


âœ… è¿™æ˜¯å¤šè¿›ç¨‹ + tqdm çš„æœ€ä¼˜è§£ä¹‹ä¸€
    ä¸ºä»€ä¹ˆä¸ç”¨ mapï¼Ÿ
    map ä¼š ç­‰æ‰€æœ‰ worker å®Œæˆæ‰è¿”å›
    tqdm å®Œå…¨æ²¡æ³•æ›´æ–°

#### Map reduce

âœ”ï¸ ç†è®ºä¸Šå†…å­˜å¤Ÿ
âœ”ï¸ ä½†å› ä¸ºåå¤äº§ç”Ÿå³°å€¼
âœ”ï¸ Linux è§¦å‘ swap
âœ”ï¸ swap çš„æ˜¯â€œè¿˜åœ¨ç”¨çš„ Python å¯¹è±¡â€
âœ”ï¸ å¯¼è‡´ page fault + swap å¾ªç¯
âœ”ï¸ ç¨‹åºçœ‹ä¼¼æ²¡æ­»ï¼Œå®åˆ™è¢« IO åæ‰
##### Map éƒ¨åˆ†çš„å°ç»†èŠ‚

1. åˆ›å»ºtmp_dir
##### ä¸€ã€pathlib.Pathï¼šä¸æ˜¯â€œæ›´å¥½çœ‹â€ï¼Œè€Œæ˜¯æ›´å°‘ bug
1ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç”¨ str + os.path äº†ï¼Ÿ
    ä¼ ç»Ÿå†™æ³•çš„é—®é¢˜ä½ è‚¯å®šå·²ç»è¸©è¿‡ï¼š
```python
path = "/tmp" + "/" + "part_0001.pkl"   # å®¹æ˜“é”™
path = os.path.join("/tmp", "part_0001.pkl")  # å¯è¯»æ€§å·®
```


Path çš„æ ¸å¿ƒä»·å€¼ä¸æ˜¯ç®€æ´ï¼Œæ˜¯:  

ğŸ‘‰ æŠŠâ€œè·¯å¾„â€å½“æˆä¸€ä¸ªå¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²

2ï¸âƒ£ Path çš„ä¸‰ä¸ªæœ€å¸¸ç”¨åŠ¨ä½œï¼ˆ90% åœºæ™¯:  
âœ… åˆ›å»º / è¡¨ç¤ºè·¯å¾„ï¼ˆä¸è®¿é—®ç£ç›˜ï¼‰
```python
from pathlib import Path
p = Path("/tmp/bpe")
```

âš ï¸ æ³¨æ„ï¼š 
è¿™ä¸€æ­¥ä¸ä¼šåˆ›å»ºç›®å½•  
åªæ˜¯ä¸€ä¸ªâ€œè·¯å¾„æè¿°å¯¹è±¡â€  

âœ… è·¯å¾„æ‹¼æ¥ï¼ˆæœ€é‡è¦ï¼‰
```python
p = Path("/tmp/bpe")
file = p / "part_0001.pkl"
```
ä½ ç°åœ¨ç”¨çš„æ­£æ˜¯è¿™ä¸ªï¼š  
path = tmp_dir / f"part_{i:04d}.pkl"


è¿™ä¸€æ­¥çš„éšå«å¥½å¤„ï¼š

1. è‡ªåŠ¨å¤„ç† /
2. Windows / Linux é€šç”¨
3. ä¸ä¼šå‡ºç° // æˆ–æ¼ /

âœ… ä¸è€ API äº¤äº’
```python
with open(path, "wb") as f:
open()
pickle.dump()
torch.save()
```
ğŸ‘‰ éƒ½å¤©ç„¶æ”¯æŒ Path

è¿™æ˜¯ Path èƒ½åœ¨å·¥ç¨‹é‡ŒçœŸæ­£â€œè½åœ°â€çš„åŸå› ã€‚

3ï¸âƒ£ Path åœ¨ä½ è¿™æ®µä»£ç é‡Œçš„çœŸå®ä»·å€¼
```python
tmp_dir = Path(tempfile.mkdtemp(...))
path = tmp_dir / f"part_{i:04d}.pkl"
```
ä½ åœ¨åšçš„æ˜¯ï¼š 
æŠŠ MapReduce çš„ä¸­é—´æ–‡ä»¶ï¼Œå½“æˆâ€œç»“æ„åŒ–è·¯å¾„ç©ºé—´â€æ¥ç®¡ç†  
è€Œä¸æ˜¯ï¼š 
åˆ°å¤„æ‹¼å­—ç¬¦ä¸²   
æä¸æ¸…å“ªäº›æ˜¯ç›®å½•ï¼Œå“ªäº›æ˜¯æ–‡ä»¶  
è¿™æ˜¯ infra é£æ ¼ï¼Œä¸æ˜¯è„šæœ¬é£æ ¼ã€‚  

##### äºŒã€tempfile.mkdtempï¼šä½ åœ¨å‘æ“ä½œç³»ç»Ÿâ€œç§Ÿä¸€å—å®‰å…¨ç©ºé—´â€
1ï¸âƒ£ mkdtemp åˆ°åº•å¹²äº†ä»€ä¹ˆï¼Ÿ  
```python
tmp = tempfile.mkdtemp() 
```
æ“ä½œç³»ç»Ÿå±‚é¢è¯­ä¹‰æ˜¯ï¼š
â€œå¸®æˆ‘åˆ›å»ºä¸€ä¸ª å”¯ä¸€ã€ä¸å†²çªã€å·²å­˜åœ¨ çš„ä¸´æ—¶ç›®å½•ï¼Œå¹¶æŠŠè·¯å¾„ç»™æˆ‘â€  
å®ƒä¿è¯äº†ä¸‰ä»¶äº‹ï¼š 
1. ç›®å½•ä¸€å®šå­˜åœ¨
2. åå­—ä¸ä¼šæ’
3. åˆ›å»ºè¿‡ç¨‹æ˜¯åŸå­çš„ï¼ˆçº¿ç¨‹/è¿›ç¨‹å®‰å…¨ï¼‰

2ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç”¨ mkdir("/tmp/xxx")ï¼Ÿ
å› ä¸ºè¿™æ®µä»£ç åœ¨å¹¶è¡Œ / å¤šè¿›ç¨‹ç¯å¢ƒé‡Œï¼š  
```python
os.makedirs("/tmp/bpe_mapreduce")
```
å¯èƒ½ä¼šï¼š
1. è¢«å¦ä¸€ä¸ªè¿›ç¨‹æŠ¢å…ˆåˆ›å»º
2. æŠ›å¼‚å¸¸
3. æˆ– silently è¦†ç›–  

è€Œ mkdtemp çš„è¯­ä¹‰æ˜¯ï¼š  
ğŸ‘‰ è¿™ä¸ªç›®å½•åªå±äºä½ è¿™ä¸€æ¬¡è¿è¡Œ  

3ï¸âƒ£ mkdtemp vs TemporaryDirectoryï¼ˆéå¸¸é‡è¦ï¼‰  
API	ä¼šè‡ªåŠ¨åˆ é™¤å—	é€‚åˆè°  
mkdtemp()	âŒ ä¸ä¼š	MapReduce / pipeline  
TemporaryDirectory()	âœ… with ç»“æŸè‡ªåŠ¨åˆ 	å°è„šæœ¬ / unit test  

ä½ ç°åœ¨ç”¨ mkdtemp æ˜¯å¯¹çš„ï¼Œå› ä¸ºï¼š

1. reduce å¯èƒ½å¾ˆä¹…  
2. å¯èƒ½è¦ checkpoint / debug  
3. ä¸­é€”å´©æºƒä½ è¿˜æƒ³ ls çœ‹æ–‡ä»¶  

4ï¸âƒ£ åœ¨ä½ è¿™ä¸ª MapReduce é‡Œçš„çœŸå®æ„ä¹‰
```python
tmp_dir = Path(tempfile.mkdtemp(prefix="bpe_mapreduce_"))
```  
ä½ å…¶å®æ˜¯åœ¨è¯´ï¼š  
â€œæˆ‘ç°åœ¨è¦å¼€å§‹ä¸€ä¸ª ç‹¬ç«‹çš„ MapReduce jobï¼Œ  
ç»™æˆ‘ä¸€ä¸ª å¹²å‡€çš„ scratch spaceã€‚â€   
è¿™æ˜¯ åˆ†å¸ƒå¼ç³»ç»Ÿæ€ç»´ï¼Œä¸æ˜¯ Python è¯­æ³•ç³–ã€‚   

##### ä¸‰ã€prefixï¼šä¸æ˜¯è£…é¥°ï¼Œæ˜¯â€œè°ƒè¯•ä¸è¿ç»´æ¥å£â€  
1ï¸âƒ£ prefix çš„ä½œç”¨éå¸¸æœ´ç´   
```python
tempfile.mkdtemp(prefix="bpe_mapreduce_")  
```
æœ€ç»ˆè·¯å¾„å¤§æ¦‚æ˜¯ï¼š  
```bash 
/tmp/bpe_mapreduce_8qk4v9n2  
```
prefix åšäº†ä»€ä¹ˆï¼Ÿ  
1. ç»™éšæœºç›®å½•ååŠ ä¸€ä¸ªäººç±»å¯è¯»æ ‡ç­¾  
2. å‰©ä¸‹éƒ¨åˆ†ä¿è¯å”¯ä¸€æ€§  

2ï¸âƒ£ ä¸ºä»€ä¹ˆ prefix åœ¨ infra ä»£ç é‡Œéå¸¸é‡è¦ï¼Ÿ

è®¾æƒ³ä½ åœ¨æœåŠ¡å™¨ä¸Šï¼š
```bash
ls /tmp
```
çœ‹åˆ°ï¼š
```bash
tmpa9f3
tmpz71k
bpe_mapreduce_8qk4v9n2
torch_compile_xxx
```

ä½ ä¸€çœ¼å°±çŸ¥é“ï¼š
1.å“ªä¸ªæ˜¯ä½ çš„ job
2. å“ªä¸ªå¯ä»¥åˆ 
3. å“ªä¸ªä¸æ•¢åŠ¨
ğŸ‘‰ è¿™å°±æ˜¯ è¿ç»´å¯è§‚æµ‹æ€§

3ï¸âƒ£ prefix çš„â€œéšè—å¥½å¤„â€   
âœ… å´©æºƒåå¯è¿½æº¯
ç¨‹åºæŒ‚äº†  
ä½ è¿˜èƒ½ cd /tmp/bpe_mapreduce_*
çœ‹ï¼š part_*.pkl
1. å“ªä¸ª chunk æœ€æ…¢
2. æ–‡ä»¶å¤§å°åˆ†å¸ƒ

âœ… å¤š job å¹¶è¡Œä¸æ··  
åŒæ—¶è·‘ TinyStories + OWT  
prefix ä¸åŒå³å¯åŒºåˆ†  

##### å››ã€enumerate å¹²çš„äº‹æƒ…
enumerate å¹²çš„äº‹æƒ…ï¼ˆéå¸¸é‡è¦ï¼‰
```python
for i, sub in enumerate(iterator):
```
enumerate åšçš„ä¸æ˜¯â€œæ¢å¤åŸ indexâ€ï¼Œè€Œæ˜¯ï¼š  
åœ¨æ¶ˆè´¹ iterator çš„é‚£ä¸€åˆ»ï¼ŒæŒ‰â€œåˆ°è¾¾é¡ºåºâ€åˆ†é…ä¸€ä¸ªæ–° id



### Problem(train_bpe_tinystories):BPETrainingonTinyStories (2points)

#### a  How many hours and memory did training take?What is the longest token in the vocabulary? Does it make sense?
1. Serialize the resulting vocabulary and merges to disk 
    for further inspection

1. éœ€è¦åˆ›å»ºå­˜æ”¾çš„æ–‡ä»¶å¤¹

```python
#å­˜æ”¾çš„ç›®å½• æ˜¯é¡¹ç›®æ–‡ä»¶å¤¹ä¸­çš„
OUT_DIR = "/home/dexterding/projects/assignment1-basics/bpe_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

VOCAB_PATH = os.path.join(OUT_DIR, "tinystories_vocab.pkl")
MERGES_PATH = os.path.join(OUT_DIR, "tinystories_merges.pkl")
PROFILE_PATH = os.path.join(OUT_DIR, "profile.txt")
```

2. å°†è¿”å›çš„vocabå’Œmerges ç”¨pickle.dump(XXXX,f) çš„æ–¹å¼å­˜æ”¾
```python
    # =========================
    # SAVE ARTIFACTS
    # =========================
    with open(VOCAB_PATH, "wb") as f:
        pickle.dump(vocab, f)

    with open(MERGES_PATH, "wb") as f:
        pickle.dump(merges, f)
```


3. å¦‚ä½•æµ‹è¯• æ‰€ç”¨çš„æ—¶é—´

åœ¨è®­ç»ƒçš„å¼€å¤´å’Œç»“å°¾åˆ†åˆ«æ”¾ä¸€ä¸ª 
```python
start_time = time.time()
## training code
end_time = time.time()
    print(f"Training time (this run): {(end_time - start_time)/60:.2f} minutes")

```

4. æµ‹è¯•vocabå’Œ merge å°±ç”¨ len(vocab/merges)

5. longest token 
å› ä¸º vocabçš„ç»“æ„å°±æ˜¯ dict[int,bytes]
5.1 å¯ä»¥åœ¨è·‘è®­ç»ƒçš„ä»£ç é‡Œæ”¾è¿™ä¸ª 
```python 
    longest_token = max(vocab.values(), key=len)
```
5.2 ä¹Ÿå¯ä»¥ç›´æ¥è¯»å–pickeleæ–‡ä»¶
```python
with open(VOCAB_PATH, "rb") as f:
    vocab = pickle.load(f)
tok_id, tok_bytes = max(vocab.items(), key=lambda kv: len(kv[1]))
print("longest_token_id:", tok_id)
print("byte_len:", len(tok_bytes))
print("raw_bytes_repr:", repr(tok_bytes))
print("decoded_utf8 (replace):", tok_bytes.decode("utf-8", errors="replace"))
```



#### b Profile your code. What part of the tokenizer training process takes the most time
```bash        
1           1262.415    1262.415 1766.558   1766.558 /home/dexterding/projects/assignment1-basics/cs336_basics/debug/parallel_train_bpe_cache.py:171(train_bpe)
5094536393  231.086     0.000    231.086    0.000 {built-in method builtins.len}
2255121258  121.755     0.000    121.755    0.000 {method 'append' of 'list' objects}
28286       51.462      0.002    80.908     0.003 {built-in method builtins.max}
9743        0.033       0.000    80.902     0.008 /home/dexterding/projects/assignment1-basics/cs336_basics/debug/parallel_train_bpe_cache.py:127(get_highest_pair)```

å¯è§å¤§å¤šæ•°æ—¶é—´æ˜¯ lenï¼ˆï¼‰å’Œ list.append å æ®å¤§å¤šæ•°æ—¶é—´ å› ä¸ºä¸€ç›´åœ¨å¤§è§„æ¨¡æ„å»º

Training time (this run): 7.15 minutes
Peak RSS (main + children): 7.47 GB
Main RSS delta (end-start): 0.06 GB
Vocab size: 10000 | merges: 9743
Longest token bytes: 13
```

## 2.6 BPE Tokenizer: Encoding and Decoding

### 2.6.1 Encoding text
#### å°ç»†èŠ‚
1.æ³¨æ„æ­¤å¤„çš„@classmethod å’Œ åç»­çš„return cls æœ¬è´¨ä¸Šæ˜¯ è¿”å›ä¸€ä¸ªclass instance

#### æ€è·¯ç†è§£ 
##### Step1: Pre-tokenize.
We first pre-tokenize the sequence and represent each pre-token as a sequence of UTF-8 bytes, just as we did in BPE training.

é¦–å…ˆè¿˜æ˜¯è¦æŠŠtext å˜æˆsequence of bytes 
é—®é¢˜æ˜¯æµç¨‹æ˜¯å¦è¿˜ä¸€æ ·
1. é¦–å…ˆsplit special token
å› ä¸ºæœ‰å¯èƒ½å‡ºç° "You are cat<|speical|>" 
å› ä¸ºspecial tokenå’Œå…¶ä»–è¯è¯­è¿ä¸€å— ä¸å¸Œæœ›tokenizeræŠŠ cat<|speical|>è¿åœ¨ä¸€èµ·
æ‰€ä»¥å°±è¦å…ˆsplitåå†ç”¨PATåˆ†è¯
ç„¶åå†é€ä¸ªencode(utf-8)

We will be merging these bytes within each pre-token into
vocabulary elements, handling each pre-token independently(nomergesacrosspre-tokenboundaries).

##### Step2:Apply the merges.
We then take the sequence of vocabulary element merges created during BPE
training, and apply it to our pre-tokens in the same order of creation.


 é˜²å¾¡æ€§ç¼–ç¨‹ encodeå®Œæˆ 
 é€»è¾‘æ˜¯greedyåŠ æ”¹å¾ªç¯å˜é‡ 
 é€šè¿‡æ›´æ”¹tokens 
 æ¥æ›´æ–°å¾ªç¯å˜é‡æ¡ä»¶ 
 ç”¨å­—å…¸ç”Ÿæˆå¼åè½¬



 # 3 Transformer Language Model Architecture

è¯­è¨€æ¨¡å‹çš„
è¾“å…¥æ˜¯ä¸€æ‰¹ï¼ˆbatchedï¼‰æ•´æ•°å½¢å¼çš„ token ID åºåˆ—  
ï¼ˆå³å½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ torch.Tensorï¼‰ï¼Œ

è¾“å‡ºæ˜¯ä¸€æ‰¹åœ¨è¯è¡¨ä¸Šçš„å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ  
ï¼ˆå³å½¢çŠ¶ä¸º `(batch_size, sequence_length, vocab_size)` çš„ PyTorch Tensorï¼‰ï¼Œ

å…¶ä¸­æ¯ä¸€ä¸ªä½ç½®ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œéƒ½æ˜¯åœ¨é¢„æµ‹â€œä¸‹ä¸€ä¸ª tokenâ€ã€‚

`åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶`   
æˆ‘ä»¬ä½¿ç”¨è¿™äº›â€œä¸‹ä¸€ä¸ªè¯â€çš„é¢„æµ‹predictionsç»“æœï¼Œ  
æ¥è®¡ç®—çœŸå®ä¸‹ä¸€ä¸ªè¯ä¸æ¨¡å‹é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µæŸå¤± cross-entropy lossã€‚

`åœ¨æ¨ç†ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰æ—¶`  
æˆ‘ä»¬åªå–åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„â€œä¸‹ä¸€ä¸ªè¯æ¦‚ç‡åˆ†å¸ƒâ€ï¼Œ  
ç”¨å®ƒæ¥ç”Ÿæˆä¸‹ä¸€ä¸ª token  
ï¼ˆä¾‹å¦‚ï¼šå–æœ€å¤§æ¦‚ç‡çš„ tokenï¼Œæˆ–ä»åˆ†å¸ƒä¸­é‡‡æ ·ï¼‰ï¼Œ  
ç„¶åæŠŠç”Ÿæˆçš„ token è¿½åŠ åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œ  
å¹¶ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚  

## 3.1 Transformer è¯­è¨€æ¨¡å‹ 

ç»™å®šä¸€æ®µ token ID åºåˆ—ï¼Œ
Transformer è¯­è¨€æ¨¡å‹ä¼šï¼š

1. ä½¿ç”¨è¾“å…¥ embeddingæŠŠ token ID è½¬æˆç¨ å¯†å‘é‡  

2. å°†è¿™äº›å‘é‡é€å…¥ num_layers ä¸ª Transformer block

3. å†é€šè¿‡ä¸€ä¸ªå­¦ä¹ å¾—åˆ°çš„çº¿æ€§æŠ•å½±  
ï¼ˆä¹Ÿå« output embedding æˆ– LM headï¼‰  
æ¥äº§ç”Ÿé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„ logits  
<img src="069ecc5a291e290743f71d6dcb6fccb.png" width="40%">

### 3.1.1 Token Embeddingsï¼ˆè¯å…ƒåµŒå…¥ï¼‰
åœ¨æœ€å¼€å§‹çš„ä¸€æ­¥ä¸­ï¼ŒTransformer ä¼šæŠŠï¼ˆæŒ‰ batch ç»„ç»‡çš„ï¼‰token ID åºåˆ—åµŒå…¥æˆä¸€ä¸²å‘é‡ï¼Œ  
è¿™äº›å‘é‡åŒ…å«äº† token èº«ä»½çš„ä¿¡æ¯ï¼ˆè§å›¾ 1 ä¸­çš„çº¢è‰²æ–¹å—ï¼‰ã€‚   

æ›´å…·ä½“åœ°è¯´ï¼Œç»™å®šä¸€ä¸ª token ID çš„åºåˆ—ï¼ŒTransformer è¯­è¨€æ¨¡å‹ä½¿ç”¨ä¸€ä¸ª token embedding å±‚ æ¥ç”Ÿæˆä¸€ç»„å‘é‡ã€‚  
è¿™ä¸ª embedding å±‚æ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º  
(batch_size, sequence_length) çš„æ•´æ•°å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œ
å¹¶è¾“å‡ºä¸€ä¸ªå½¢çŠ¶ä¸º  
(batch_size, sequence_length, d_model) çš„å‘é‡å¼ é‡ã€‚  

#### ç†è§£  
Embedding â‰  Linear  
æœ¬è´¨æ˜¯ï¼š  
embedding_matrix[token_id]  
è€Œä¸æ˜¯ x @ W  
æ‰€ä»¥åœ¨å®ç°ä¸­ä½ åªèƒ½ç”¨ç´¢å¼•ï¼Œä¸æ˜¯çŸ©é˜µä¹˜æ³•  

### 3.1.2 Pre-norm Transformer Block
åœ¨å®Œæˆ embedding ä¹‹åï¼Œæ¿€æ´»å€¼ä¼šè¢«é€å…¥å¤šä¸ªç»“æ„å®Œå…¨ç›¸åŒçš„ç¥ç»ç½‘ç»œå±‚ä¸­å¤„ç†ã€‚  
ä¸€ä¸ªæ ‡å‡†çš„ decoder-only Transformer è¯­è¨€æ¨¡å‹ ç”± num_layers ä¸ªå®Œå…¨ç›¸åŒçš„å±‚ç»„æˆï¼ˆé€šå¸¸ç§°ä¸º Transformer â€œblocksâ€ï¼‰ã€‚

æ¯ä¸ª block ä¼šï¼š
é€šè¿‡ è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰ åœ¨åºåˆ—ç»´åº¦ä¸Šèšåˆä¿¡æ¯
é€šè¿‡ å‰é¦ˆç½‘ç»œï¼ˆfeed-forward layersï¼‰ å¯¹ä¿¡æ¯è¿›è¡Œéçº¿æ€§å˜æ¢

<img src = "69ca137f59e992d3b43fd3ef257f83c.png" width="40%">

1. â€œidentically structuredâ€ æ˜¯å…³é”®  
æ‰€æœ‰ blockï¼š
ç»“æ„ä¸€æ ·  
å‚æ•°ä¸åŒ  
  ç±»ä¼¼ CNN é‡Œâ€œé‡å¤å †å å·ç§¯å±‚â€

2. decoder-only  
- åªæœ‰ self-attention  
- æ²¡æœ‰ encoder / cross-attention
- é…åˆ causal mask

3. ä¸ºä»€ä¹ˆè¾“å…¥è¾“å‡ºå½¢çŠ¶å¿…é¡»ä¸€æ ·
- å› ä¸ºæœ‰ residualï¼š
```python
x + f(x)
```

4. æ€»ç»“è¿™ä¸ª block
- â€œå…ˆçœ‹å…¨å±€ï¼ˆattentionï¼‰ï¼Œå†åšé€ä½ç½®çš„éçº¿æ€§å˜æ¢ï¼ˆFFNï¼‰â€

## 3.2 Output Normalization and Embedding

è¾“å‡ºå½’ä¸€åŒ–ä¸è¾“å‡ºåµŒå…¥

åœ¨ç»è¿‡ num_layers ä¸ª Transformer block ä¹‹åï¼Œæˆ‘ä»¬ä¼šå–æœ€ç»ˆçš„æ¿€æ´»å€¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªå¯¹æ•´ä¸ªè¯è¡¨çš„æ¦‚ç‡åˆ†å¸ƒã€‚  

æˆ‘ä»¬å°†å®ç° pre-norm Transformer blockï¼ˆè¯¦è§ Â§3.5ï¼‰ã€‚  
è¿™ç§ç»“æ„è¿˜è¦æ±‚åœ¨æœ€åä¸€ä¸ª Transformer block ä¹‹åï¼Œ  
å†é¢å¤–ä½¿ç”¨ä¸€æ¬¡ layer normalizationï¼Œä»¥ç¡®ä¿è¾“å‡ºçš„å°ºåº¦æ˜¯åˆé€‚çš„ã€‚  

åœ¨å®Œæˆå½’ä¸€åŒ–ä¹‹åï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ä¸€ä¸ªæ ‡å‡†çš„ã€å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼Œ  
å°† Transformer block çš„è¾“å‡ºè½¬æ¢ä¸ºé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„ logits  
ï¼ˆä¾‹å¦‚è§ Radford et al. [2018] çš„å…¬å¼ 2ï¼‰ã€‚


## 3.3 Remark: Batching, Einsum and Efficient Computation

åœ¨æ•´ä¸ª Transformer ä¸­ï¼Œæˆ‘ä»¬ä¼šåå¤å¯¹è®¸å¤šâ€œç±»ä¼¼ batch çš„è¾“å…¥â€æ‰§è¡Œç›¸åŒçš„è®¡ç®—ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š

1. Batch çš„å…ƒç´ ï¼šå¯¹ batch ä¸­çš„æ¯ä¸ªelement åº”ç”¨ç›¸åŒçš„ Transformer å‰å‘è®¡ç®—

2. åºåˆ—é•¿åº¦ç»´åº¦ï¼šåƒ RMSNorm å’Œå‰é¦ˆç½‘ç»œè¿™æ ·çš„â€œé€ä½ç½®ï¼ˆposition-wiseï¼‰â€æ“ä½œï¼Œå¯¹åºåˆ—ä¸­æ¯ä¸ªä½ç½®åšç›¸åŒçš„å¤„ç†

3. æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰ï¼šå¤šå¤´æ³¨æ„åŠ›ä¸­çš„ attention æ“ä½œï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨ head ç»´åº¦ä¸Šåš batch


```python
channels_last = torch.randn(64, 32, 32, 3) # (batch, height, width, channel)
B = torch.randn(32*32, 32*32)
## Rearrange an image tensor for mixing across all pixels
channels_last_flat = channels_last.view(-1, channels_last.size(1) * channels_last.size(2), channels_last.size(3)
)
'''
æ­¤å¤„ æ³¨æ„æ˜¯ channels_last.size(1) * channels_last.size(2)
'''
channels_first_flat = channels_last_flat.transpose(1, 2)
channels_first_flat_transformed = channels_first_flat @ B.T
channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)
channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)
'''
æ­¤å¤„ æ³¨æ„æ˜¯ ä½¿ç”¨çš„æ‹†åŒ… ç›´æ¥æ‹†å‡ºæ¥B H W C 
'''
Instead, using einops:
height = width = 32
## Rearrange replaces clunky torch view + transpose
channels_first = rearrange(
channels_last,
"batch height width channel-> batch channel (height width)"
)
channels_first_transformed = einsum(
channels_first, B,
"batch channel pixel_in, pixel_out pixel_in-> batch channel pixel_out"
)
channels_last_transformed = rearrange(
channels_first_transformed,
"batch channel (height width)-> batch height width channel",
height=height, width=width
)
```
### 3.3.1 æ•°å­¦è®°å·ä¸å†…å­˜é¡ºåºï¼ˆMathematical Notation and Memory Orderingï¼‰
è®¸å¤šæœºå™¨å­¦ä¹ è®ºæ–‡åœ¨æ•°å­¦è®°å·ä¸­ä½¿ç”¨è¡Œå‘é‡ï¼ˆrow vectorsï¼‰ï¼Œè¿™ç§è¡¨ç¤ºæ–¹å¼ä¸ NumPy å’Œ PyTorch é»˜è®¤ä½¿ç”¨çš„è¡Œä¸»åºï¼ˆrow-majorï¼‰å†…å­˜å¸ƒå±€éå¸¸å¥‘åˆã€‚

åœ¨ä½¿ç”¨è¡Œå‘é‡çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªçº¿æ€§å˜æ¢å¯ä»¥å†™æˆï¼š
$$ 
 y = x W^{T}
$$

å…¶ä¸­ï¼š
$$ 
ğ‘Šâˆˆğ‘…^{ğ‘‘_{out}Ã—ğ‘‘_{in}} 
$$
$$
xâˆˆğ‘…^{1Ã—d_{m}}
$$	â€‹

---
è€Œåœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œæ›´å¸¸è§çš„æ˜¯ä½¿ç”¨åˆ—å‘é‡ï¼ˆcolumn vectorsï¼‰ã€‚æ­¤æ—¶ï¼Œçº¿æ€§å˜æ¢å†™æˆï¼š
$$ 
y = Wx
$$ 
$$ 
ğ‘Šâˆˆğ‘…^{ğ‘‘_{out}Ã—ğ‘‘_{in}} 
$$
$$
xâˆˆğ‘…^{d_{in}}
$$	
 æ˜¯ä¸€ä¸ªåˆ—å‘é‡ column vector
 åœ¨æ•°å­¦æ¨å¯¼ä¸­ç»Ÿä¸€ä½¿ç”¨åˆ—å‘é‡è®°å·ï¼Œå› ä¸ºè¿™ç§æ–¹å¼åœ¨æ•°å­¦ä¸Šæ›´å®¹æ˜“ç†è§£ã€‚

ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼š
ğŸ‘‰ å¦‚æœä½ åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨çŸ©é˜µä¹˜æ³•ï¼ˆå¦‚ @ï¼‰ï¼Œå°±å¿…é¡»éµå¾ª è¡Œå‘é‡çº¦å®šï¼Œå› ä¸º PyTorch ä½¿ç”¨çš„æ˜¯è¡Œä¸»åºå†…å­˜å¸ƒå±€ã€‚

å¦‚æœä½ ä½¿ç”¨ einsum æ¥åšçŸ©é˜µè¿ç®—ï¼Œé‚£ä¹ˆè¿™ä¸ªé—®é¢˜åŸºæœ¬å¯ä»¥å¿½ç•¥ã€‚
## 3.4 åŸºæœ¬æ¨¡å—ï¼šLinear ä¸ Embedding
### 3.4.1 å‚æ•°åˆå§‹åŒ–
æœ‰æ•ˆè®­ç»ƒç¥ç»ç½‘ç»œé€šå¸¸éœ€è¦ç²¾å¿ƒè®¾è®¡çš„å‚æ•°åˆå§‹åŒ–ã€‚ç³Ÿç³•çš„åˆå§‹åŒ–ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚  
è™½ç„¶ Pre-norm Transformer å¯¹åˆå§‹åŒ–ç›¸å¯¹é²æ£’ï¼Œä½†åˆå§‹åŒ–æ–¹å¼ä»ä¼šæ˜¾è‘—å½±å“è®­ç»ƒé€Ÿåº¦å’Œæ”¶æ•›æ€§ã€‚   
åœ¨æœ¬æ¬¡ä½œä¸šä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹è¿‘ä¼¼åˆå§‹åŒ–æ–¹æ¡ˆï¼š  
å…¶ä»–çš„ç•™åˆ°assignment3 é‡Œ

#### Linear æƒé‡åˆå§‹åŒ–
$$
W \sim \mathcal{N}
\left(
0,\;
\frac{2}{d_{\text{in}} + d_{\text{out}}}
\right),
\quad W \in [-3\sigma, 3\sigma]
$$

â‘  æ­£æ€åˆ†å¸ƒï¼ˆGaussian / Normalï¼‰

$$
\mathcal N(\mu, \sigma^2)
$$

- å‡å€¼ï¼š
$$\mu = 0\$$
- æ–¹å·®ï¼š
$$\sigma^2 = \frac{2}{d_{\text{in}} + d_{\text{out}}}\$$

è¿™æ˜¯ä¸€ç§ **Xavier / Glorot åˆå§‹åŒ–çš„å˜ä½“**ã€‚

---

#### â‘¡ ä¸ºä»€ä¹ˆæ–¹å·®æ˜¯ 
$sigma^2 = \frac{2}{d_{\text{in}} + d_{\text{out}}}$

- $d_{\text{in}}$ï¼šè¾“å…¥ç»´åº¦  
- $d_{\text{out}}$ï¼šè¾“å‡ºç»´åº¦  

ç›´è§‰ç†è§£ï¼š

- æ¯ä¸€å±‚çš„è¾“å‡ºæ˜¯å¾ˆå¤šæƒé‡ Ã— è¾“å…¥çš„**ç´¯åŠ **
- å¦‚æœæƒé‡å¤ªå¤§ â†’ æ¿€æ´»å€¼ / æ¢¯åº¦çˆ†ç‚¸
- å¦‚æœæƒé‡å¤ªå° â†’ æ¿€æ´»å€¼ / æ¢¯åº¦æ¶ˆå¤±
- è¿™ä¸ªæ–¹å·®è®©å‰å‘å’Œåå‘ä¼ æ’­çš„å°ºåº¦ä¿æŒç¨³å®š

ä¸€å¥è¯æ€»ç»“ï¼š

> **å±‚è¶Šå®½ï¼Œæ¯ä¸ªæƒé‡å°±åº”è¯¥è¶Šå°**

---

#### â‘¢ truncated at $[-3\sigma, 3\sigma]$ æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

è¿™æ˜¯ **æˆªæ–­æ­£æ€åˆ†å¸ƒï¼ˆTruncated Normalï¼‰**ï¼š

- å…ˆæŒ‰æ­£æ€åˆ†å¸ƒé‡‡æ ·
- å¦‚æœå€¼è¶…å‡º \([-3\sigma, 3\sigma]\)ï¼Œå°±ä¸¢å¼ƒå¹¶é‡æ–°é‡‡æ ·

åŸå› ï¼š

- é«˜æ–¯åˆ†å¸ƒæ˜¯æ— ç•Œçš„ï¼Œæå°æ¦‚ç‡ä¼šé‡‡åˆ°ç‰¹åˆ«å¤§çš„å€¼
- Transformer å¯¹è¿™ç§ outlier éå¸¸æ•æ„Ÿ
- Â±3Ïƒ è¦†ç›–çº¦ **99.7%** çš„æ¦‚ç‡è´¨é‡
- å‡ ä¹ä¸æ”¹å˜åˆ†å¸ƒå½¢çŠ¶ï¼Œä½†æ˜¾è‘—æé«˜ç¨³å®šæ€§

---


### Embedding åˆå§‹åŒ–
$$
E \sim \mathcal{N}(0, 1), \quad E \in [-3, 3]
$$
Embedding çš„æœ¬è´¨æ˜¯ï¼š

- **æŸ¥è¡¨ï¼ˆlookupï¼‰**
- ä¸åšçŸ©é˜µä¹˜æ³•
- ä¸å‘ç”Ÿå¤§è§„æ¨¡ç´¯åŠ 

å› æ­¤ï¼š

- ä¸å­˜åœ¨è¾“å…¥ç»´åº¦å¯¼è‡´çš„æ•°å€¼æ”¾å¤§é—®é¢˜
- ä½¿ç”¨å•ä½æ–¹å·®$\sigma^2 = 1$å°±è¶³å¤Ÿ
- åŒæ ·ç”¨æˆªæ–­é˜²æ­¢æç«¯å€¼
### RMSNorm åˆå§‹åŒ–

$$
g_i = 1
$$

| æ¨¡å—        | åˆå§‹åŒ–æ–¹å¼                               |
| --------- | ----------------------------------- |
| Linear    | `torch.nn.init.trunc_normal_`       |
| Embedding | `torch.nn.init.trunc_normal_`       |
| RMSNorm   | `nn.Parameter(torch.ones(d_model))` |

### 3.4.2 Linear æ¨¡å—

åŸæ–‡ç¿»è¯‘

Linear å±‚æ˜¯ Transformer å’Œç¥ç»ç½‘ç»œä¸­æœ€åŸºæœ¬çš„æ„ä»¶ä¹‹ä¸€ã€‚

ä½ éœ€è¦å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ Linear ç±»ï¼ˆç»§æ‰¿ torch.nn.Moduleï¼‰ï¼Œå®Œæˆå¦‚ä¸‹çº¿æ€§å˜æ¢ï¼š

çº¿æ€§å˜æ¢å®šä¹‰ä¸ºï¼š
$$
\mathbf{y} = \mathbf{W} \mathbf{x}
$$
å…¶ä¸­ï¼š
$$
\mathbf{W} \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}
$$
æ³¨æ„ï¼š
ä¸åŒ…å« bias é¡¹ï¼ˆè¿™æ˜¯ç°ä»£å¤§æ¨¡å‹çš„å¸¸è§åšæ³•ï¼‰ã€‚


###ä½œä¸šé¡»çŸ¥

1. é¦–å…ˆç»§æ‰¿class
ä¸åªæ˜¯
```python 
class Linear(nn.Module)
è€Œä¸”ä¸èƒ½å¿˜äº† 
super().__inti__()
```
2. åˆ›å»ºç©ºçš„torch ä¸ç”¨ones ä¸ç”¨zeros
```python
ç”¨
torch.empty(x,y,device=device,dtype=dtype)
```

3. åˆ›å»ºé«˜æ–¯åˆ†å¸ƒ

è¿™ä¸ªtrunc_normalçš„æ“ä½œæ—¶in placeçš„
```python
torch.nn.init.trunc_normal_(
    tensor=W,
    mean=mean,
    std=std,
    a=-3*std,
    b=3*std
)
```

4. pytrochè‡ªåŠ¨è®°å½•å‚æ•° éœ€è¦ç”¨nn.Parameter
```python
self.W = nn.Parameter(W)
```

5. adapter
```python
embedding.load_state_dict({"weights":weights})
```
å‰é¢çš„weights æ˜¯class ä¸­çš„instance attribute
åé¢çš„weightsæ˜¯è¯»å–çš„å€¼

## 3.5 Pre-Norm Transformer Blockï¼ˆé¢„å½’ä¸€åŒ– Transformer å—ï¼‰

æ¯ä¸ª Transformer block åŒ…å«ä¸¤ä¸ªå­å±‚ï¼ˆsub-layerï¼‰ï¼š
å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰  
é€ä½ç½®å‰é¦ˆç½‘ç»œï¼ˆPosition-wise Feed-Forward Networkï¼‰  
åœ¨æœ€åˆçš„ Transformerï¼ˆVaswani et al., 2017ï¼‰ä¸­ï¼Œæ¯ä¸ªå­å±‚çš„ç»“æ„æ˜¯ï¼š  

å­å±‚ â†’ æ®‹å·®è¿æ¥ â†’ LayerNorm

è¿™ç§ç»“æ„è¢«ç§°ä¸º Post-Norm Transformerï¼ˆåå½’ä¸€åŒ–ï¼‰ï¼Œå› ä¸º LayerNorm ä½œç”¨åœ¨å­å±‚è¾“å‡ºä¸Šã€‚

åç»­ç ”ç©¶å‘ç°ï¼Œå¦‚æœå°† LayerNorm ä»å­å±‚è¾“å‡ºç§»åŠ¨åˆ°å­å±‚è¾“å…¥ï¼Œå¹¶åœ¨æœ€åä¸€ä¸ª Transformer block ä¹‹åå†åŠ ä¸€æ¬¡å½’ä¸€åŒ–ï¼Œå¯ä»¥æ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ï¼ˆNguyen & Salazar, 2019ï¼›Xiong et al., 2020ï¼‰ã€‚è¿™ç§ç»“æ„è¢«ç§°ä¸º Pre-Norm Transformerï¼ˆé¢„å½’ä¸€åŒ–ï¼‰ã€‚

åœ¨ Pre-Norm ç»“æ„ä¸­ï¼š

æ¯ä¸ªå­å±‚ å…ˆåšå½’ä¸€åŒ–  
å†è¿›å…¥å­å±‚è®¡ç®—  
æœ€åé€šè¿‡ æ®‹å·®è¿æ¥ï¼ˆresidual connectionï¼‰ ä¸è¾“å…¥ç›¸åŠ 

ç›´è§‚ç†è§£æ˜¯ï¼š

ä»è¾“å…¥ embedding åˆ°æœ€ç»ˆè¾“å‡ºï¼Œæœ‰ä¸€æ¡**â€œå¹²å‡€çš„æ®‹å·®æµï¼ˆresidual streamï¼‰â€**ï¼Œä¸­é—´æ²¡æœ‰è¢«å½’ä¸€åŒ–æ‰“æ–­ï¼Œè¿™æœ‰åŠ©äº æ¢¯åº¦ä¼ æ’­æ›´ç¨³å®šã€‚  
ç›®å‰å‡ ä¹æ‰€æœ‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-3ã€LLaMAã€PaLMï¼‰éƒ½é‡‡ç”¨ Pre-Norm Transformerï¼Œå› æ­¤æœ¬ä½œä¸šä¹Ÿå®ç°è¿™ä¸€ç‰ˆæœ¬ã€‚

Pre-Norm Transformer çš„æ•°å­¦å½¢å¼

è®¾è¾“å…¥ä¸º
$$
ğ‘¥âˆˆğ‘…^{ğ‘‘_{model}} 
$$
ä¸€ä¸ª Transformer block çš„ä¸¤å±‚å¯å†™ä¸ºï¼š
ï¼ˆ1ï¼‰è‡ªæ³¨æ„åŠ›å­å±‚
$$
z = x + \mathrm{MHA}(\mathrm{Norm}(x))
$$
ï¼ˆ2ï¼‰å‰é¦ˆç½‘ç»œå­å±‚

$$
y = z + \mathrm{FFN}(\mathrm{Norm}(z))
$$
å…¶ä¸­ï¼š
- Norm æ˜¯ RMSNormï¼ˆæœ¬ä½œä¸šä½¿ç”¨ï¼‰
- MHA æ˜¯å¤šå¤´è‡ªæ³¨æ„åŠ›
- FFN æ˜¯é€ä½ç½®å‰é¦ˆç½‘ç»œ
- +æ˜¯æ®‹å·®è¿æ¥

åŸå§‹ Transformer ä½¿ç”¨çš„æ˜¯ Layer Normalizationï¼ˆLayerNormï¼‰ã€‚
æœ¬ä½œä¸šä¸­ï¼Œæˆ‘ä»¬æŒ‰ç…§ Touvron et al. (2023) çš„åšæ³•ï¼Œæ”¹ç”¨ RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰ã€‚

ç»™å®šä¸€ä¸ªæ¿€æ´»å‘é‡ï¼š
$$
\mathbf{a} = (a_1, a_2, \dots, a_{d_{\text{model}}})
\in \mathbb{R}^{d_{\text{model}}}
$$

RMSNorm will rescale each activation ai as follows

$$
\mathrm{RMSNorm}(a_i)
=
\frac{a_i}{\mathrm{RMS}(\mathbf{a})}
\cdot g_i
$$
where RMS(a)
$$
\mathrm{RMS}(\mathbf{a})
=
\sqrt{
\frac{1}{d_{\text{model}}}
\sum_{i=1}^{d_{\text{model}}} a_i^2
+ \varepsilon
}
$$

giæ˜¯ å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼ˆgainï¼‰ 

ä¸€å…±æœ‰ $ğ‘‘_{model}ä¸ª ğ‘”_{i}$

Îµ æ˜¯æ•°å€¼ç¨³å®šé¡¹ï¼Œé€šå¸¸è®¾ä¸º 1leâˆ’5

#### ä½œä¸šå®æµ‹ linear + embedding + RMSNorm
1. è¿™é‡Œè¦nn parameterï¼ˆtorch ones/zeros/emptyï¼‰å¥½åƒåŒºåˆ«éƒ½ä¸å¤§  å› ä¸ºéƒ½ä¼š 
```python
rmsnorm.load_state_dict({"weights":weights})
```

2. åœ¨tensorçš„è®¡ç®—è¿‡ç¨‹ä¸­ è®°å¾— sum æˆ–è€…meançš„ dimension
3. math.sqrt ä¸èƒ½å¤„ç†tensor çš„sqrt 
4. keep dimsension æ‰èƒ½broadcast
5. è¿™é‡Œè½¬æ¢typeçš„æ—¶å€™ å¯ä»¥åˆ©ç”¨ 

```python
'''
è¿™é‡Œ è®°å¾—è®°ä½in_dtype
ç„¶åä½¿ç”¨x.to(torch.float32)è¿™ç§ 
'''
in_dtype = x.dtype
x = x.to(torch.float32)
# Your code here performing RMSNorm
...
result = ...
# Return the result in the original dtype
return result.to(in_dtype)
```
6. å¦‚ä½•ä½¿ç”¨einops 
```python
from einops import einsum
from einops import reduce
```
7. einsumæ„Ÿå— 
åœ¨æ ‡æ³¨çš„åŒæ—¶ å®Œæˆäº† rearrange å¹¶ä¸”åŠ ä¸ŠçŸ©é˜µä¹˜æ³•  
```python
#/home/dexterding/projects/assignment1-basics/cs336_basics/transformer_modules/linear_module.py
forward(self,x:torch.Tensor)->torch.Tensor:
        return x@self.W.T
        
        return einsum(x,self.W,'batch sequence in_f, out_f in_f -> batch sequence out_f')
```
è¿™é‡Œæ˜¯ä¸¤ç§ æˆ‘è§‰å¾—éƒ½å†™å¯ä»¥åŒä¿®

8. reduceæ„Ÿå—
ååˆ†æ¸…æ¥šçš„æ ‡æ³¨äº†è§£äº† å°‘æ‰çš„ æ˜¯å“ªä¸€ä¸ªdimensionä»¥åŠå…¶å«ä¹‰  
ä¸‰è€…ç­‰ä»·
```python
x.pow(2).mean(dim=-1,keepdim=True)

x.pow(2).sum(dim=-1,keepdim= True)/self.d_model

mean_square = reduce(x.pow(2), "batch sequence d_model -> batch sequence 1", "mean")
```

### 3.5.2 Position-Wise Feed-Forward Network
åŸæ–‡

åœ¨æœ€åˆçš„ Transformer è®ºæ–‡ï¼ˆVaswani ç­‰ï¼Œ2017ï¼Œç¬¬ 3.3 èŠ‚ï¼‰ä¸­ï¼Œ**å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰**ç”±ä¸¤å±‚çº¿æ€§å˜æ¢ç»„æˆï¼Œä¸­é—´ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ï¼ˆReLU(x) = max(0, x)ï¼‰ã€‚å…¶ä¸­ï¼Œä¸­é—´éšè—å±‚çš„ç»´åº¦é€šå¸¸æ˜¯è¾“å…¥ç»´åº¦çš„ 4 å€ã€‚

ç„¶è€Œï¼Œç°ä»£è¯­è¨€æ¨¡å‹ç›¸è¾ƒäºè¿™ä¸€åŸå§‹è®¾è®¡ï¼Œé€šå¸¸å¼•å…¥äº†`ä¸¤`ä¸ªä¸»è¦å˜åŒ–ï¼š  
ä½¿ç”¨ä¸åŒäº ReLU çš„æ¿€æ´»å‡½æ•°ï¼›

å¼•å…¥ä¸€ç§ é—¨æ§æœºåˆ¶ï¼ˆ`gating mechanism`ï¼‰ã€‚
å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ç§åä¸º `SwiGLU` çš„æ¿€æ´»å‡½æ•°ï¼Œè¯¥å‡½æ•°å·²è¢«è¯¸å¦‚ Llama 3 å’Œ Qwen 2.5 ç­‰å¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨ã€‚

`SwiGLU` å°† `SiLU`ï¼ˆåˆç§° Swishï¼‰æ¿€æ´»å‡½æ•° ä¸ä¸€ç§ç§°ä¸º `Gated Linear Unitï¼ˆGLUï¼‰` çš„é—¨æ§ç»“æ„ç»“åˆèµ·æ¥ã€‚

æ­¤å¤–ï¼Œéµå¾ªè‡ª PaLM ä¸ LLaMA ä»¥æ¥çš„å¤§å¤šæ•°ç°ä»£ LLM çš„å®è·µï¼Œæˆ‘ä»¬åœ¨è¿™äº›çº¿æ€§å±‚ä¸­ çœç•¥ bias é¡¹ã€‚

1ï¸âƒ£ SiLU / Swish æ¿€æ´»å‡½æ•°

SiLUï¼ˆæˆ– Swishï¼‰æ¿€æ´»å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
$$
\mathrm{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

å®ƒä¸ ReLU ç±»ä¼¼ï¼Œä½†åœ¨ 0 é™„è¿‘æ˜¯ å¹³æ»‘çš„ï¼ˆsmoothï¼‰ï¼Œè€Œä¸æ˜¯æœ‰æŠ˜ç‚¹ã€‚

2ï¸âƒ£ Gated Linear Unitï¼ˆGLUï¼‰  

GLU æœ€æ—©ç”± Dauphin ç­‰äººåœ¨ 2017 å¹´æå‡ºï¼Œå…¶å®šä¹‰ä¸ºï¼š  
ä¸€ä¸ªçº¿æ€§å˜æ¢ç»è¿‡ sigmoid åï¼Œä¸å¦ä¸€ä¸ªçº¿æ€§å˜æ¢è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
$$
\mathrm{GLU}(x, W_1, W_2)
= \sigma(W_1 x) \odot (W_2 x)
$$

GLU è¢«è®¤ä¸ºå¯ä»¥ï¼š  
é€šè¿‡æä¾›ä¸€æ¡â€œçº¿æ€§æ¢¯åº¦é€šè·¯â€ï¼Œåœ¨ä¿ç•™éçº¿æ€§èƒ½åŠ›çš„åŒæ—¶ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

3ï¸âƒ£ SwiGLUï¼šSiLU + GLU çš„ç»„åˆ

å°† SiLUï¼ˆSwishï¼‰å’Œ GLU ç»“åˆï¼Œå°±å¾—åˆ°äº† SwiGLUï¼Œæˆ‘ä»¬å°†åœ¨ Transformer çš„å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨å®ƒï¼š
$$
\mathrm{FFN}(x)
= \mathrm{SwiGLU}(x, W_1, W_2, W_3)
= W_2 \left( \mathrm{SiLU}(W_1 x) \odot (W_3 x) \right)
$$

ä¸€ä¸ªçº¿æ€§æŠ•å½±W1è´Ÿè´£ éçº¿æ€§å˜æ¢ï¼ˆSiLUï¼‰  
å¦ä¸€ä¸ªçº¿æ€§æŠ•å½±W3è´Ÿè´£ é—¨æ§

ä¸¤è€…é€å…ƒç´ ç›¸ä¹˜åï¼Œå†ç»è¿‡è¾“å‡ºçº¿æ€§å±‚W2

åœ¨æ ‡å‡†è®¾ç½®ä¸­ï¼Œå‰é¦ˆå±‚çš„éšè—ç»´åº¦æ»¡è¶³ï¼š
$$
x \in \mathbb{R}^{d_{\text{model}}}
$$

$$
W_1, W_3 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}
$$

$$
W_2 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}
$$

$$
d_{\text{ff}} = \frac{8}{3} d_{\text{model}}
$$


4ï¸âƒ£ ç»éªŒè§†è§’ï¼ˆEmpirical Perspectiveï¼‰

Shazeerï¼ˆ2020ï¼‰é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æå‡ºå°† SiLU/Swish ä¸ GLU ç»“åˆï¼Œå¹¶é€šè¿‡å®éªŒè¡¨æ˜ï¼š

SwiGLU åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ä¼˜äº ReLU

ä¹Ÿä¼˜äº ä¸å¸¦é—¨æ§çš„ SiLU

å°½ç®¡æœ‰ä¸€äº›å¯å‘å¼è§£é‡Šï¼Œä½†ä½œè€…æœ¬äººä¿æŒäº†éå¸¸å…‹åˆ¶ã€ç»éªŒä¸»ä¹‰çš„æ€åº¦ï¼Œå¹¶ç•™ä¸‹äº†ä¸€å¥éå¸¸è‘—åçš„è¯ï¼š

æˆ‘ä»¬å¹¶ä¸è§£é‡Šä¸ºä»€ä¹ˆè¿™äº›ç»“æ„æœ‰æ•ˆï¼›
æˆ‘ä»¬å°†å®ƒä»¬çš„æˆåŠŸï¼Œå’Œå…¶ä»–ä¸€åˆ‡ä¸€æ ·ï¼Œå½’å› äºç¥çš„ä»æ…ˆã€‚

---

### 3.5.3 Relative Positional Embeddings ç›¸å¯¹ä½ç½®ç¼–ç  
ä¸ºäº†å‘æ¨¡å‹ä¸­æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œæˆ‘ä»¬å°†å®ç° æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Position Embeddings, RoPEï¼‰ã€‚  

1. å¦‚ä½•ç†è§£è¿™ä¸ªæ³¨å…¥ä½ç½®ä¿¡æ¯çš„å‘¢  
RoPE ä¸æ˜¯åœ¨ QK è¿‡ç¨‹ä¸­â€œå­¦ä¼šâ€è·ç¦»å·®
è€Œæ˜¯ï¼šåœ¨ QK ç‚¹ç§¯æ—¶ï¼Œâ€œæš´éœ²â€å‡ºäº†è·ç¦»å·®   
ä¾èµ–çš„ä¸æ˜¯ç»å¯¹ä½ç½®ç¼–ç  è€Œæ˜¯ç›¸å¯¹ä½ç½®  
ä¸ºäº†ä¸¤ä¸ª QKä¹˜ç§¯èƒ½è·å¾—ä½ç½®ä¿¡æ¯ 
æ•°å­¦å…³é”®æ˜¯ äºŒç»´æ—‹è½¬çŸ©é˜µçš„è½¬ç½® Ã— æ—‹è½¬çŸ©é˜µ = è§’åº¦ç›¸å‡  

2. ä¸ºä»€ä¹ˆæ˜¯2dçŸ©é˜µä»¥åŠå¦‚ä½•ä½œç”¨åœ¨d_modelå•ä½é•¿åº¦çš„  
ç”±äºåªæœ‰2dçš„æ—‹è½¬çŸ©é˜µèƒ½å¤Ÿæœ‰è¿™ç§ å€’ç½®ä¹‹åæ˜¯é€†æ—‹è½¬çš„æ€§è´¨
æ‰€ä»¥æ˜¯æŠŠä¸€ä¸ªq çš„embedding  
ä»(d_model) -> (2,d_model)
ç„¶åæ¯ä¸ªä¹˜ä»¥å¯¹åº”çš„æ—‹è½¬çŸ©é˜µ 

3. å®é™…çš„æ“ä½œå’Œç†è§£   
é»˜è®¤ $d$ ä¸ºå¶æ•°ï¼Œè¾“å…¥å‘é‡ $x\in\mathbb{R}^d$ï¼ˆåœ¨ Transformer é‡Œå¯¹åº”æŸä¸ª token çš„ $q$ æˆ– $k$ï¼‰ã€‚


#### 1. 2D å­ç©ºé—´ï¼šæ¯ä¸€å¯¹ç»´åº¦çš„æ—‹è½¬ï¼ˆæœ€ç›´è§‚ç‰ˆæœ¬ï¼‰

æŠŠ $x$ çš„ç»´åº¦ä¸¤ä¸¤é…å¯¹ã€‚ç¬¬ $k$ å¯¹ï¼ˆä» 0 å¼€å§‹è®¡ ä¸æ˜¯1å“¦ï¼‰å®šä¹‰ä¸ºï¼š

$$
x_{(k)}=
\begin{pmatrix}
x_{2k}\\
x_{2k+1}
\end{pmatrix}\in\mathbb{R}^2,\quad k\in\{0,\dots,\frac d2-1\}.
$$

RoPE åœ¨ token ä½ç½® $i$ã€ç¬¬ $k$ å¯¹ç»´åº¦ä¸Šçš„æ—‹è½¬è§’ä¸ºï¼š

$$
\theta_{i,k}= i\cdot \Theta^{-\frac{2k}{d}}, \quad k\in\{0,\dots,\frac d2-1\}.
$$
ï¼ˆè®²ä¹‰é‡Œæ˜¯è¿™ä¹ˆå†™çš„ï¼š ä¸ä¸Šé¢ç­‰ä»·ï¼‰

$$
\theta_{i,k}= \frac i{\Theta^{\frac{2k-2}{d}}}, \quad k\in\{1,\dots,\frac d2\}.
$$



å¯¹åº”çš„ $2\times2$ æ—‹è½¬çŸ©é˜µï¼š

$$
R(\theta_{i,k})=
\begin{pmatrix}
\cos\theta_{i,k} & -\sin\theta_{i,k}\\
\sin\theta_{i,k} & \cos\theta_{i,k}
\end{pmatrix}.
$$

äºæ˜¯å¯¹è¿™ä¸€å¯¹ç»´åº¦çš„ RoPE å˜æ¢æ˜¯ï¼š

$$
x'_{(k)} = R(\theta_{i,k})\,x_{(k)}.
$$

---



#### 2. Block-diagonalï¼šç”¨ä¸€ä¸ªå¤§çŸ©é˜µæŠŠâ€œæ‹†åˆ† + é€é¡¹æ—‹è½¬ + æ‹¼æ¥â€å†™å®Œ

æŠŠæ‰€æœ‰ $2\times2$ æ—‹è½¬å—æ²¿å¯¹è§’çº¿æ’èµ·æ¥ï¼Œå¾—åˆ° $d\times d$ çš„ block-diagonal çŸ©é˜µï¼š

$$
R_i =
\begin{pmatrix}
R(\theta_{i,0}) & & & \\
& R(\theta_{i,1}) & & \\
& & \ddots & \\
& & & R(\theta_{i,\frac d2-1})
\end{pmatrix}\in\mathbb{R}^{d\times d}.
$$

é‚£ä¹ˆä½ è„‘å­é‡Œçš„ä¸‰æ­¥æ“ä½œï¼ˆæ‹†åˆ† â†’ æ¯å¯¹æ—‹è½¬ â†’ æ‹¼æ¥ï¼‰å¯ä»¥ç”¨ **ä¸€ä¸ªå¼å­**è¡¨ç¤ºï¼š

$$
\boxed{
x'_i = R_i\,x_i
}
$$

> æ³¨æ„ï¼šå®ç°æ—¶é€šå¸¸ **ä¸ä¼šçœŸçš„æ„é€ ** $R_i$ï¼ˆé‚£æ ·æ˜¯ $O(d^2)$ï¼‰ï¼Œä½†å®ƒéå¸¸é€‚åˆåšæ•°å­¦æ¨å¯¼ä¸è¯æ˜ã€‚

---



#### 3. å®ç°å‹å¥½å…¬å¼ï¼šä¸ç”¨æ„é€ çŸ©é˜µçš„ç­‰ä»·å†™æ³•ï¼ˆçœŸæ­£å¸¸ç”¨ï¼‰

å®é™…å®ç°é‡Œé€šå¸¸é¢„è®¡ç®—/ç¼“å­˜ $\sin$ å’Œ $\cos$ï¼Œå†ç”¨é€å…ƒç´ æ“ä½œå®ç°æ—‹è½¬ã€‚
##### 3.0 ä¸ºä»€ä¹ˆè¦è¿™æ ·å­çš„æ•°å­¦åŸç†

å‡è®¾ä¸€ä¸ªtoken çš„q embeddingæ˜¯ (d_modelçš„)   
åˆ‡æˆä¸€ä¸ªä¸ªå°çš„ï¼ˆd_modelï¼‰çš„pairçš„æ—¶å€™  
pair_1 = [10,20] âˆˆ $R^{ï¼ˆ1ï¼Œ2ï¼‰}$ ä¹˜ä¸€ä¸ªæ—‹è½¬çŸ©é˜µ$R(\theta_{i,0}),R^{ï¼ˆ2ï¼Œ2ï¼‰}$  
æ•°å­¦ç»“æœæ˜¯ 10 * R(0,0) + 20 *R(1,0) + 10 * R(0,1) + 20 *R(1,1)

$$
R(\theta_{i,k})=
\begin{pmatrix}
\cos\theta_{i,k} & -\sin\theta_{i,k}\\
\sin\theta_{i,k} & \cos\theta_{i,k}
\end{pmatrix}.
$$

ä¹Ÿå°±æ˜¯è¯´å‘¢  
R(0,0) = R(1,1)  
R(1,0) = - R(0,1)

10 * R(0,0) + 20 *R(1,0) + 10 * R(0,1) + 20 *R(1,1)  
å˜æˆ  
10 * R(0,0) - 20 *R(0,1) + 10 * R(0,1) + 20 *R(0,0)
å˜æˆ  
10 * $\cos\theta_{i,k}$ - 20 *$\sin\theta_{i,k}$ + 10 * $\sin\theta_{i,k}$ + 20 *$\cos\theta_{i,k}$  
å˜æˆ
10 * $\cos\theta_{i,k}$ +20 *$\cos\theta_{i,k}$ - 20 *$\sin\theta_{i,k}$ + 10 * $\sin\theta_{i,k}$    

ä¹Ÿå°±æ˜¯è¯´åªè¦æ„å»ºä¸€ä¸ª cos å’Œä¸€ä¸ª sin  
å†æŠŠ 10 å’Œ 20 çš„å¥‡æ•°å¶æ•°ä½ç½®å¯¹è°ƒå°±èƒ½ä¼˜åŒ–è¿ç®—  
##### 3.1 æ„é€ æŒ‰ç»´åº¦å¯¹é½çš„ $\cos_i,\sin_i$

å…ˆä¸ºæ¯ä¸€å¯¹ç»´åº¦å¾—åˆ° $\cos\theta_{i,k},\sin\theta_{i,k}$ï¼Œç„¶åæŠŠæ¯ä¸ªå€¼é‡å¤ä¸¤æ¬¡ä»¥å¯¹é½ $(2k,2k+1)$ï¼š

$$
\cos_i = (\cos\theta_{i,0},\cos\theta_{i,0},\cos\theta_{i,1},\cos\theta_{i,1},\dots)
$$

$$
\sin_i = (\sin\theta_{i,0},\sin\theta_{i,0},\sin\theta_{i,1},\sin\theta_{i,1},\dots)
$$

##### 3.2 å®šä¹‰ä¸€ä¸ªâ€œäº¤æ¢+å–è´Ÿâ€çš„ç®—å­ rotate

å®šä¹‰ä¸€ä¸ªç®—å­æŠŠæ¯ä¸€å¯¹ $(x_{2k},x_{2k+1})$ å˜æˆ $(-x_{2k+1},x_{2k})$ï¼š

$$
\operatorname{rotate}(x) = (-x_1,\, x_0,\,-x_3,\, x_2,\,\dots).
$$

ä¹Ÿå°±æ˜¯ï¼š

$$
(x_{2k},x_{2k+1})\mapsto (-x_{2k+1},x_{2k}).
$$

##### 3.3 RoPE çš„ç»Ÿä¸€å…¬å¼ï¼ˆæ ¸å¿ƒï¼‰

ç”¨é€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard ä¹˜ï¼Œè®°ä½œ $\odot$ï¼‰ï¼ŒRoPE å¯ä»¥å†™æˆï¼š

$$
\boxed{
x'_i = x_i \odot \cos_i + \operatorname{rotate}(x_i)\odot \sin_i
}
$$

è¿™ä¸ªå¼å­åœ¨ **æ•°å­¦ä¸Šç­‰ä»·äº** $x'_i = R_i x_i$ï¼Œä½†è®¡ç®—å¤æ‚åº¦æ˜¯ **$O(d)$**ï¼Œéå¸¸ GPU å‹å¥½ã€‚

---

##### 4. å¤æ•°è§†è§’ï¼šä¸ºä»€ä¹ˆè¿™ä¸ªå†™æ³•â€œå¾ˆæ¼‚äº®â€

æŠŠæ¯ä¸€å¯¹ç»´åº¦çœ‹ä½œä¸€ä¸ªå¤æ•°ï¼š

$$
z_k = x_{2k} + \mathrm{i}\,x_{2k+1}.
$$

é‚£ä¹ˆæ—‹è½¬ç­‰ä»·äºå¤æ•°ä¹˜æ³•ï¼š

$$
z'_k = z_k \cdot e^{\mathrm{i}\theta_{i,k}}.
$$

å±•å¼€ $e^{\mathrm{i}\theta}=\cos\theta+\mathrm{i}\sin\theta$ åï¼Œå®éƒ¨/è™šéƒ¨åˆšå¥½å¯¹åº”ï¼š

$$
\begin{aligned}
x'_{2k} &= x_{2k}\cos\theta_{i,k} - x_{2k+1}\sin\theta_{i,k},\\
x'_{2k+1} &= x_{2k}\sin\theta_{i,k} + x_{2k+1}\cos\theta_{i,k}.
\end{aligned}
$$

è¿™å°±æ˜¯ä¸Šé¢ â€œ$x\odot\cos + \operatorname{rotate}(x)\odot\sin$â€ çš„æ¥æºã€‚

---

##### 5. ç›¸å¯¹ä½ç½®ä¸ºä»€ä¹ˆè‡ªç„¶å‡ºç°ï¼ˆRoPE çš„å…³é”®æ€§è´¨ï¼‰

RoPE æœ€å¸¸è¢«æåˆ°çš„ä¸€ä¸ªå¥½å¤„æ˜¯ï¼š  
åœ¨æ³¨æ„åŠ›ä¸­ï¼Œå®ƒè®© **ç›¸å¯¹ä½ç½®**è‡ªåŠ¨å‡ºç°ã€‚

è®¾ $q_i,k_j\in\mathbb{R}^d$ï¼ŒRoPE åä¸º $R_i q_i$ã€$R_j k_j$ã€‚å¯¹å†…ç§¯ï¼ˆç‚¹ç§¯ï¼‰æœ‰ï¼š

$$
\langle R_i q_i,\; R_j k_j\rangle
= \langle q_i,\; R_{j-i} k_j\rangle.
$$

ç›´è§‰è§£é‡Šï¼š  
æ—‹è½¬çŸ©é˜µæ­£äº¤ï¼Œæ»¡è¶³ $R_i^\top R_j = R_{j-i}$ï¼Œäºæ˜¯è§’åº¦ä¼šâ€œç›¸å‡â€ï¼Œç›¸å¯¹ä½ç§» $j-i$ è‡ªç„¶è¿›å…¥æ³¨æ„åŠ›ç›¸ä¼¼åº¦ã€‚

---




```python
import torch.nn as nn    
import torch 
class RoPE(nn.Module):
    def __init__(self,
                 theta: float,
                 d_k:int,
                 max_seq_len:int,
                 device=None):
        super().__init__()    
        if d_k % 2 != 0:
            raise ValueError(f"d_k must be even, got {d_k}")

        self.theta = float(theta)
        self.d_k = int(d_k)
        self.max_seq_len = int(max_seq_len)
        
        #ä¸€å…±æœ‰kå¯¹ (d_k/2 ,) (0,2,4,....)
        k = torch.arange(0,d_k,2,device=device)
        #ä¸€å…±æœ‰kå¯¹ (d_k/2 ,) (0,2,4,....)        
        inv_freq = (self.theta ** (-k/d_k))
        
        pos = torch.arange(self.max_seq_len,device=device)
        
        # angles[i,k] = (i,1) * (1,k)
        angles = pos[:,None] * inv_freq[None,:]
        cos = torch.cos(angles)
        sin = torch.sin(angles)

        # Fixed buffers (not learnable, shared across layers)
        self.register_buffer("cos_cached", cos, persistent=False)
        self.register_buffer("sin_cached", sin, persistent=False)
        
    def _rotate_half(self,x:torch.Tensor) -> torch.Tensor:
        # x: (..., seq_len, d_k)
        x_even = x[..., 0::2]
        x_odd = x[..., 1::2]
        rot = torch.stack((-x_odd, x_even), dim=-1)  # (..., seq_len, d_k/2, 2)
        
        return rot.flatten(-2)                       # (..., seq_len, d_k)
        
    def forward(self,
                x:torch.Tensor,
                token_positions:torch.Tensor)-> torch.Tensor:
        # batch , seq_len
        
        '''
        (..., seq_len, 1)
        Ã—
        (1, d_k/2)
        â†“
        (..., seq_len, d_k/2)
        '''
        cos = self.cos_cached[token_positions]  # (..., seq_len, d_k/2)
        sin = self.sin_cached[token_positions]  # (..., seq_len, d_k/2)
        
        
        # cos/sin: (..., seq_len, d_k)
        
        
        cos = cos.repeat_interleave(2, dim=-1).to(dtype=x.dtype)
        sin = sin.repeat_interleave(2, dim=-1).to(dtype=x.dtype)

        # apply: x*cos + rotate(x)*sin
        return x * cos + self._rotate_half(x) * sin
```
#### å·¥ç¨‹ï¼šå®ç° ä»¥åŠä¹‹å‰æ²¡ç¢°è¿‡çš„æ­¥éª¤

##### 1. å·©å›ºï¼š
- 1. å¤–ç§¯broadcastï¼š
```python
# angles[i,k] = (i,1) * (1,k)
angles = pos[:,None] * inv_freq[None,:]
```

- 2. è·³è·ƒå–å€¼ï¼š
```python
x_even = x[..., 0::2]
x_odd = x[..., 1::2]
```

##### 2. å®¹æ˜“é”™çš„ï¼š
###### 1. torch.stack 
ç‰¢è®°æ˜¯åŠ ä¸€ä¸ªç»´åº¦ ï¼ˆ4ï¼Œï¼‰ dim = 1 å’Œ dim = -1 å®Œå…¨ä¸ä¸€æ · å‰è€…å˜æˆäº† (n,4) åƒæ˜¯å †å 
åè€…æ­¤å¤„æ¯ä¸ªå¯¹åº”ä½ç½®åŠ ä¸€å—äº† å˜æˆï¼ˆ4ï¼Œnï¼‰
([1,2,3,4],[5,6,7,8])
(2,4) æ˜¯

[[1,2,3,4],
[5,6,7,8]]

(4,2) æ˜¯ [[1,5],
          [2,6],
          [3,7],
          [4,8]]
```python
rot = torch.stack((-x_odd, x_even), dim=-1)  # (..., seq_len, d_k/2, 2)
return rot.flatten(-2)
```
###### 2. x.flatten()å’Œtorch.flatten 
torch.flatten(x, start_dim=0, end_dim=-1)
x = x.flatten(start_dim=1)

```python
rot.flatten(-2)    
```

##### 3. æ²¡ç¢°è¿‡çš„
âœ…1.torch.cos  

`è¿™æ˜¯å•¥:`  
å¯¹ tensor é‡Œçš„æ¯ä¸ªå…ƒç´ åšä½™å¼¦è¿ç®—ï¼ˆé€å…ƒç´ ï¼‰ã€‚  
`æ€ä¹ˆç”¨`
```python
cos = torch.cos(angles)
```
`å®¹æ˜“è¸©çš„å‘:`  
ä¸æ˜¯çŸ©é˜µæ“ä½œï¼Œæ˜¯element-wise è¿ç®—  
è¾“å…¥å¿…é¡»æ˜¯å¼§åº¦ï¼ˆradiantï¼‰ï¼Œä¸æ˜¯è§’åº¦ï¼ˆÂ°ï¼‰  


âœ… 2. self.register_buffer
`è¿™æ˜¯å•¥:`
æŠŠå€¼å­˜è¿›æ¨¡å‹é‡Œï¼Œä½†ä¸æ˜¯å‚æ•°ï¼ˆä¸ä¼šè®­ç»ƒæ›´æ–°ï¼‰ã€‚

`æ€ä¹ˆç”¨:`
```python
self.register_buffer("cos_cached", cos, persistent=False)
```

`è®¿é—®ï¼š`
```python
cos = self.cos_cached[token_positions]
```
`ä»€ä¹ˆæ—¶å€™ç”¨:`

âœ” æƒ³è®©å€¼è·Ÿç€æ¨¡å‹ä¸€èµ· .to(device)  
âœ” æƒ³åœ¨ .eval()ã€.train() æ—¶è¿˜èƒ½ç”¨  
âœ˜ ä½†åˆä¸æƒ³è¢«ä¼˜åŒ–å™¨æ›´æ–°  
ï¼ˆå…¸å‹ï¼šRoPE çš„ sin/cos ç¼“å­˜ï¼‰  

`å®¹æ˜“è¸©çš„å‘:`

åå­—ç”¨å­—ç¬¦ä¸²ï¼š"cos_cached"  
forward é‡Œè®¿é—®ï¼šself.cos_cached[...]  
persistent=False â†’ ä¸ä¼šè¢«å­˜è¿› checkpointï¼ˆèŠ‚çœç©ºé—´ï¼‰  

âœ… 3. repeat_interleave(2, dim=-1)  
`è¿™æ˜¯å•¥`  
æŠŠæŸä¸€ç»´åº¦çš„å…ƒç´ **â€œæŒ‰é¡ºåºé‡å¤â€**ã€‚  

`æ€ä¹ˆç”¨: `   
x = angles.repeat_interleave(2, dim=-1)    


`å‡è®¾ï¼š`
angles = [a, b, c]  
ç»“æœï¼š  
repeat_interleave â†’ [a, a, b, b, c, c]  
  
`ä»€ä¹ˆæ—¶å€™ç”¨:`  
åœ¨ RoPE é‡Œï¼Œéœ€è¦æŠŠï¼š  
[d_k/2] ç»´åº¦  
â†’ å˜æˆ  
[d_k]  

å› ä¸ºæ¯ä¸€ç»´ cos/sin è¦é…å¯¹ç»™ (even, odd) ç»´åº¦ã€‚
å®¹æ˜“è¸©çš„å‘:  
å®ƒæ˜¯é‡å¤å…ƒç´ ï¼Œä¸æ˜¯æ‰©å±• shape  
å’Œ repeat() ä¸æ˜¯ä¸€å›äº‹ï¼ˆrepeat å¤åˆ¶æ•´ä¸ªå—ï¼‰  
---
### 3.5.4.1 softmax
#### 1. Softmax çš„å®šä¹‰
ç»™å®šå‘é‡ $$ x = (x_1, x_2, \dots, x_n) $$  
softmax çš„ç›®æ ‡æ˜¯å°†ä»»æ„å®æ•°å‘é‡æ˜ å°„ä¸ºä¸€ä¸ª**æ¦‚ç‡åˆ†å¸ƒ**ï¼š

- æ¯ä¸€é¡¹ $ \in (0, 1) $
- æ‰€æœ‰é¡¹æ±‚å’Œä¸º 1

åœ¨attention çš„self attentionä¸­
softmax çš„ä½œç”¨æ˜¯ä¸ºäº†è®©attentionèƒ½åœ¨ä¸åŒçš„tokenä¸­å¾—åˆ°æ³¨æ„åŠ›çš„ç™¾åˆ†æ¯”
å…¬å¼æ˜¯
$$\text{softmax}(x_i)
= \frac{e^{x_i}}{\sum_j e^{x_j}}$$
#### 2. æ•°å€¼ä¸ç¨³å®šçš„æ ¹æº

æ•°å€¼ä¸ç¨³å®š**å¹¶ä¸æ˜¯æ¥è‡ªé™¤æ³•**ï¼Œè€Œæ˜¯æ¥è‡ªæŒ‡æ•°å‡½æ•°ï¼š

$$
e^{x}
$$

åœ¨æœ‰é™ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆå¦‚ float32ï¼‰ä¸­ï¼š

- $e^{88}  \approx 1.65 \times 10^{38} $ï¼ˆæ¥è¿‘ä¸Šé™ï¼‰
- $e^{89} \rightarrow \infty $ï¼ˆæº¢å‡ºï¼Œoverflowï¼‰
```python
x = [100, 1, -3]
exp(x) = [inf, 2.7, 0.05]
```
å› æ­¤ï¼š

ä¸€æ—¦å‡ºç° $ \infty $ï¼Œsoftmax å°±å¯èƒ½äº§ç”Ÿï¼š
- åˆ†æ¯æ˜¯ $\infty$ inf 
- $ \infty / \infty = \mathrm{NaN}$ 
- æ¢¯åº¦çˆ†ç‚¸æˆ–è®­ç»ƒå´©æºƒ

---

#### 3. Softmax çš„å¹³ç§»ä¸å˜æ€§ï¼ˆå…³é”®æ€§è´¨ï¼‰

softmax å¯¹æ‰€æœ‰åˆ†é‡åŒæ—¶åŠ ä¸Šæˆ–å‡å»ä¸€ä¸ªå¸¸æ•° **ä¸æ•æ„Ÿ**ï¼š

$$
\frac{e^{x_i}}{\sum_j e^{x_j}}
=
\frac{e^{x_i - c}}{\sum_j e^{x_j - c}}
\quad \forall c \in \mathbb{R}
$$

#### è¯æ˜

$$
\frac{e^{x_i - c}}{\sum_j e^{x_j - c}}
=
\frac{e^{-c} e^{x_i}}{e^{-c} \sum_j e^{x_j}}
=
\frac{e^{x_i}}{\sum_j e^{x_j}}
$$

å› æ­¤ï¼š

> **softmax åœ¨æ•°å­¦ä¸Šå¯¹æ•´ä½“å¹³ç§»å®Œå…¨ç­‰ä»·**

---

#### 4. ä¸ºä»€ä¹ˆé€‰æ‹©å‡å»æœ€å¤§å€¼ï¼Ÿ

ä»¤ï¼š
$$
c = \max_j x_j
$$
åˆ™å¯¹ä»»æ„ $ i $ï¼š
$$
x_i - c <= 0
$$
äºæ˜¯ï¼š
$$
e^{x_i - c} \in (0, 1]
$$

##### 5.æ•°å€¼å±‚é¢çš„ç›´æ¥åæœ

- æœ€å¤§æŒ‡æ•°å€¼ï¼š$ e^0 = 1 $
- æ°¸è¿œä¸ä¼šå‘ç”ŸæŒ‡æ•°ä¸Šæº¢ï¼ˆoverflowï¼‰
- æŒ‡æ•°ä¸‹æº¢$ e^{-1000} \approx 0 $æ˜¯å®‰å…¨çš„

> **exp çš„ä¸‹æº¢æ˜¯å®‰å…¨çš„ï¼Œä¸Šæº¢æ˜¯è‡´å‘½çš„**

---

#### 5. ä¸€ä¸ªç›´è§‚å¯¹æ¯”ç¤ºä¾‹

ç»™å®šï¼š

$x = [1000,\ 999,\ 995]$

##### ä¸åšç¨³å®šåŒ–

$e^x = [\infty, \infty, \infty]$

$\mathrm{softmax}(x) = [\mathrm{NaN},\ \mathrm{NaN},\ \mathrm{NaN}]$

##### å‡å»æœ€å¤§å€¼

$x - \max(x) = [0,\ -1,\ -5]$

$e^{x - \max(x)} = [1,\ e^{-1},\ e^{-5}]$

$\mathrm{softmax}(x) \approx [0.73,\ 0.27,\ 0.005]$

æ•°å­¦ç­‰ä»·ï¼Œä½†æ•°å€¼ç»“æœå¤©å£¤ä¹‹åˆ«ã€‚

---


#### å·¥ç¨‹å®ç°  
```python
max_x = torch.max(x, dim=dim, keepdim=True).values
exp(x - max_x)
```
è¿™é‡Œçš„ values æ˜¯å› ä¸ºtorch.max è¿”å›ä¸¤ç»„ä¸œè¥¿   
keepdim = True æ–¹ä¾¿ broadcast


---
### 3.5.4.2  ScaledDot-ProductAttention
#### å·¥ç¨‹å®ç°

##### 1. Mask çš„ä½ç½®ä¸ä½œç”¨

`å¯¹è±¡`ï¼š  
Mask **ä½œç”¨åœ¨ softmax ä¹‹å‰çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆlogitsï¼‰ä¸Š**ï¼š

$$S = \frac{QK^T}{\sqrt{d_k}}$$
è€Œä¸æ˜¯ä½œç”¨åœ¨ softmax ä¹‹åã€‚  

---
`å¦‚ä½•åœ¨å³å°†è¿›è¡Œsoftmaxçš„çŸ©é˜µé‡Œ  è¿ç”¨mask`  
å› ä¸ºsoftmaxçš„è®¡ç®—å¼
softmax å®šä¹‰ä¸ºï¼š
$$\text{softmax}(x_i)
= \frac{e^{x_i}}{\sum_j e^{x_j}}$$
å¦‚æœå¸Œæœ›æŸä¸ªä½ç½® **å®Œå…¨ä¸è¢« attend**ï¼Œå°±éœ€è¦å®ƒçš„æ¦‚ç‡ä¸º 0ï¼š

$$\
e^{-\infty} = 0
$$

å› æ­¤ï¼š
- **å…è®¸ attend çš„ä½ç½®ï¼šä¿ç•™åŸå€¼**
- **ç¦æ­¢ attend çš„ä½ç½®ï¼šè®¾ä¸º $\infty$**

---



å…ˆæ ¹æ®true or false æ¥ æˆ–è€… 1 å’Œ0  

å¯¹äºmaskä¸Š
æ˜¯trueçš„è¿ç”¨åŸæ¥çš„æ•°å€¼
æ˜¯falseçš„ä½¿ç”¨-inf ä½¿å¾—æŒ‡æ•°è¶‹å‘äº0

ç„¶åå†ä½¿ç”¨softmax ç”Ÿæˆç™¾åˆ†æ¯”çš„æ³¨æ„åŠ›å€¼

æ¥ä½¿ç”¨torch where 
ç®€å•æ¥è¯´å°±æ˜¯

```python
scores = torch.where(
    mask,
    scores,
    torch.tensor(float("-inf"), device=scores.device)
)
```

ç›´è§‚å«ä¹‰
```python
if mask[i, j]:
    scores[i, j] ä¿æŒä¸å˜
else:
    scores[i, j] = -inf

```
æœ¬è´¨æ˜¯ é€å…ƒç´ ä¸‰å…ƒè¿ç®—ç¬¦ï¼ˆternary operatorï¼‰ã€‚  

2. einsum é‡Œ è¦æ³¨æ„æ˜¯jaxtypingå“¦

3. jaxtyping éœ€è¦å•ç‹¬å¼€ä¸€ç« 

### 3.5.5 Causal Multi-Head Self-Attention


å…³äºcacheçš„å“ªé‡Œè®¿é—® å¯ä»¥ç†è§£çš„æ˜¯ä¸€ä¸ªlist
ä½†æ˜¯å¦‚æœæ˜¯ä¸€ä¸ªtensor å¤šç»´åº¦çš„å‘¢ é«˜çº§çš„indexè®¿é—®éœ€è¦
### 3.6 transformer block and language model 
adapteré‡Œåˆ†åˆ«ä¸åŒçš„ å—å¯ä»¥åˆ†ä¸€å—å—çš„load state

å…³äº å¦‚ä½•ç”Ÿæˆå¤šä¸ªblockçš„ transformer language model

ä»¥åŠtransformer block
é‚£é‡Œ

### æœ€åçš„Linear modelæ˜¯å¹²å˜›çš„

æœ€åå¾—åˆ°çš„æ˜¯ [B,seq_len,d_model]  
ç»è¿‡å…¨è¿æ¥å±‚
ä¸€ä¸ª [d_model,vocab]  
æ˜¯é€šè¿‡å¯¹äºæ¯ä¸ªd_modelçš„çº¿æ€§ç»„åˆ æ¥å¾—åˆ°vocabä¸­ å•ä¸ªçš„ç›¸ä¼¼åº¦  
æœ€åè¦é€šè¿‡softmax æ¥å¾—å‡ºæ¦‚ç‡  ç©¶ç«Ÿæ˜¯å“ªä¸€ä¸ªtoken
æ‰€ä»¥transformerä¸­ qkvä¹‹å æ¯ä¸€è¡Œçš„d_model éƒ½æ˜¯å¯¹äºä¸‹ä¸€ä¸ªtokençš„d_modelçš„é¢„æµ‹ 



# 4. è®¡ç®—flopsæ€ä¹ˆç®—
## 1. çŸ©é˜µä¹˜æ³•çš„å®šä¹‰
æ‹¿æœ€ç®€å•çš„ çº¿æ€§å±‚Linearæ¥è¯´
æ¯ä¸ªå…ƒç´ 
ç»™å®šï¼š
$$
A \in \mathbb{R}^{m\times k}
$$
$$
B \in \mathbb{R}^{k\times n}
$$
çŸ©é˜µä¹˜æ³• (C=AB) çš„å…ƒç´ çº§å®šä¹‰æ˜¯ï¼š
$$ C_{ij}=\sum_{t=1}^{k} A_{i t}, B_{t j},$$
$$ \quad i=1,\dots,m,   j=1,\dots,n. $$

### 2.1 ä¹˜æ³•æ¬¡æ•°ï¼ˆmultiplicationsï¼‰
æ¯ä¸€é¡¹ $A_{it} ,B_{tj}$ éƒ½éœ€è¦ 1 æ¬¡ä¹˜æ³•ï¼Œå…±æœ‰ k é¡¹ï¼š

$$\text{mult} = k$$

### 2.2 åŠ æ³•æ¬¡æ•°ï¼ˆadditionsï¼‰
æŠŠ k ä¸ªä¹˜ç§¯åŠ èµ·æ¥ï¼š

ä¸¤ä¸ªæ•°ç›¸åŠ éœ€è¦ 1 æ¬¡åŠ æ³•ï¼›
(k) ä¸ªæ•°ç›¸åŠ éœ€è¦ k-1 æ¬¡åŠ æ³•ï¼ˆå› ä¸ºæ¯å¤šåŠ ä¸€ä¸ªæ•°ï¼Œæ‰éœ€è¦ä¸€æ¬¡åŠ æ³•ï¼‰ã€‚
å› æ­¤ï¼š
$$\text{add} = k-1$$


### 2.3 æ•´ä¸ªçŸ©é˜µæœ‰å¤šå°‘ä¸ªå…ƒç´  
cæ˜¯m*n ä¸€å…±å°±æ˜¯
$$m * n * (2k-1) = 2k(-1)mn = 2mkn -mn$$

æ‰€ä»¥ä¸€ä¸ª(m,k) (k,n)çš„çŸ©é˜µè¿ç®—è¦â‰ˆ 2mknçš„è®¡ç®—æ¬¡æ•°

## æ¯ä¸€ä¸ªmoduleçš„flops orè®¡ç®—æ¨å¯¼

### 1 embeddingå±‚ 
å¯¹æ¯ä¸ªä½ç½® 

(b,t)ï¼Œæ‹¿åˆ°ä¸€ä¸ª token idï¼šid=in_indices[b,t]ï¼Œç„¶åï¼š
x[b,t,:]=E[id,:]
ä¹Ÿå°±æ˜¯ ä» E é‡Œæ‹·è´ä¸€æ•´è¡Œé•¿åº¦ D çš„å‘é‡ã€‚

#### 1.1 è¿™ç®— FLOPs å—ï¼Ÿ  

ä¸ç®—ã€‚

å› ä¸ºè¿™é‡Œæ²¡æœ‰ï¼š  
æµ®ç‚¹ä¹˜æ³• *  
æµ®ç‚¹åŠ æ³• +  
æµ®ç‚¹é™¤æ³• /  
sqrtã€exp ç­‰  
å®ƒæ˜¯ ç´¢å¼• + å†…å­˜æ‹·è´ã€‚
æ‰€ä»¥ embedding çš„ FLOPsï¼š
0
 FLOPs
0 FLOPs
	â€‹
ä½†å®ƒä¸æ˜¯â€œæ²¡æˆæœ¬â€
å®ƒçš„æˆæœ¬ä¸»è¦æ˜¯ å†…å­˜è¯»ï¼šä¼šè¯»å‡º 

Bâ‹…Tâ‹…D ä¸ª floatï¼ˆä»¥åŠå†™åŒæ ·å¤šåˆ°è¾“å‡ºï¼‰ã€‚

å¦‚æœè¦åšâ€œæ—¶é—´/å¸¦å®½ä¼°ç®—â€ï¼Œembedding æ˜¯è¦ç®— bytes çš„ï¼Œä¸æ˜¯ç®— FLOPs

### 2 RMSNormå±‚
#### 2.1è®°å¿†æ–¹æ³•  
1. RMSN root mean squan norm  
2.1 root å³${^{1/2}}$ å¼€æ ¹å·  
2.2 mean square æ˜¯å¯¹æ¯ä¸ªå…ƒç´ å¹³æ–¹æ±‚å’Œ å é™¤ä»¥d_model
2.3 normå½’ä¸€åŒ– æŒ‡çš„æ˜¯ rmsï¼ˆ$a_i$ï¼‰ æ˜¯ç”¨æ¥é™¤æ¯ä¸ª$a_i$çš„  

#### 2.2 å¯¹äºå•ä¸ªtoken D  çš„è®¡ç®—æ¬¡æ•°
å¹³æ–¹ç®—square:é€å…ƒç´ â†’Dæ¬¡  
æ±‚å’Œâ†’D-1æ¬¡  
å¹³æ–¹é™¤ä»¥d_modelâ†’1
åŠ eps ä¸€æ¬¡â†’1  
sqrt ä¸€æ¬¡â†’1  
å½’ä¸€åŒ–norm Dæ¬¡é™¤æ³• â†’ D
ä¹˜gain D æ¬¡ä¹˜æ³• â†’ D
åˆè®¡4D+2

å…¨éƒ¨ token BT(4D+2)

ç»™å®šä¸€ä¸ªæ¿€æ´»å‘é‡ï¼š
$$
\mathbf{a} = (a_1, a_2, \dots, a_{d_{\text{model}}})
\in \mathbb{R}^{d_{\text{model}}}
$$

RMSNorm will rescale each activation ai as follows

$$
\mathrm{RMSNorm}(a_i)
=
\frac{a_i}{\mathrm{RMS}(\mathbf{a})}
\cdot g_i
$$
where RMS(a)
$$
\mathrm{RMS}(\mathbf{a})
=
\sqrt{
\frac{1}{d_{\text{model}}}
\sum_{i=1}^{d_{\text{model}}} a_i^2
+ \varepsilon
}
$$

### 3. SwiGLU_feed_forward  

#### 3.1 è®°å¿†æ–¹æ³•  
1. FFN Feed Forward NetWork
å¯¹æ¯ä¸ª token ç‹¬ç«‹ åšéçº¿æ€§å˜æ¢
ä¸å‘ç”Ÿ tokenâ€“token äº¤äº’

2. GLUï¼ˆGated Linear Unitï¼‰
ä¸¤ä¸ªçº¿æ€§æŠ•å½±
ä¸€ä¸ªä½œä¸º gate,ä¸€ä¸ªä½œä¸º value
é€šè¿‡é€å…ƒç´ ä¹˜æ³•è¿›è¡Œ gating

3. SwiGLU
å°† SiLUï¼ˆSwishï¼‰å’Œ GLU ç»“åˆï¼Œå°±å¾—åˆ°äº† SwiGLU
$$
\mathrm{FFN}(x)
= \mathrm{SwiGLU}(x, W_1, W_2, W_3)
= W_2 \left( \mathrm{SiLU}(W_1 x) \odot (W_3 x) \right)
$$



1ï¸âƒ£ SiLU / Swish æ¿€æ´»å‡½æ•°

SiLUï¼ˆæˆ– Swishï¼‰æ¿€æ´»å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
$$
\mathrm{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

å®ƒä¸ ReLU ç±»ä¼¼ï¼Œä½†åœ¨ 0 é™„è¿‘æ˜¯ å¹³æ»‘çš„ï¼ˆsmoothï¼‰ï¼Œè€Œä¸æ˜¯æœ‰æŠ˜ç‚¹ã€‚  

2ï¸âƒ£ Gated Linear Unitï¼ˆGLUï¼‰  
å…¶å®šä¹‰ä¸ºï¼š  
ä¸€ä¸ªçº¿æ€§å˜æ¢ç»è¿‡ sigmoid åï¼Œä¸å¦ä¸€ä¸ªçº¿æ€§å˜æ¢è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
$$
\mathrm{GLU}(x, W_1, W_2)
= \sigma(W_1 x) \odot (W_2 x)
$$

GLU è¢«è®¤ä¸ºå¯ä»¥ï¼š  
é€šè¿‡æä¾›ä¸€æ¡â€œçº¿æ€§æ¢¯åº¦é€šè·¯â€ï¼Œåœ¨ä¿ç•™éçº¿æ€§èƒ½åŠ›çš„åŒæ—¶ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

3ï¸âƒ£ SwiGLUï¼šSiLU + GLU çš„ç»„åˆ

å°† SiLUï¼ˆSwishï¼‰å’Œ GLU ç»“åˆï¼Œå°±å¾—åˆ°äº† SwiGLU
$$
\mathrm{FFN}(x)
= \mathrm{SwiGLU}(x, W_1, W_2, W_3)
= W_2 \left( \mathrm{SiLU}(W_1 x) \odot (W_3 x) \right)
$$
$$
= W_2 \left( \mathrm{\frac{W_1 x}{1 + e^{-W_1 x}}} \odot (W_3 x) \right)
$$

### 3. Multi-Head Attentionï¼ˆå« RoPEï¼‰ FLOPs
3.1 è®°å¿†æ–¹æ³•ï¼ˆä»åå­—æ‹†ï¼‰

Multi-Headï¼š  
æŠŠ ğ‘‘_model åˆ‡æˆ H ä¸ª head  

Self-Attentionï¼š  
QKáµ€ï¼štokenâ€“token ç›¸ä¼¼åº¦  
*Vï¼šæŒ‰æƒé‡èšåˆä¿¡æ¯  

with RoPEï¼š  
å¯¹ Qã€K çš„æ¯ä¸€å¯¹ç»´åº¦åšäºŒç»´æ—‹è½¬  
ä¸å¼•å…¥ token é—´é¢å¤–è®¡ç®—ï¼Œåªæ˜¯é€å…ƒç´ ç®—æœ¯  

### è¿˜è¦ç®—å¥½å¤šä¹‹åå†ç»§ç»­

# 5 training a transformer LM è®­ç»ƒä¸€ä¸ªtransformeræ¨¡å‹

## 5.1 CrossEntropy loss
### 5.1.1å…ˆä¸è°ˆæ¨¡å‹ï¼Œåªè°ˆâ€œé¢„æµ‹ä¸€ä»¶äº‹â€
å‡è®¾æˆ‘é—®ä½ ä¸€ä¸ªé—®é¢˜ï¼š
â€œæ˜å¤©ä¸‹é›¨å—ï¼Ÿâ€

ä½ ç»™æˆ‘ä¸€ä¸ªæ¦‚ç‡ã€‚

ä½ ç»™çš„æ¦‚ç‡	æˆ‘çš„æ„Ÿè§‰  
0.9	å¾ˆæœ‰ä¿¡å¿ƒ  
0.6	æœ‰ç‚¹çŠ¹è±«  
0.1	å‡ ä¹ä¸ä¿¡  

ç°åœ¨å‡è®¾ï¼šçœŸå®ç­”æ¡ˆæ˜¯ã€Œä¼šä¸‹é›¨ã€  
é‚£æˆ‘æ€ä¹ˆè¯„ä»·ä½ è¿™æ¬¡é¢„æµ‹ï¼Ÿ  

### 5.1.2 ä¸€ä¸ªâ€œåˆç†çš„æƒ©ç½šå‡½æ•°â€åº”è¯¥æ»¡è¶³ä»€ä¹ˆï¼Ÿ  

æˆ‘ä»¬è¦è®¾è®¡ä¸€ä¸ª loss(prob)ï¼Œå®ƒè‡³å°‘åº”è¯¥æ»¡è¶³ï¼š 
1. æ¦‚ç‡è¶Šå¤§ï¼Œæƒ©ç½šè¶Šå°  
è¿™æ˜¯æœ€åŸºæœ¬çš„ã€‚

2. æ¦‚ç‡ â†’ 1ï¼Œæƒ©ç½š â†’ 0  
ä½ å®Œå…¨é¢„æµ‹å¯¹äº†ï¼Œä¸è¯¥å†ç½šä½ ã€‚  

3. æ¦‚ç‡ â†’ 0ï¼Œæƒ©ç½š â†’ âˆ  
ä½ éå¸¸è‡ªä¿¡åœ°é¢„æµ‹é”™äº†ï¼Œå¿…é¡»ç‹ ç‹ ç½šã€‚  

### 5.1.3 ç°åœ¨çœ‹ âˆ’log(p)ï¼ˆå…³é”®æ¥äº†ï¼‰
| pï¼ˆé¢„æµ‹æ­£ç¡®çš„æ¦‚ç‡ï¼‰ | log(p) | âˆ’log(p) |
| ---------- | ------ | ------- |
| 1.0        | 0      | 0       |
| 0.9        | âˆ’0.105 | 0.105   |
| 0.5        | âˆ’0.693 | 0.693   |
| 0.1        | âˆ’2.30  | 2.30    |
| 0.01       | âˆ’4.60  | 4.60    |


âœ… æ€§è´¨ 1ï¼šp â†’ 1ï¼Œloss â†’ 0   

å®Œå…¨ç¬¦åˆç›´è§‰  

âœ… æ€§è´¨ 2ï¼šp å¾ˆå°ï¼Œloss æ€¥å‰§å˜å¤§   

â€œä½ æ€ä¹ˆæ•¢è¿™ä¹ˆè‡ªä¿¡è¿˜é”™ï¼Ÿâ€
  
âœ… æ€§è´¨ 3ï¼šå·®è·æ˜¯â€œæˆå€â€çš„   

0.01 æ¯” 0.1 ä¸æ˜¯å·® 0.09ï¼Œè€Œæ˜¯å·®ä¸€ä¸ªæ•°é‡çº§   
è¿™ç‚¹å¯¹è¯­è¨€æ¨¡å‹æå…¶é‡è¦ã€‚

### 5.1.4 ç°åœ¨å›åˆ°è¯­è¨€æ¨¡å‹ï¼šå®ƒåœ¨å¹²å˜›ï¼Ÿ

åœ¨ LM é‡Œï¼š

æ¨¡å‹ä¸æ˜¯å›ç­”ã€Œä¸‹é›¨å—ã€
è€Œæ˜¯ï¼š
â€œä¸‹ä¸€ä¸ª token æ˜¯ vocab é‡Œå“ªä¸€ä¸ªï¼Ÿâ€

è¿™æ˜¯ä¸€ä¸ª categorical distribution

### 5.1.5 çœŸå®æ ‡ç­¾å…¶å®æ˜¯ä¸€ä¸ªâ€œæç«¯åˆ†å¸ƒâ€

çœŸå®çš„ä¸‹ä¸€ä¸ª token æ˜¯ç¡®å®šçš„ï¼Œæ¯”å¦‚ï¼š

è¯è¡¨ = [cat, dog, apple, ...]
çœŸå®ç­”æ¡ˆ = dog

çœŸå®åˆ†å¸ƒæ˜¯ï¼š
ä¸€ä¸ª one-hot å‘é‡ï¼š

$$
\mathbf{y} = (0, \dots, 1, \dots, 0)
$$

å…¶ä¸­åªæœ‰ç¬¬ $y$ é¡¹ä¸º 1

---
### 5.1.6 Cross-Entropy åœ¨è¿™é‡Œé€€åŒ–æˆä»€ä¹ˆï¼Ÿ

äº¤å‰ç†µå®šä¹‰ä¸ºï¼š

$$
H(\mathbf{y}, \mathbf{p}) = -\sum_{k=1}^{V} y_k \log p_k
$$

ç”±äº $\mathbf{y}$ æ˜¯ one-hotï¼š

$$
\ell = -\log p_y
$$

> **è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼š  
loss åªæ˜¾å¼åœ°çœ‹â€œæ­£ç¡® token çš„æ¦‚ç‡â€ã€‚**
ä½†å…¶å®å› ä¸º softmaxçš„å…³ç³» å…¶ä»–é¡¹ä¹Ÿå·²ç»æƒ©ç½šå®Œäº†
---

### 5.1.7 æŠŠ softmax ä»£å…¥ lossï¼ˆä¸€æ­¥ä¸è·³ï¼‰

æˆ‘ä»¬ä»æœ€åŸå§‹çš„å®šä¹‰å¼€å§‹ï¼š

#### Step 1ï¼šå†™å‡º loss

$$
\ell = -\log p_y
$$

---

#### Step 2ï¼šä»£å…¥ softmax ä¸­çš„ $p_y$

$$
\ell = -\log \left(
\frac{e^{o_y}}{\sum_{k=1}^{V} e^{o_k}}
\right)
$$

---

#### Step 3ï¼šä½¿ç”¨å¯¹æ•°çš„åŸºæœ¬æ€§è´¨

$$
\log \frac{a}{b} = \log a - \log b
$$

äºæ˜¯ï¼š

$$
\ell
= -\left(
\log e^{o_y}
- \log \sum_{k} e^{o_k}
\right)
$$

---

#### Step 4ï¼šä½¿ç”¨ $\log e^x = x$

$$
\ell
= -\left(
o_y
- \log \sum_{k} e^{o_k}
\right)
$$
å³

$$
{
\ell = -o_y + \log \sum_{k=1}^{V} e^{o_k}
}
$$
---


### 5.1.8 è¿™ä¸ªå…¬å¼åœ¨â€œæƒ©ç½šä»€ä¹ˆâ€ï¼Ÿ

æˆ‘ä»¬æŠŠ loss æ‹†æˆä¸¤éƒ¨åˆ†ï¼š

$$
\ell = \underbrace{-o_y}_{\text{å¥–åŠ±æ­£ç¡® token}}
\quad + \quad
\underbrace{\log \sum_k e^{o_k}}_{\text{æƒ©ç½šæ‰€æœ‰ logits}}
$$

å› ä¸ºloss è¶Šå°è¶Šå¥½ æ‰€ä»¥`å‡`å¤§æ­£æ•° æˆ–è€…ä¿è¯`åŠ `å°æ­£æ•°

---

#### 5.1.8.1 ç¬¬ä¸€é¡¹ï¼š$-o_y$

- æ­£ç¡® token çš„ logit è¶Šå¤§
- loss è¶Šå°

ğŸ‘‰ **é¼“åŠ±æ¨¡å‹æŠŠæ­£ç¡® token çš„ logit æ‹‰é«˜**

---

#### 5.1.8.1 ç¬¬äºŒé¡¹ï¼š$\log \sum_k e^{o_k}$ï¼ˆå…³é”®ï¼‰

è¿™ä¸€é¡¹åŒ…å«ï¼š

- æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬é”™è¯¯ tokenï¼‰
- å¹¶ä¸”æ˜¯ **exp å† sum æœ€ålog**

æ€§è´¨ï¼š

- æŸä¸ªé”™è¯¯ token çš„ logit å¦‚æœå¾ˆå¤§
- $e^{o_k}$ ä¼šæŒ‡æ•°çº§æ”¾å¤§
- æ•´ä¸ª loss ä¼šè¢«æ‹‰é«˜

ğŸ‘‰ **è¿™ä¸€æ­¥å·²ç»åœ¨â€œæƒ©ç½šç»™é”™è¯¯ token è¿‡é«˜æ¦‚ç‡â€**

---

### 5.1.9 ä¸€ä¸ªéå¸¸é‡è¦çš„äº‹å®ï¼ˆsoftmax æ˜¯é›¶å’Œåšå¼ˆï¼‰

Softmax çš„æ¦‚ç‡å¿…é¡»æ»¡è¶³ï¼š

$$
\sum_k p_k = 1
$$

å› æ­¤ï¼š

> **å¦‚æœä½ æŠŠæ¦‚ç‡ç»™é”™ token ç»™é«˜äº†ï¼Œ  
æ­£ç¡® token çš„æ¦‚ç‡ä¸€å®šä¼šè¢«å‹ä½ã€‚**

è¿™ä¸¤ä»¶äº‹åœ¨æ•°å­¦ä¸Šæ˜¯ç»‘å®šçš„ã€‚
### å®ç°
ä¸Šè¿°ç»™å‡ºæ¥çš„æµç¨‹
```python
for each batch:
  for each position i:
    logits = o_i
    probs = softmax(logits)
    loss_i = -log(probs[target_i])
final_loss = mean(loss_i)

```
è€Œpdfä¸­åœ¨å¹²ä»€ä¹ˆå‘¢
è®²ä¹‰ç¬¬ä¸€å¥è¯å°±å·²ç»åœ¨åšä¸€ä»¶ä½ æ²¡æ˜¾å¼åšçš„äº‹ï¼š

the Transformer language model defines a distribution

è¿™å¥è¯ä¸æ˜¯åºŸè¯ï¼Œå®ƒåœ¨åšä¸€ä¸ªæ•°å­¦å»ºæ¨¡å£°æ˜ï¼š
$pÎ¸â€‹(xi+1â€‹âˆ£x1:iâ€‹)$  

æ¨¡å‹ä¸æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œ  
è€Œæ˜¯ä¸€ä¸ªâ€œå¸¦å‚æ•°çš„æ¦‚ç‡åˆ†å¸ƒâ€
åœ¨è®²ä¹‰çš„è§†è§’é‡Œï¼š
x_{i+1}  

æ˜¯ä¸€ä¸ª éšæœºå˜é‡

å®ƒçš„åˆ†å¸ƒæ˜¯ï¼š

xi+1â€‹âˆ¼pÎ¸â€‹(â‹…âˆ£x1:iâ€‹)

$â„“(Î¸;D)=âˆ£Dâˆ£m1â€‹xâˆˆDâˆ‘â€‹i=1âˆ‘mâ€‹âˆ’logpÎ¸â€‹(xi+1â€‹âˆ£x1:iâ€‹)$

## 5.2 SGD optimizer

1. æ ¸å¿ƒæ¦‚å¿µï¼šä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿæƒ³è±¡ä½ åœ¨ä¸€ä¸ªèµ·ä¼çš„å±±è°·ä¸­ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä½ç‚¹ï¼ˆå³ Loss æŸå¤±å‡½æ•°çš„æœ€å°å€¼ï¼‰ã€‚æ¢¯åº¦çš„æœ¬è´¨åœ¨å¾®ç§¯åˆ†ä¸­ï¼Œæ¢¯åº¦ $\nabla f$ æŒ‡å‘çš„æ˜¯å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬è¦æ‰¾æœ€å°å€¼ï¼Œè‡ªç„¶è¦å¾€æ¢¯åº¦çš„åæ–¹å‘èµ°ã€‚ \

æƒ³è±¡ä¸€ä¸‹ 
å¯¼æ•°å¦‚æœæ˜¯æ­£çš„ é‚£ä¹ˆå‘æ­£æ–¹å‘èµ° å‡½æ•°å€¼ä¸Šå‡
å¯¼æ•°å¦‚æœæ˜¯è´Ÿçš„ é‚£ä¹ˆå‘è´Ÿæ–¹å‘èµ° å‡½æ•°å€¼ä¸Šå‡

æ›´æ–°å…¬å¼:  
$$w_{new} = w_{old} - \eta \cdot \nabla L(w)$$

$w$: æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰ã€‚  
$\eta$ (Learning Rate): æ­¥é•¿ï¼Œå†³å®šä½ æ¯ä¸€æ­¥è·¨å¤šå¤§ã€‚  
$\nabla L(w)$: æ¢¯åº¦ï¼Œå†³å®šä½ å¾€å“ªèµ°ã€‚  

### PyTorch ä¸­optimizerçš„å®ç°åŸç†

PyTorch çš„æ‰€æœ‰ä¼˜åŒ–å™¨éƒ½ç»§æ‰¿è‡ª 
`torch.optim.Optimizerã€‚`  


```python
from collections.abc import Callable, Iterable
from typing import Optional
import torch
import math

class SGD(torch.optim.Optimizer):
    def __init__(self, params, lr=1e-3):
        if lr < 0:
            raise ValueError(f"Invalid learning rate: {lr}")
        defaults = {"lr": lr}
        super().__init__(params, defaults)
        '''
        5. æ˜¯è¿™é‡Œé€šè¿‡paramså’Œdefaults æ¥ç”Ÿæˆçš„para_groupsçš„
        '''

    def step(self, closure: Optional[Callable] = None):
        loss = None if closure is None else closure()

        '''
        4. group["params"] æ˜¯å­˜åœ¨self.para_groupsé‡Œ 
        '''
        for group in self.param_groups:
            lr = group["lr"] # Get the learning rate.
            
            '''
            3. ä½†æ˜¯ påˆæ˜¯å“ªé‡Œæ¥çš„å‘¢  æ˜¯åœ¨ group["params"] 
            '''
            for p in group["params"]:
                if p.grad is None:
                    continue
            
                state = self.state[p] # Get state associated with p.
                t = state.get("t", 0) # Get iteration number from the state, or initial value.
                
                '''
                2. å¦‚ä½•è·å–æ¢¯åº¦å‘¢ å…¶å®æ˜¯åœ¨ è¿™ä¸€æ­¥ grad åœ¨ p.grada.dataé‡Œ 
                '''
                grad = p.grad.data # Get the gradient of loss with respect to p.
                
                '''
                1. ç”¨æ¢¯åº¦æ›´æ–°çš„é‚£ä¸€æ­¥åœ¨æ­¤ p.dataæ˜¯å‚æ•°æœ¬èº« 
                '''
                p.data-= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.
                state["t"] = t + 1 # Increment iteration number.
                return loss

```



```python

weights = torch.nn.Parameter(5 * torch.randn((10, 10)))
opt = SGD([weights], lr=1)

for t in range(100):
    opt.zero_grad() # Reset the gradients for all learnable parameters.
    loss = (weights**2).mean() # Compute a scalar loss value.
    print(loss.cpu().item())
    loss.backward() # Run backward pass, which computes gradients.
    opt.step() # Run optimizer step.
```

æ­¤å¤„å¯è§è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰å‡ ç‚¹
1. optimzer ä¼˜åŒ–å™¨é‡Œ éœ€è¦ä½¿ç”¨zero_gradé¿å…æ¢¯åº¦ç´¯ç§¯  
2. åé¢çš„loss æŸå¤±å‡½æ•° åç»­éœ€è¦backward() æˆ‘çš„ç†è§£ä¸ºæ›´æ–°   
æŠŠæ‰€æœ‰gradientä¼ å›å»  ä¼ åˆ°parameteré‡Œçš„gradé‡Œ å¯ä»¥ä»ä¸Šé¢çš„2é‡Œæ¨æµ‹å‡ºæ¥
3. opt èµ°stepçš„ä¸€æ­¥    




## 5.3 AdamW 
### é¦–å…ˆ å…³äº åœ¨ torch.optim.Optimizerä¸­çš„subclassä¸­ initåœ¨å¹²å˜›
```python
def __init__(self, params, lr=1e-3):
if lr < 0:
raise ValueError(f"Invalid learning rate: {lr}")
defaults = {"lr": lr}
super().__init__(params, defaults)
```
### super().__init__(params, defaults) å†…éƒ¨çœŸæ­£å‘ç”Ÿçš„äº‹

æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥ã€‚

1ï¸âƒ£ ä½ ä¼ è¿›æ¥çš„æ˜¯ä»€ä¹ˆï¼Ÿ

```python
params = model.parameters()
defaults = {"lr": lr}
```

`params` æ˜¯ï¼š  
ä¸€ä¸ª iterable
é‡Œé¢æ˜¯ torch.nn.Parameter å¯¹è±¡

`defaults` æ˜¯ï¼š  
ä¸€å¥—â€œè¿™ä¸€ç»„å‚æ•°çš„é»˜è®¤è¶…å‚æ•°â€

2ï¸âƒ£ Optimizer åŸºç±»åšçš„ç¬¬ä¸€ä»¶äº‹

PyTorch ä¼šæ£€æŸ¥ params çš„å½¢æ€ï¼š

æƒ…å†µ Aï¼ˆæœ€å¸¸è§ï¼‰
```python
params = [p1, p2, p3, ...]
```


â†’ è¿™æ˜¯ â€œå•ä¸€å‚æ•°ç»„â€

æƒ…å†µ Bï¼ˆä½ æ‰‹åŠ¨åˆ†ç»„ï¼‰
```python
params = [
    {"params": [...], "lr": 1e-3},
    {"params": [...], "lr": 3e-4},
]
```

â†’ è¿™æ˜¯ â€œå¤šå‚æ•°ç»„â€

3ï¸âƒ£ å¯¹æƒ…å†µ Aï¼šPyTorch è‡ªåŠ¨å¸®ä½ åŒ…ä¸€å±‚

å¦‚æœä½ ä¼ çš„æ˜¯æƒ…å†µ Aï¼Œå†…éƒ¨ä¼šç­‰ä»·åœ°å˜æˆï¼š
```python
self.param_groups = [
    {
        "params": list(params),
        **defaults
    }
]
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ è„‘å­é‡Œæƒ³çš„è¿™ä¸€è¡Œï¼š
```python
{
    "params": [p1, p2, p3, ...],
    "lr": lr
}
```

æ˜¯çœŸçš„å­˜åœ¨çš„ã€‚

ä¸‰ã€å¦‚æœ defaults é‡Œå¤šå†™ç‚¹ä¸œè¥¿ä¼šæ€æ ·ï¼Ÿ

ä½ è¿™é‡Œå†™çš„æ˜¯ï¼š
```python
defaults = {
    "lr": lr,
    "betas": betas,
    "eps": eps,
    "weight_decay": weight_decay,
}
```

é‚£ä¹ˆæœ€ç»ˆï¼š
```python
self.param_groups = [
    {
        "params": [p1, p2, p3, ...],
        "lr": lr,
        "betas": betas,
        "eps": eps,
        "weight_decay": weight_decay,
    }
]
```

âœ”ï¸ å®Œå…¨æ­£ç¡®
âœ”ï¸ PyTorch ä¸ä¼šâ€œåƒæ‰â€ä»»ä½• defaults é‡Œçš„é”®
âœ”ï¸ å®ƒä»¬å°±æ˜¯ç»™ step() ç”¨çš„

å››ã€å…³é”®ç»†èŠ‚ï¼šdefaults â‰  å¼ºåˆ¶å€¼

è¿™æ˜¯ä¸€ä¸ªéå¸¸å®¹æ˜“å¿½ç•¥ã€ä½†ä½ å·²ç»ç«™åœ¨èƒ½ç†è§£å®ƒçš„å±‚çº§äº†çš„ç‚¹ï¼š

defaults åªæ˜¯â€œé»˜è®¤å€¼â€

å¦‚æœä½ ä¼ çš„æ˜¯å¤šå‚æ•°ç»„ï¼š
```python
params = [
    {"params": group1, "lr": 1e-3},
    {"params": group2, "lr": 1e-4},
]
```

é‚£ä¹ˆï¼š

group1 ç”¨è‡ªå·±çš„ "lr"
group2 ç”¨è‡ªå·±çš„ "lr"

åªæœ‰æ²¡å†™çš„é”®ï¼Œæ‰ä¼šä» defaults é‡Œè¡¥

äº”ã€ä¸ºä»€ä¹ˆ Optimizer è¦è¿™æ ·è®¾è®¡ï¼Ÿ

è¿™ä¸€è®¾è®¡è®©ä¸‰ä»¶äº‹åŒæ—¶æˆç«‹ï¼š

1ï¸âƒ£ æœ€ç®€å•ç”¨æ³•
```python
AdamW(model.parameters(), lr=1e-3)
```

2ï¸âƒ£ å·¥ç¨‹çº§æ§åˆ¶
```python
AdamW([
    {"params": embed, "lr": 1e-3},
    {"params": blocks, "lr": 3e-4},
])
```

3ï¸âƒ£ Optimizer å†…éƒ¨å®Œå…¨é€šç”¨
```python
for group in self.param_groups:
    lr = group["lr"]
```

ğŸ‘‰ Optimizer ä¸éœ€è¦çŸ¥é“ layer çš„æ¦‚å¿µ

### é‚£ä¹ˆ paramsé‡Œåˆ°åº•ä»€ä¹ˆ å¯¹äºoptimzeræ¥è¯´
1ï¸âƒ£ ä»€ä¹ˆæ˜¯ torch.nn.Parameter  
`group["params"]` æ˜¯ä¸€ç»„ torch.nn.Parameter å¯¹è±¡çš„ list
å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª Tensorï¼Œä½†å¤šäº†ä¸¤ç‚¹ï¼š  
1. requires_grad=True
2. è¢« nn.Module æ³¨å†Œï¼ˆä¼šå‡ºç°åœ¨ model.parameters() é‡Œï¼‰

2ï¸âƒ£ group["params"] çš„æ¥æº

å½“ä½ å†™ï¼š

```python
{"params": model.blocks.parameters()}
```


PyTorch ä¼šåœ¨ Optimizer åˆå§‹åŒ–æ—¶ï¼š
```python
list(model.blocks.parameters())
```

æ‰€ä»¥ï¼š
```python
group["params"] == [
    block0.attn.q_proj.weight,
    block0.attn.q_proj.bias,
    block0.attn.k_proj.weight,
    ...
    blockN.ffn.w2.weight,
]
```

ğŸ‘‰ å®ƒæ˜¯â€œæ‹æ‰åçš„æ‰€æœ‰ Parameterâ€
  
ç„¶åæ­¤å¤„çš„`state` æ˜¯å¯¹äº è¿™æ ·å­ä¸€ä¸ªLinearå±‚çš„state  
linear.weightï¼šä¸€å¼  512Ã—1024 çš„è¡¨
state[p]["m"]ï¼šä¸€å¼ ä¸€æ¨¡ä¸€æ ·å¤§å°çš„è¡¨
state[p]["v"]ï¼šä¸€å¼ ä¸€æ¨¡ä¸€æ ·å¤§å°çš„è¡¨

q:é‚£ä¹ˆstateæœ¬èº«æ˜¯æ€æ ·çš„æ•°æ®ç»“æ„å‘¢   
a:æ˜¯ä¸€ä¸ªå­—å…¸ defaultdict
```python
state = {
    p1: {...},
    p2: {...},
    p3: {...},
}

state[p] = {
    "t": int,
    "m": Tensor(shape == p.shape),
    "v": Tensor(shape == p.shape),
}
```
torch.zeros_like(p)   
ä¼šåˆ›å»ºä¸€ä¸ªï¼š  
1. shape å’Œ p å®Œå…¨ä¸€æ ·
2. dtype å’Œ p å®Œå…¨ä¸€æ ·
3. device å’Œ p å®Œå…¨ä¸€æ ·

æ­¥éª¤,æ•°å­¦å…¬å¼ (Algorithm 1),
1. è®¡ç®—æ¢¯åº¦,gâ†âˆ‡Î¸â€‹â„“(Î¸;Btâ€‹),g = p.grad,è·å– Autograd å¼•æ“è®¡ç®—å¥½çš„æ¢¯åº¦ã€‚
2. æ›´æ–°ä¸€é˜¶çŸ©,mâ†Î²1â€‹m+(1âˆ’Î²1â€‹)g,"m.mul_(beta1).add_(g, alpha=1-beta1)",åŠ¨é‡éƒ¨åˆ†ï¼šå¹³æ»‘æ¢¯åº¦ï¼Œå†³å®šæ›´æ–°çš„æ–¹å‘ã€‚
3. æ›´æ–°äºŒé˜¶çŸ©,vâ†Î²2â€‹v+(1âˆ’Î²2â€‹)g2,"v.mul_(beta2).addcmul_(g, g, value=1-beta2)",ç¼©æ”¾éƒ¨åˆ†ï¼šè®¡ç®—æ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡ï¼Œå†³å®šæ­¥é•¿çš„ç¼©æ”¾ã€‚
4. åå·®ä¿®æ­£,Î±tâ€‹â†Î±1âˆ’Î²1tâ€‹1âˆ’Î²2tâ€‹â€‹â€‹,bc1 = 1 - beta1 ** stepbc2 = 1 - beta2 ** stepstep_size = lr * math.sqrt(bc2) / bc1,"ä¿®æ­£ m,v åœ¨è®­ç»ƒåˆæœŸå‘ 0 åç§»çš„é—®é¢˜ã€‚"
5. å‚æ•°æ›´æ–°,Î¸â†Î¸âˆ’Î±tâ€‹vâ€‹+Ïµmâ€‹,"denom = v.sqrt().add_(eps)p.addcdiv_(m, denom, value=-step_size)",è‡ªé€‚åº”æ›´æ–°ï¼šå°†åŠ¨é‡é™¤ä»¥äºŒé˜¶çŸ©çš„å¹³æ–¹æ ¹ï¼Œå®ç°å› åœ°åˆ¶å®œçš„æ›´æ–°ã€‚
6. æƒé‡è¡°å‡,Î¸â†Î¸âˆ’Î±Î»Î¸,"p.add_(p, alpha=-lr * weight_decay)",è§£è€¦è¡°å‡ï¼šç›´æ¥è®©å‚æ•°å€¼å¾€ 0 é æ‹¢ï¼Œä¸ç»è¿‡æ¢¯åº¦çš„ç¼©æ”¾ã€‚


### åŸå­çº§åˆ«æ“ä½œ 
è¿™é‡Œå› ä¸ºä¼˜åŒ–å™¨æœ‰å¤§é‡å‚æ•° å¦‚æœå…¨éƒ¨éƒ½ç”Ÿæˆæ–°çš„å˜é‡å å†å†™å›
vramä¼šçˆ†ç‚¸  
adamwçš„æ•°å­¦å…¬å¼  

å¦‚æœä½ æƒ³å®ç°è¿™ä¸ªæ•°å­¦å¼å¯¹åº”çš„åŸå­çº§ï¼ˆIn-placeï¼‰  
æ“ä½œ 

$a = a + b$   
a.add_(b)

$a = a + k \times b$  
a.add_(b, alpha=k)

$a = a \times b$  
a.mul_(b)

$a = a \times k$  
a.mul_(k)

$a = a + k \times (b \times c)$  
a.addcmul_(b, c, value=k)  

$a = a + k \times (b / c)$  
a.addcdiv_(b, c, value=k)

---
å®æˆ˜ç»ƒä¹ ï¼š
è§£è€¦æƒé‡è¡°å‡ (Weight Decay)å…¬å¼ï¼š   
$\theta = \theta - \alpha \cdot \lambda \cdot \theta$  
æŒ‰ç…§â€œæ‹†å¼€å†åˆå¹¶â€çš„é€»è¾‘

`è§‚å¯Ÿç»“æ„`ï¼šè¿™æ˜¯â€œä¸€ä¸ªå€¼å‡å»å®ƒæœ¬èº«çš„ä¸€éƒ¨åˆ†â€ã€‚  
æå–å…¬å› å¼ï¼š
$\theta = \theta \cdot (1 - \alpha \cdot \lambda)$ã€‚

`å¯¹åº”æ“ä½œ`ï¼šè¿™æ˜¯ä¸€ä¸ªä¹˜æ³•ï¼Œç³»æ•°æ˜¯   
$(1 - \alpha \cdot \lambda)$ã€‚

`ä»£ç å®ç°`ï¼š
```Python
p.mul_(1 - lr * weight_decay)
```
æˆ–è€…ä½¿ç”¨ add_ï¼šPython# p = p + (-lr * wd) * p

```python
p.add_(p, alpha=-lr * weight_decay)
```

---

`1. æ›´æ–°ä¸€é˜¶çŸ©` $m$å…¬å¼ï¼š $m = \beta_1 m + (1 - \beta_1) g$ 
- æ™®é€šå†™æ³•ï¼šm = (beta1 * m) + (1 - beta1) * g  
(ç¼ºç‚¹ï¼šäº§ç”Ÿäº† beta1*m å’Œ (1-beta1)*g ç­‰å¤šä¸ªä¸´æ—¶å¼ é‡)  

- åŸå­çº§å†™æ³•ï¼šm.mul_(beta1).add_(g, alpha=1 - beta1)  
(é€»è¾‘ï¼šå…ˆå°† $m$ åŸåœ°ä¹˜ä¸Š $\beta_1$ï¼Œå†åŠ ä¸Š $g \times (1-\beta_1)$)  


`2. æ›´æ–°äºŒé˜¶çŸ©` $v$å…¬å¼ï¼š $v = \beta_2 v + (1 - \beta_2) g^2$  
- æ™®é€šå†™æ³•ï¼š v = (beta2 * v) + (1 - beta2) * (g * g) (ç¼ºç‚¹ï¼šg*g ä¼šåœ¨æ˜¾å­˜ä¸­åˆ›å»ºä¸€ä¸ªå’Œå‚æ•°çŸ©é˜µä¸€æ ·å¤§çš„ä¸´æ—¶å‰¯æœ¬)

- åŸå­çº§å†™æ³•ï¼š v.mul_(beta2).addcmul_(g, g, value=1 - beta2) (é€»è¾‘ï¼šä½¿ç”¨ addcmul ç›´æ¥å®Œæˆâ€œå¹³æ–¹ + ä¹˜ç³»æ•° + ç´¯åŠ â€çš„èåˆæ“ä½œ)  

`3. è®¡ç®—ä¿®æ­£åçš„æ­¥é•¿ `
å…¬å¼ï¼š $\alpha_t = \alpha \cdot \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$  
- æ™®é€š/åŸå­çº§ä¸€è‡´ï¼šè¿™æ­¥é€šå¸¸æ¶‰åŠæ ‡é‡è®¡ç®—ï¼Œä¸æ¶‰åŠå¤§å¼ é‡ï¼Œå› æ­¤ç›´æ¥å†™å³å¯ï¼š
bias_correction1   
= 1 - beta1 ** stepbias_correction2   
= 1 - beta2 ** stepstep_size   
= lr * (math.sqrt(bias_correction2) / bias_correction1)    

`4. å‚æ•°æ¢¯åº¦æ›´æ–° `
$\theta$å…¬å¼ï¼š $\theta = \theta - \alpha_t \frac{m}{\sqrt{v} + \epsilon}$ 
- æ™®é€šå†™æ³•ï¼šp = p - step_size * (m / (torch.sqrt(v) + eps))  
(ç¼ºç‚¹ï¼šé™¤æ³•å’Œæ ¹å·ä¼šäº§ç”Ÿå¤§é‡ä¸­é—´å˜é‡ï¼Œå¢åŠ æ˜¾å­˜å³°å€¼)  
- åŸå­çº§å†™æ³•ï¼šdenom = v.sqrt().add_(eps)p.addcdiv_(m, denom, value=-step_size)    
(é€»è¾‘ï¼šå…ˆåŸåœ°æ±‚ $v$ çš„æ ¹å·å¹¶åŠ  $\epsilon$ï¼Œå†ç”¨ addcdiv å®Œæˆâ€œé™¤æ³• + ä¹˜ç³»æ•° + ç´¯åŠ â€)

`5. æƒé‡è¡°å‡ (Weight Decay)å…¬å¼`ï¼š   
$\theta = \theta - \alpha \lambda \theta$
- æ™®é€šå†™æ³•ï¼šp = p - (lr * wd * p)  
- åŸå­çº§å†™æ³•ï¼šp.add_(p, alpha=-lr * wd) æˆ– p.mul_(1 - lr * wd)(é€»è¾‘ï¼šç›´æ¥åœ¨ $\theta$ æ‰€åœ¨çš„å†…å­˜åœ°å€ä¸Šå‡å»å®ƒçš„ä¸€éƒ¨åˆ†)  


```python
import math
import torch
from torch.optim import Optimizer


class AdamW(Optimizer):
    def __init__(
        self,
        params,
        lr=1e-3,
        betas=(0.9, 0.95),
        eps=1e-8,
        weight_decay=0.01,
    ):
        r"""
        AdamW Optimizer

        æ•°å­¦ç¬¦å·è¯´æ˜ï¼ˆper-parameterï¼‰ï¼š

        å‚æ•°ï¼š
            Î¸ âˆˆ â„^n        â€”â€” å‚æ•°å‘é‡
            g_t            â€”â€” ç¬¬ t æ­¥çš„æ¢¯åº¦ âˆ‡_Î¸ L(Î¸_t)

        è¶…å‚æ•°ï¼š
            Î±              â€”â€” å­¦ä¹ ç‡ (lr)
            Î²â‚, Î²â‚‚         â€”â€” ä¸€é˜¶ / äºŒé˜¶åŠ¨é‡è¡°å‡ç³»æ•° (betas)
            Îµ              â€”â€” æ•°å€¼ç¨³å®šé¡¹ (eps)
            Î»              â€”â€” æƒé‡è¡°å‡ç³»æ•° (weight_decay)

        åˆå§‹åŒ–ï¼š
            mâ‚€ = 0
            vâ‚€ = 0

        æ³¨æ„ï¼š
        - self.state åœ¨çˆ¶ç±» Optimizer ä¸­å·²åˆ›å»ºï¼š
          self.state = defaultdict(dict)
        - æ¯ä¸ª Parameter p éƒ½æœ‰è‡ªå·±çš„ä¸€ä»½ state[p]
        """

        # ---------- è¶…å‚æ•°åˆæ³•æ€§æ£€æŸ¥ ----------
        if lr < 0.0:
            raise ValueError(f"Invalid learning rate: {lr}")
        if eps < 0.0:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta1 value: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta2 value: {betas[1]}")
        if weight_decay < 0.0:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        # defaultsï¼šparam_group çš„å…œåº•è¶…å‚æ•°
        defaults = dict(
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
        )

        # çˆ¶ç±»ä¼šï¼š
        # 1. æ„é€  self.param_groups
        # 2. æ„é€  self.state = defaultdict(dict)
        super().__init__(params, defaults)

    def step(self):
        r"""
        æ‰§è¡Œä¸€æ¬¡ AdamW å‚æ•°æ›´æ–°

        å¯¹äºæ¯ä¸ªå‚æ•° Î¸ï¼Œåœ¨ç¬¬ t æ­¥æ‰§è¡Œï¼š

        (1) ä¸€é˜¶åŠ¨é‡ï¼š
            m_t = Î²â‚ m_{t-1} + (1 - Î²â‚) g_t

        (2) äºŒé˜¶åŠ¨é‡ï¼š
            v_t = Î²â‚‚ v_{t-1} + (1 - Î²â‚‚) g_tÂ²

        (3) Bias correctionï¼š
            Î±_t = Î± Â· sqrt(1 - Î²â‚‚^t) / (1 - Î²â‚^t)

        (4) Adam æ›´æ–°ï¼š
            Î¸'_t = Î¸_t - Î±_t Â· m_t / (sqrt(v_t) + Îµ)

        (5) AdamW è§£è€¦æƒé‡è¡°å‡ï¼š
            Î¸_{t+1} = Î¸'_t - Î± Â· Î» Â· Î¸_t
        """

        # ===== éå†æ¯ä¸ªå‚æ•°ç»„ï¼ˆè¶…å‚æ•°å±‚ï¼‰=====
        for group in self.param_groups:
            Î± = group["lr"]
            Î²1, Î²2 = group["betas"]
            Îµ = group["eps"]
            Î» = group["weight_decay"]

            # ===== éå†è¯¥ç»„å†…çš„æ¯ä¸€ä¸ª Parameter =====
            for p in group["params"]:
                if p.grad is None:
                    continue

                # g_t = âˆ‡_Î¸ L(Î¸_t)
                g_t = p.grad.data

                # å–å‡ºè¯¥å‚æ•°çš„ stateï¼ˆper-parameterï¼‰
                state = self.state[p]

                # ---------- æ‡’åˆå§‹åŒ– ----------
                # mâ‚€ = 0, vâ‚€ = 0
                if len(state) == 0:
                    state["step"] = 0
                    state["m"] = torch.zeros_like(p)  # m
                    state["v"] = torch.zeros_like(p)  # v

                m = state["m"]
                v = state["v"]

                # t â† t + 1
                state["step"] += 1
                t = state["step"]

                # ---------- (1) ä¸€é˜¶åŠ¨é‡ ----------
                # m_t = Î²â‚ m_{t-1} + (1 - Î²â‚) g_t
                m.mul_(Î²1).add_(g_t, alpha=1 - Î²1)

                # ---------- (2) äºŒé˜¶åŠ¨é‡ ----------
                # v_t = Î²â‚‚ v_{t-1} + (1 - Î²â‚‚) g_tÂ²
                v.mul_(Î²2).addcmul_(g_t, g_t, value=1 - Î²2)

                # ---------- (3) Bias correction ----------
                # Î±_t = Î± Â· sqrt(1 - Î²â‚‚^t) / (1 - Î²â‚^t)
                bias_correction1 = 1.0 - Î²1 ** t
                bias_correction2 = 1.0 - Î²2 ** t
                Î±_t = Î± * math.sqrt(bias_correction2) / bias_correction1

                # ---------- (4) Adam æ›´æ–° ----------
                # Î¸'_t = Î¸_t - Î±_t Â· m_t / (sqrt(v_t) + Îµ)
                denom = v.sqrt().add_(Îµ)
                p.data.addcdiv_(m, denom, value=-Î±_t)

                # ---------- (5) AdamWï¼šè§£è€¦æƒé‡è¡°å‡ ----------
                # Î¸_{t+1} = Î¸'_t - Î± Â· Î» Â· Î¸_t
                if Î» != 0.0:
                    p.data.add_(p.data, alpha=-Î± * Î»)

```

## 5.4 Learning rate scheduling
Warm-up:$$\alpha_t = \frac{t}{T_w} \alpha_{\text{max}}, \quad \text{if } t < T_w$$Cosine Annealing:$$\alpha_t = \alpha_{\text{min}} + \frac{1}{2} \left( 1 + \cos \left( \frac{t - T_w}{T_c - T_w} \pi \right) \right) (\alpha_{\text{max}} - \alpha_{\text{min}}), \quad \text{if } T_w \le t \le T_c$$Post-annealing:$$\alpha_t = \alpha_{\text{min}}, \quad \text{if } t > T_c$$ 
## 5.5 Gradient cliping

gradient clipping çš„æ•´ä½“ L2 normï¼Œå°±æ˜¯ï¼š

æŠŠ æ¨¡å‹é‡Œæ‰€æœ‰ layer çš„æ‰€æœ‰å‚æ•°å¼ é‡ï¼ˆä¾‹å¦‚ self-attention é‡Œçš„ Wq / Wk / Wv / Woï¼ŒFFN é‡Œçš„ W1 / W2 / W3ï¼Œembeddingï¼Œnorm çš„ gain ç­‰ï¼‰

1. ğŸ‘‰ æ¯ä¸€ä¸ªå‚æ•°å¼ é‡é‡Œçš„â€œæ¯ä¸€ä¸ªæ ‡é‡å…ƒç´ â€
2. ğŸ‘‰ å…¨éƒ¨æ‹¿å‡ºæ¥
3. ğŸ‘‰ é€ä¸ªå¹³æ–¹
4. ğŸ‘‰ å…¨éƒ¨åŠ åœ¨ä¸€èµ·
5. ğŸ‘‰ å†æ•´ä½“å¼€æ ¹å·

| æ¦‚å¿µ        | ä»£ç                           |
| --------- | --------------------------- |
| æ‰€æœ‰æ¢¯åº¦å…ƒç´ å¹³æ–¹å’Œ | `grad.pow(2).sum()`         |
| æ‰€æœ‰å‚æ•°ä¸€èµ·    | å¤–å±‚ `for param in params`    |
| L2 norm   | `torch.sqrt(total_norm_sq)` |
| clip å› å­   | `M / (â€–gâ€–â‚‚ + eps)`          |
| ä¸æ”¹å˜æ–¹å‘     | `mul_(clip_coef)`           |

# 6 Training loop
##  6.1 data loader æ•°æ®åŠ è½½å™¨

<!-- å‡è®¾æˆ‘æ‹¿åˆ°äº†ä¸€ä¸ªnumpy array 
ä»–çš„é•¿åº¦å°±æ˜¯ n 
æˆ‘å¾—åˆ°äº†ä¸€ä¸ªbatch size å’Œ context length 
å•ä¸ªbatch æ˜¯ numpy arrayçš„ 
æ˜¯xã€iï¼Œi+context_lengthã€‘ 
outputæ˜¯ xã€i+1ï¼Œi+context_length +1ã€‘ 
å¹¶ä¸”è¦ç»™å‡ºbatchä¸ªè¿™æ ·å­çš„ å¯¹ 
ä½†æˆ‘çš„é—®é¢˜æ˜¯ æ€ä¹ˆåˆ‡å‰²å‘¢ ï¼Ÿ 
ä¸€ä¸² numpy array å¦‚æœè¦è·å¾—æœ€å¤§çš„æ•ˆç‡
 åœ¨åˆ‡å‰²çš„æ—¶å€™å°±åº”è¯¥è¶Šåˆ†æ•£è¶Šå¥½ 
 åŒæ—¶ä¹Ÿä¸èƒ½é¦–ä½ä¸å‘¼åº” ä¸€ç‚¹ä¸è¿æ¥ -->

 é¦–å…ˆå‘¢ å¾—åˆ°çš„dataset æ˜¯ npt.NDarray 
 x=(x1â€‹,x2â€‹,â€¦,xnâ€‹)
 ```python
 è¾“å…¥: [x2, x3, x4]
 ç›®æ ‡: [x3, x4, x5]

 ```
 1. batch size 
 2. context_length
 3. device

 ç»™å®šçš„ 1d numpy array
 è¦æ ¹æ® batch sizeå’Œ context length åˆ‡åˆ†
 è¾“å‡ºï¼š
    ä¸¤ä¸ª PyTorch Tensor
 è¾“å…¥ tokens
    ç›®æ ‡ tokens
 å½¢çŠ¶éƒ½æ˜¯ï¼š
    (batch_size, context_length)
    å¹¶ä¸” å·²ç»è¢«æ”¾åˆ°æŒ‡å®š device ä¸Š

```python
input  = x[i     : i + context_length]
target = x[i + 1 : i + context_length + 1]
```
batch ä¸ª

returnçš„æ˜¯
```python
inputs.shape  == (batch_size, context_length)
targets.shape == (batch_size, context_length)
```

```python
for step in training_steps:
    x_batch, y_batch = get_batch(...)
    logits = model(x_batch)
    loss = cross_entropy(logits, y_batch)
    loss.backward()
    optimizer.step()
```

### é«˜çº§index 
é«˜çº§ indexing æ°¸è¿œæ˜¯åœ¨åš gatherï¼š
index æä¾›åæ ‡ï¼Œ
dataset æä¾›å€¼ï¼Œ
è¾“å‡º shape = index shapeï¼ˆå¿…è¦æ—¶å†åŠ ä¸Šå‰©ä½™ç»´åº¦ï¼‰ã€‚


#### 2d dataset
##### ğŸ‘‰æƒ…å†µä¸€ï¼š2D dataset +ã€Œä¸€ä¸ª index å¼ é‡ã€
```python
ä¾‹å­
dataset.shape == (H, W)      # 2D
idx.shape == (B, M)          # æ•´æ•°çŸ©é˜µ
out = dataset[idx]
```
è§„åˆ™æ˜¯ï¼š

idx é‡Œçš„æ¯ä¸€ä¸ªæ•´æ•°ï¼Œéƒ½è¢«å½“ä½œ â€œè¡Œç´¢å¼•ï¼ˆrow indexï¼‰â€  
åˆ—ç´¢å¼•é»˜è®¤æ˜¯ :ï¼ˆæ•´è¡Œï¼‰  

ä¹Ÿå°±æ˜¯è¯´ï¼Œè¯­ä¹‰ç­‰ä»·äºï¼š
```python
out[i, j] = dataset[idx[i, j], :]
```


æ‰€ä»¥ï¼š  
```python
out.shape == (B, M, W)  
```
ä½ æ‹¿åˆ°çš„æ˜¯ å¾ˆå¤šè¡Œçš„æ‹·è´  
ä¸¾ä¸ªå…·ä½“çš„æ•°å€¼ä¾‹å­  
```python
dataset = np.arange(12).reshape(4, 3)
# [[ 0,  1,  2],
#  [ 3,  4,  5],
#  [ 6,  7,  8],
#  [ 9, 10, 11]]

idx = np.array([[0, 2],
                [1, 3]])

dataset[idx]

```
ç»“æœæ˜¯ï¼š

```python
[
  [[ 0,  1,  2],
   [ 6,  7,  8]],

  [[ 3,  4,  5],
   [ 9, 10, 11]]
]
```
 index å†³å®šâ€œå–å“ªäº›è¡Œâ€ï¼Œshape ç”± idx å’Œ datasetå…±åŒå†³å®š  å†³å®š

##### ğŸ‘‰æƒ…å†µäºŒï¼š2D dataset +ã€Œä¸¤ä¸ª index å¼ é‡ã€ï¼ˆæœ€å¸¸è§ä¹Ÿæœ€å…³é”®ï¼‰

è¿™æ˜¯ä½ ä»¥ååœ¨ embedding / attention / KV cache é‡Œæœ€å¸¸è§çš„æƒ…å†µã€‚

ä¾‹å­
```python
rows.shape == (B, M)
cols.shape == (B, M)

out = dataset[rows, cols]
```
è¿™æ—¶è§„åˆ™å˜æˆï¼š

`(rows[i, j], cols[i, j])` ç»„æˆä¸€ä¸ªåæ ‡ç‚¹ï¼Œ
å» dataset é‡Œå–â€œä¸€ä¸ªæ ‡é‡â€

ä¹Ÿå°±æ˜¯è¯´ï¼š

`out[i, j] = dataset[rows[i, j], cols[i, j]]`


æ‰€ä»¥ï¼š

`out.shape == rows.shape == cols.shape
`
ä¸å†å¤šå‡ºä¸€ä¸ªç»´åº¦

æ•°å€¼ä¾‹å­
```python
rows = np.array([[0, 1],
                 [2, 3]])
cols = np.array([[1, 2],
                 [0, 1]])

dataset[rows, cols]


ç»“æœæ˜¯ï¼š

[
  [1, 5],
  [6, 10]
]
```


ğŸ‘‰ è¿™æ˜¯â€œé€ç‚¹ gatherâ€

##### æƒ…å†µä¸‰ï¼š2D dataset +ã€Œ1D indexï¼ˆæœ€ç›´è§‚çš„ï¼‰ã€**
```python
rows = np.array([0, 2, 3])
out = dataset[rows]
```


ç­‰ä»·äºï¼š

```python
out[i] = dataset[rows[i], :]
```

è¾“å‡ºæ˜¯ 2D

å–çš„æ˜¯æ•´è¡Œ

shape = (len(rows), W)

| dataset | index å½¢å¼        | è¯­ä¹‰      | è¾“å‡º shape           |
| ------- | --------------- | ------- | ------------------ |
| 2D      | `idx` (1D / 2D) | å– **è¡Œ** | `idx.shape + (W,)` |
| 2D      | `(rows, cols)`  | å– **ç‚¹** | `rows.shape`       |
| 1D      | `idx` (ä»»æ„ç»´)     | å– **ç‚¹** | `idx.shape`        |
### broadcast é‡æ–°å†æ¥ä¸€é
`idx = starts[:, None] + offsets[None, :]`

`starts[:, None]` æŠŠ starts ä» [n] å˜æˆ [n, 1]

`offsets[None, :]` æŠŠ offsets ä» [m] å˜æˆ [1, m]

ä¸¤è€…ç›¸åŠ æ—¶ï¼Œè§¦å‘ broadcasting
æœ€ç»ˆç»“æœæ˜¯ä¸€ä¸ª [n, m] çš„ tensor

## 6.2 Checkpoint
éœ€è¦æŠŠmodelå’Œoptimizerçš„stateéƒ½å­˜åœ¨ dicté‡Œ   
ä½¿ç”¨`.state_dict()`èƒ½å¾—åˆ°é‚£äº›parameters

1. å­˜çš„æ—¶å€™`save_checkpoint`  è¿™ä¸ªfunction
ä½¿ç”¨`torch.save(obj,out) ` 

2. åŠ è½½çš„æ—¶å€™ä½¿ç”¨`load_checkpoint()`  è¿™ä¸ªfunction
åŠ è½½ä¼šè¿”å›å½“å‰çš„iteration

## 6.3 trainï¼ï¼ï¼

æŠŠæ‰€æœ‰ä¸œè¥¿æ”¾åœ¨ä¸€èµ·å§   

### 6.3.1 Paser
Qï¼š ä¸ºä»€ä¹ˆé€‰æ‹©ä½¿ç”¨paser  
Aï¼š  å› ä¸ºä½œä¸šé‡Œå†™çš„æ˜¯  
 `Ability to configure and control the various model and optimizer hyperparameters`  
 è¦æœ‰èƒ½åŠ›èƒ½å¤Ÿconfigure ä»¥åŠæ§åˆ¶å„ç§æ¨¡å‹å‚æ•°å’Œè¶…å‚æ•°  
 æ‰€ä»¥ä½¿ç”¨paser

Q: å¦‚ä½•ä½¿ç”¨paser  
#### 1. å››ä¸ªåŸºç¡€çš„ç‚¹
```python 
'''
1.åˆ›é€ argparser
'''
ap = argparse.ArgumentPaser()

'''
2.åŠ æ”¯æŒçš„argument
2.1 å£°æ˜éœ€è¦æ—¶ --lr
2.2 typeè¦å£°æ˜ æ”¯æŒçš„æœ‰flaot
2.3 æ˜¯å¦æ˜¯å¿…é¡»çš„ 
'''
ap.add_argument("--lr",type = float,required = True)

'''
3ç”Ÿæˆ 
'''
args = ap.parse_args()

print("lr = ".args.lr)
```

#### 2. åŠ é»˜è®¤å€¼çš„å‚æ•°
```python
ap.add_argument("--batch-size", type=int, default=32)  
# å†åŠ ä¸€ä¸ª
```

#### 3. æœ€å¸¸è§çš„bool å¼€å…³ ï¼ˆstore_trueï¼‰
```python
ap.add_argument("--compile", action="store_true")  
# ä¸å†™åˆ™ Falseï¼Œå†™äº†å°± True

import argparse
ap = argparse.ArgumentParser()
ap.add_argument("--compile", action="store_true")  # é»˜è®¤ Falseï¼Œå‡ºç°åˆ™ True
args = ap.parse_args()
if args.compile:
    print("å¼€å¯ compile æ¨¡å¼")
else:
    print("ä¸å¼€å¯ compile æ¨¡å¼")
```

```python
python train.py
# ä¸å¼€å¯ compile æ¨¡å¼

python train.py --compile
# å¼€å¯ compile æ¨¡å¼
```

#### 4.æœ‰ä»€ä¹ˆä½¿ç”¨è§„èŒƒå‘¢
1.  æŠŠCLIå½“æˆAPI ç¨³å®š å¯å¤ç° å¯ä»¥è¯»

- 1.1å‚æ•°åç¨³å®šï¼š   
ç»Ÿä¸€ç”¨kebab-case ï¼š `--batch-size`  
æ‰€æœ‰å­—æ¯å°å†™ï¼Œå•è¯ä¹‹é—´ç”¨è¿å­—ç¬¦ï¼ˆæ¨ªæ  -ï¼‰è¿æ¥ã€‚

-  1.2å‚æ•°å€¼é€æ˜ï¼š  
 helpé‡Œè¦èƒ½çœ‹åˆ°é»˜è®¤å€¼ 
`ï¼ˆArgumentDefaultsHelpFormatterï¼‰`
ä¾‹å­ï¼š
ä½ åªéœ€è¦åœ¨åˆå§‹åŒ–æ—¶æ·»åŠ  formatter_class=argparse.ArgumentDefaultsHelpFormatterï¼š
```Python
import argparse
def parse_args():
    # å…³é”®ç‚¹ï¼šæ·»åŠ  formatter_class
    parser = argparse.ArgumentParser(
        description="è®­ç»ƒè„šæœ¬ç¤ºä¾‹",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # è¿™é‡Œçš„ default å€¼ä¼šè‡ªåŠ¨æ˜¾ç¤ºåœ¨ help ä¿¡æ¯ä¸­
    parser.add_argument("--batch-size", type=int, default=32, help="æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°")
    parser.add_argument("--lr", type=float, default=1e-4, help="åˆå§‹å­¦ä¹ ç‡")
    parser.add_argument("--device", type=str, default="cuda", help="è¿è¡Œè®¾å¤‡")

    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
```
ä¼šæ˜¾ç¤ºhelpåŠ å…¥äº†éƒ¨åˆ† ä¹Ÿä¼šæ˜¾ç¤ºdefaultæ˜¯ä»€ä¹ˆ
```python
options:
  -h, --help            show this help message and exit
  --batch-size BATCH_SIZE
                        æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•° (default: 32)
  --lr LR               åˆå§‹å­¦ä¹ ç‡ (default: 0.0001)
  --device DEVICE       è¿è¡Œè®¾å¤‡ (default: cuda)
```
- 1.3 å¯å¤ç°å¿…é€‰é¡¹ï¼š  
è‡³å°‘æŠŠä¸€ä¸‹éƒ½å˜æˆå‚æ•°å¹¶è®°å½•åˆ°æ—¥å¿—/é…ç½®æ–‡ä»¶
    
    - seedã€æ•°æ®è·¯å¾„ã€æ¨¡å‹ç»“æ„è¶…å‚ã€ä¼˜åŒ–å™¨è¶…å‚ã€
    - lr scheduleã€batch/contextã€dtypeã€deviceã€
    - è®­ç»ƒæ­¥æ•°ã€eval/save é¢‘ç‡ã€resume/ckpt è·¯å¾„

è¿™ä¸€éƒ¨åˆ†æ€ä¹ˆå®ç° ä¾é `Argpaser` åŠ ä¸Š `Dataclass/OmegaConf` åŠ ä¸Š`YAML`çš„ç»„åˆæ–¹æ¡ˆ  