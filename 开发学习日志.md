# 12-14
# 2 Byte-Pair Encoding (BPE) tokenizer
## 2.1 The Unicode Standard

å’Œpdfé‡Œå†™çš„ä¸€æ · unicode defineäº†15000+çš„character   
s æ˜¯code point 115
```python
>>> z = ord('s')
115

>>> chr(115)
's'
```
æ­¤å¤„å¯è§ord() : 
  convert ä¸€ä¸ª single Unicode character ä¸€ä¸ªunicodeçš„å­—ç¬¦
  å˜æˆ its integer representation ä»–çš„æ•´æ•°è¡¨ç¤º 
  è¿™ä¸ªinteger æŒ‡çš„å°±æ˜¯ Unicode code point
chr():
  æŠŠä¸€ä¸ª æ•´æ•°è¡¨è¾¾
  (è¿™ä¸ªæ•´æ•°å¿…é¡»æ˜¯ä¸€ä¸ª valid Unicode code point) 
  è½¬æ¢æˆå¯¹åº”çš„ single Unicode character
  (è¿”å›å€¼åœ¨ Python é‡Œæ˜¯ä¸€ä¸ª é•¿åº¦ä¸º 1 çš„å­—ç¬¦ä¸²)

print(chr(0))
- è¾“å‡ºçš„æ˜¯ å­—ç¬¦æœ¬èº«
- è¯¥å­—ç¬¦æ˜¯ NULLï¼ˆä¸å¯æ‰“å°ï¼‰
- æ‰€ä»¥ç»ˆç«¯ çœ‹èµ·æ¥ä»€ä¹ˆéƒ½æ²¡æœ‰

repr(chr(0))
- è¾“å‡ºçš„æ˜¯ escaped representation
- ä½¿ç”¨çš„æ˜¯ åå…­è¿›åˆ¶å½¢å¼ \x00
- è¿™æ˜¯ä¸ºäº† è®©äººç±»å’Œè°ƒè¯•å™¨å¯è¯»

NULLæœ¬èº«è¿æ‰“å° éƒ½ä¸å ä½ç½®
```python
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```
## 2.2 Unicode Encodings

å› ä¸ºç›´æ¥ç”¨unicodeä¸ç°å®
150k çš„item å¾ˆå¤šéƒ½ç”¨ä¸åˆ°
å¾ˆå¤šå­—ç¬¦åŠå…¶ç½•è§
vocab ä¼šå¾ˆç¨€ç–

æˆ‘ä»¬ç›´æ¥ç”¨unicode encoding UTF-8
å› ä¸º
ä¸ç”¨ç»†çœ‹ ä½†æ˜¯å¤§æ¦‚æ˜¯è¿™æ ·åšçš„ 


[[../../ç†è§£/utf-8.md]]
UTF-8 çš„ç›®çš„ä¸æ˜¯å‹ç¼©å­—ç¬¦æ•°é‡ï¼Œè€Œæ˜¯æŠŠä»»æ„ Unicode å­—ç¬¦æ˜ å°„æˆç”±æœ‰é™çš„ byteï¼ˆ0â€“255ï¼‰ç»„æˆçš„åºåˆ—ï¼Œä»è€Œç”¨å¯å˜é•¿åº¦ç¼–ç è¡¨ç¤ºæ‰€æœ‰å­—ç¬¦ï¼Œå¹¶ä¿è¯å‘åå’Œå‘å‰å…¼å®¹ã€‚


### Problem(unicode2):


(b)
The function incorrectly decodes each byte separately, but UTF-8 characters may span multiple bytes, causing multi-byte characters such as "ã“" to be decoded incorrectly.

## 2.4 BPE Tokenizer Training
(æ€»ç»“ç‰ˆ ç°åœ¨ä¸è§£é‡Š)
raw text 
 â†’ special token handling
 â†’ pre-tokenizer (regex)
 â†’ UTF-8 bytes
 â†’ BPE merges
 â†’ token IDs

### Pre-tokenization 

Pre-tokenizer çš„ä½œç”¨ä¸æ˜¯â€œå†³å®šæœ€ç»ˆ tokenâ€ï¼Œè€Œæ˜¯æŠŠæ–‡æœ¬åˆ‡æˆâ€œåˆç†çš„å°å—â€ï¼Œè®© BPE åœ¨è¿™äº›å—å†…éƒ¨ç»Ÿè®¡å’Œ mergeã€‚
```python
>>> # requires `regex` package
>>> import regex as re
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```
å¯ä»¥çœ‹å‡º pre-rtokenizationæ˜¯å…ˆå°†å¥å­å˜æˆä¸€ä¸ªä¸ªè¯å’Œç¬¦å·å’Œ


è¸©çš„å‘ æ­£åˆ™è¡¨è¾¾å¼ regex å†™äº†åˆ†æ®µ
åªèƒ½ä¸€è¡Œ
å…·ä½“çœ‹ ./ç†è§£/pre-tokenizer-regex

### ä½¿ç”¨re.finditer è€Œä¸æ˜¯ re.findall
```python
>>> re.findall(PAT, "some text that i'll pre-tokenize")
>>>['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']

for m in re.finditer(PAT2, english_words, flags=re.VERBOSE):
    print(m.group(0))

The original sentence using PAT2 'some text that i'll pre-tokenize'
some
 text
 that
 i
\'ll
 pre
-
tokenize
```
#### re.findall
å¯è§ re.finall(pattern,text)
ä¼šç”Ÿæˆä¸€ä¸ªlist

#### re.finditer
ä½†æ˜¯re.finditer(pattern,text,flag = re.VERBOSE) æ˜¯ä¸€ä¸ªscanner
å¾—ç”¨ for m in ä¸Šè¿°è¿™ä¸ª
ç„¶åæ¯ä¸ªscançš„ éƒ½å¾—ç”¨ m.group(0)

#### åˆ›å»ºå­—å…¸
```python
counts = {}
```
ç›´æ¥ç”Ÿæˆå­—å…¸
å¦‚æœä½¿ç”¨ç›´æ¥æŒ‰ç…§whitespaceåˆ†çš„è¯
æ˜¯text_list = text.split(" ")
æ³¨æ„ç©ºæ ¼ ä»¥åŠè¿”å›çš„æ˜¯list

### 2.4A python é‡Œæ€ä¹ˆåˆ›å»ºå’ŒæŸ¥çœ‹`bytes`
1. é¦–å…ˆbæ˜¯ä¸€ä¸ªbytes å¯¹è±¡ 
  b = b'low'
  bytes æ˜¯ Python çš„ä¸€ä¸ªå†…å»ºç±»å‹

2. å…¶æ¬¡ bytesæ˜¯ä¸€ä¸ª ä¸å¯å˜çš„å­—èŠ‚åºåˆ— 
  æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 0~255 çš„ intï¼ˆ8-bitï¼‰
  
  å› ä¸ºæ˜¯å­—èŠ‚åºåˆ—ï¼ˆsequenceï¼‰
  æ‰€ä»¥å¯ä»¥éå†
  éå†æ—¶è¿”å›çš„ä¹Ÿæ˜¯ int
```python
listï¼ˆbï¼‰
#[108,111,119]  æ¯ä¸ª int æ˜¯è¯¥å­—èŠ‚çš„â€œæ•°å€¼â€ âœ” å–å€¼èŒƒå›´åˆšå¥½æ˜¯ 0~255
    # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 8-bit çš„å­—èŠ‚
    # Python ç”¨ 0~255 çš„ int æ¥è¡¨ç¤ºè¿™ä¸ªå­—èŠ‚çš„å€¼
```

3. bytes(intå€¼) æ˜¯åˆ›å»ºä¸€ä¸ª é•¿åº¦ä¸º n çš„ bytes å¯¹è±¡
    æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯ 0ï¼ˆ\x00ï¼‰

4. ä½†æ˜¯å¦‚æœæ˜¯ bytes([n]) 
    åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ bytes å¯¹è±¡
    ğŸ‘‰ è¿™ä¸ªå­—èŠ‚çš„å€¼æ˜¯ n
    âœ” [n] æ˜¯ä¸€ä¸ª iterable
    âœ” è¡¨ç¤ºâ€œæˆ‘è¦ä¸€ä¸ªå€¼ä¸º n çš„å­—èŠ‚â€

5.ç´¢å¼•è¿”å› int åˆ‡ç‰‡è¿”å› bytes


##### 1 `b"..."` literal
```python
b"abc"          # åªèƒ½ç›´æ¥å†™ ASCII å­—ç¬¦
b"\xe4\xb8\xad" # å¯ä»¥ç”¨åå…­è¿›åˆ¶è½¬ä¹‰å†™ä»»æ„å­—èŠ‚
```

##### 2 `str.encode(...)`ï¼šä»æ–‡æœ¬åˆ°å­—èŠ‚
```python
s = "ä¸­æ–‡"
bs = s.encode("utf-8")
# bs æ˜¯ bytes
```
 
##### 3 `bytes.decode(...)` ä»å­—èŠ‚åˆ°æ–‡æœ¬
```python
bs = b"\xe4\xb8\xad\xe6\x96\x87"
s = bs.decode("utf-8")
```


##### 4 `bytes([x,y,z])` ç”¨æ•´æ•°åˆ—è¡¨æ„é€ 
```python
bytes([65, 66, 67])  # b'ABC'
```

##### 5 bytes çš„â€œå…ƒç´ â€æ˜¯ä»€ä¹ˆ
```python
b = b"ABC"
b[0]      # 65ï¼ˆintï¼‰
b[0:2]    # b'AB'ï¼ˆbytesï¼‰
list(b)   # [65, 66, 67]
```



#### 2.4B è¯»å–æ–‡ä»¶
https://chatgpt.com/g/g-p-693f75d2365c8191baf9aaa7038e3595-cs336xiao-xi-jie/c/6948a752-5e40-832e-bb7f-f6299b8b04be
##### 1. open() ä¸ä¸Šä¸‹æ–‡ç®¡ç†å™¨ with

ä¸€å®šä¼˜å…ˆç”¨ with open(...) as f:ï¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€å¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®é‡Šæ”¾èµ„æºã€‚
ä¸ç”¨ with å°±è¦æ‰‹åŠ¨ f.close()ï¼Œå¾ˆå®¹æ˜“å¿˜ã€‚

```python
from pathlib import Path
path = Path("data.txt")
with path.open("r", encoding="utf-8") as f:
    text = f.read()
```

1ï¸âƒ£ ä½ è¦è§£å†³çš„å®é™…é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ BPE è®­ç»ƒå‰ï¼Œè¦æ±‚ï¼š
special tokensï¼ˆå¦‚ <|endoftext|>ï¼‰ä¸èƒ½å‚ä¸ pre-tokenization

ä¸èƒ½è·¨ special token åš merge
æ‰€ä»¥æµç¨‹æ˜¯ï¼š

å…ˆæŒ‰ special tokens åˆ‡æ–‡æœ¬ â†’ æ¯ä¸€æ®µå•ç‹¬åš regex pre-tokenization

1. ä¸ç”¨æ­£åˆ™ï¼ˆå¯¹æ¯”ç”¨ï¼‰
  ```python
    text = "A<X>B<X>C"
    print(text.split("<X>"))
  ```

2.  å¿…é¡»ã€Œè¿æ¥ä¸¤ä¸ªå¯åŒ¹é…çš„ä¸œè¥¿ã€

    æ­£åˆ™é‡Œçš„åŸºæœ¬å½¢å¼æ˜¯ï¼š
    A | B
    æ„æ€æ˜¯ï¼š
    åŒ¹é… A æˆ– B
    æ‰€ä»¥ä½ å¿…é¡»ç»™å®ƒå·¦æ“ä½œæ•°å’Œå³æ“ä½œæ•°ï¼š
    <X> | <Y>

3. join åšçš„äº‹ï¼ˆéå¸¸ç²¾ç¡®ï¼‰
    "|".join(["<X>", "<Y>", "<Z>"])
    ç»“æœï¼š
    "<X>|<Y>|<Z>"
    join éœ€è¦ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ æ‰€ä»¥generator expressionå’Œ list comprehensionéƒ½è¡Œ
```python
"|".join(re.escape(tok) for tok in special_tokens)
"|".join([re.escape(tok) for tok in special_tokens])
```
```python
(re.escape(tok) for tok in special_tokens)
```
å®ƒäº§ç”Ÿçš„æ˜¯ï¼š
ä¸€ä¸ª æƒ°æ€§ iterable
æ¯æ¬¡ join éœ€è¦ä¸‹ä¸€ä¸ªå…ƒç´ æ—¶æ‰è®¡ç®—

4. re.escape
    æ˜¯æŠŠç‰¹æ®Šçš„ç¬¦å· ä¾‹å¦‚ 
    special token é€šå¸¸é•¿è¿™æ ·ï¼š
    <|endoftext|>
    ä½†åœ¨æ­£åˆ™é‡Œï¼š
    |
    <
    >
    éƒ½æœ‰ç‰¹æ®Šå«ä¹‰
    ğŸ‘‰ å¦‚æœä½ ç›´æ¥ç”¨ï¼Œä¼šè¢« regex è¯¯è§£
    âŒ é”™è¯¯ç¤ºä¾‹
    ```python
    pattern = "|".join(["<|endoftext|>"])
    ```

### 2.4C sum
`sum(iterable, start=0) `
ç­‰ä»·äºï¼š

```python
total = start
for x in iterable:
    total = total + x
return total
```

1) æ±‚æ•°å­—å’Œï¼ˆæœ€å¸¸è§ï¼‰

```python
nums = [1, 2, 3]
total = sum(nums)          # 6
total2 = sum(nums, 10)     # 16  (ä» 10 å¼€å§‹åŠ )
```

2) å¯¹ç”Ÿæˆå™¨æ±‚å’Œï¼ˆçœå†…å­˜ï¼‰
```python
total = sum(i*i for i in range(10))
```

3) åˆå¹¶â€œæ”¯æŒ + çš„å¯¹è±¡â€ï¼Œç”¨ start æŒ‡å®šåˆå§‹å€¼
```python
from collections import Counter
counters = [Counter("ab"), Counter("bc")]
merged = sum(counters, Counter())
```

#####

### 2.4D Counter 
1) ç›´æ¥ç»Ÿè®¡é¢‘æ¬¡
```python
from collections import Counter
cnt = Counter("banana")
# Counter({'a': 3, 'n': 2, 'b': 1})
```

2) é€æ­¥ç´¯åŠ ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
cnt = Counter()
for tok in tokens:
    cnt[tok] += 1
```

3) æ‰¹é‡æ›´æ–°
```python
cnt.update(tokens)              # tokens æ˜¯ iterable
cnt.update({"a": 2, "b": 1})    # ä¹Ÿå¯ä»¥æ˜¯ mapping
```

4) top-k é«˜é¢‘
```python
cnt.most_common(5)
```

5) åˆå¹¶/åŠ å‡ï¼ˆè®¡æ•°è¯­ä¹‰ï¼‰
```python
c1 = Counter(a=2, b=1)
c2 = Counter(a=1, b=5, c=1)

c1 + c2   # è®¡æ•°ç›¸åŠ 
c1 - c2   # è®¡æ•°ç›¸å‡ï¼ˆä¼šè¿‡æ»¤ <=0ï¼‰
c1 & c2   # æ¯ä¸ªé”®å– min
c1 | c2   # æ¯ä¸ªé”®å– max
```
6) ä¸å­˜åœ¨çš„é”®é»˜è®¤æ˜¯ 0
```python
cnt = Counter()
cnt["missing"]    # 0
```

### 2.5E Counter vs dictï¼šå¸¸è§ç”¨æ³•å¯¹æ¯”ï¼ˆä½ å†™ä½œä¸šæœ€å¸¸ç¢°åˆ°ï¼‰
A) è®¡æ•°ï¼ˆtoken -> æ¬¡æ•°ï¼‰

âœ… ç”¨ Counter

```python
cnt = Counter()
cnt[tok] += 1
```

ç”¨ dict ä¹Ÿèƒ½åšï¼Œä½†æ›´å•°å—¦ï¼š
```python
d = {}
d[tok] = d.get(tok, 0) + 1
```
B) æ˜ å°„/ç´¢å¼•ï¼ˆtoken -> id, id -> tokenï¼‰

âœ… ç”¨ dictï¼ˆå³ä½¿ value æ˜¯æ•°å­—ä¹Ÿä¸€æ ·ï¼‰

```python
token2id = {"<pad>": 0, "<unk>": 1}
id2token = {v: k for k, v in token2id.items()}
```

C) åˆå¹¶å¤šä¸ªç»Ÿè®¡ç»“æœ

âœ… Counter.update / +=ï¼ˆæ¨èï¼‰

```python
total = Counter()
for c in sub_counters:
    total.update(c)     # æˆ– total += c
```

ä¹Ÿå¯ä»¥ç”¨ sum(sub_counters, Counter())ï¼ˆèƒ½ç”¨ä½†é€šå¸¸æ…¢ç‚¹ï¼‰

D) â€œä¸å­˜åœ¨â€è¦ä¸è¦ç®— 0

è¦å½“ 0ï¼šCounter æ›´è‡ªç„¶

è¦ä¸¥æ ¼åŒºåˆ†ä¸å­˜åœ¨ vs å€¼ä¸º 0ï¼šdict æ›´æ˜ç¡®



### 2.4E å¹¶è¡Œ multiprocessing

#### map + sum  

```python
with Pool(num_processes) as pool:
sub_counters = pool.map(parallel_worker, worker_parameters)
```

ä¸¤ä¸ªç»„æˆå…ƒç´ 
ä¸€ä¸ªæ˜¯worker function
    æ‹¿åˆ°çš„æ˜¯y ä½œä¸ºiterableä¸­çš„ä¸€ä»½ä½œä¸ºå‚æ•°
    

ä¸€ä¸ªæ˜¯å¹¶è¡Œçš„caller
    pool.map()
      ä¼šæŠŠpool.map(x,y)
      yä¸­æ¯ä¸€ä¸ªå…ƒç´  
      ä½œä¸ºå®å‚ä¼ ç»™x 
      å³taskï¼ˆpiece of yï¼‰
      
```python
def task1(x:str)->list[str]:
    # print(x) # æ‹¿åˆ°çš„æ˜¯ string
    token_list = x.split(" ")
    # print(token_list) # è¿”å›çš„æ˜¯list
    return token_list


if __name__ == "__main__":

    with Pool(4) as pool:
        results = pool.map(task1, list_text)# è¿™ä¸ªresultsæ˜¯ä¸€ä¸ªlist

    bytes_counts = sum(sub_counters, Counter()) # è¿™é‡Œå¯è§ sum æŠŠè¿”å›çš„è¿™ä¸ªiterable ç»™ä¸€æ¬¡æ€§æ‰«æå½’å¹¶
```
##### åœ¨åšä»€ä¹ˆ
map ä¼š ç­‰æ‰€æœ‰ worker éƒ½å®Œæˆ
æ‰€æœ‰ Counter ä¸€æ¬¡æ€§è¿”å›æˆä¸€ä¸ª list
æœ€åç”¨ sum æŠŠå®ƒä»¬åˆå¹¶

##### ä¼˜ç‚¹
âœ… ä»£ç æç®€
å¾ˆâ€œå‡½æ•°å¼â€ï¼Œä¸€çœ¼å°±æ‡‚
âœ… åˆå¹¶é€»è¾‘å°‘
sum å†…éƒ¨æ˜¯çº¿æ€§ mergeï¼Œæ¬¡æ•° = len(sub_counters)

##### è‡´å‘½ç¼ºç‚¹ï¼ˆåœ¨ BPE é‡Œéå¸¸æ˜æ˜¾ï¼‰
âŒå†…å­˜å³°å€¼çˆ†ç‚¸
    åŒæ—¶å­˜åœ¨ï¼š
    æ‰€æœ‰ worker çš„ Counter
    æœ€ç»ˆåˆå¹¶ç”¨çš„ Counter
    åœ¨ TinyStories / OWT ä¸Šï¼š
    â— è¿™æ˜¯æœ€å®¹æ˜“ OOM / swap / å¡æ­» çš„å†™æ³•

âŒ æ²¡æœ‰æµå¼åé¦ˆ
    tqdm åªèƒ½åŒ… mapï¼Œä½†ç»“æœæ²¡å›æ¥ä¹‹å‰ä½ å•¥ä¹Ÿå¹²ä¸äº†
    çœ‹èµ·æ¥åƒâ€œå¡ä½äº†â€

#### imap_unordered + å¢é‡ç´¯åŠ ï¼ˆæµå¼ï¼‰
from multiprocessing import Pool

```python

bytes_counts = Counter()
with Pool(num_processes) as pool:
    for sub in pool.imap_unordered(
        parallel_worker,
        worker_parameters,
        chunksize=1,
    ):
        bytes_counts += sub
'''
chunk å¾ˆé‡ â†’ chunksize å°
chunk å¾ˆè½» â†’ chunksize å¤§
'''

```
##### åœ¨åšä»€ä¹ˆ
parallel_worker ä¸€ä¸ªä»»åŠ¡ â†’ ä¸€ä¸ª Counter
worker ç®—å®Œå°±ç«‹åˆ»è¿”å›
ä¸»è¿›ç¨‹ è¾¹æ”¶åˆ°ã€è¾¹ç´¯åŠ  åˆ° bytes_counts
é¡ºåº ä¸ä¿è¯ï¼ˆunorderedï¼‰

##### ä¼˜ç‚¹
âœ… å†…å­˜å ç”¨ä½ï¼ˆå…³é”®ï¼‰
    ä¸»è¿›ç¨‹é‡Œæ°¸è¿œåªå¤šä¸€ä¸ª sub Counter
    éå¸¸é€‚åˆ TinyStories / OWT è¿™ç§ Counter æå¤§ çš„åœºæ™¯
âœ… å»¶è¿Ÿä½
    ç¬¬ä¸€ä¸ª worker ä¸€ç®—å®Œä½ å°±å¼€å§‹ç´¯åŠ 
    tqdm èƒ½å®æ—¶åŠ¨ï¼Œæ–¹ä¾¿åˆ¤æ–­â€œæ˜¯ä¸æ˜¯å¡æ­»äº†â€
âœ… æ›´ç¨³
    å¦‚æœæŸä¸ª chunk ç‰¹åˆ«å¤§ï¼Œä¸ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ç»“æœå †åœ¨å†…å­˜é‡Œ

##### ç¼ºç‚¹
âŒ Python å±‚ += æ¬¡æ•°å¤š
    æ¯ä¸ª sub éƒ½è¦åšä¸€æ¬¡ Counter merge
    å¦‚æœ worker æ•°é‡ Ã— chunk æ•°é‡ç‰¹åˆ«å¤§ï¼Œä¼šæœ‰ä¸€äº› Python å¼€é”€


### Compute BPE merges è®¡ç®—BPE merges

BPE ç®—æ³• 
1. iteratively æ•°æ¯ä¸€å¯¹bytes å¹¶ä¸” è¯†åˆ«æœ€é«˜çš„ä¸€å¯¹
2. ç»Ÿè®¡å‡ºç°æœ€å¤šçš„ä¸€å¯¹ 
3. åˆå¹¶ ä» 'a' 'b' å˜æˆab
4. æŠŠåˆå¹¶çš„åŠ å…¥vocabulary 
5. æœ€åçš„vocabularyä¼šæ˜¯256ä¸ª åˆå§‹çš„ åŠ ä¸Š bpeåœ¨è®­ç»ƒä¸­èåˆçš„
6. ä¸è€ƒè™‘è·¨pre-tokençš„è¾¹ç•Œ 
7. å¦‚æœå‡ºç°tieçš„æƒ…å†µ ä½¿ç”¨lexicographically larger
 ï¼ˆä»€ä¹ˆæ˜¯lexicographically larger å¾…è¡¥å®Œ ï¼‰
```python
max([(b'l', b'o'), (b'o', b'w')]) == (b'o', b'w')
```

#### ä½¿ç”¨tqdm
##### 1ï¸âƒ£ åŒ…è£¹ä»»ä½•å¯è¿­ä»£å¯¹è±¡

```python
from tqdm import tqdm
for x in tqdm(data):
    ...
```

ğŸ‘‰ åŸåˆ™ï¼šå‡¡æ˜¯â€œäººä¼šç­‰â€çš„å¾ªç¯ï¼Œå°±è¯¥æœ‰ tqdm
    ï¼ˆtokenize / merge / train / scan file / pool.imapï¼‰

##### 2ï¸âƒ£ æ˜ç¡® totalï¼ˆéå¸¸é‡è¦ï¼‰
```python
for sub in tqdm(pool.imap_unordered(fn, items), total=len(items)):
```

å¦‚æœæ²¡æœ‰ totalï¼š
    ETA ä¸å‡†
    ç™¾åˆ†æ¯”ä¸æ˜¾ç¤º
    çœ‹èµ·æ¥åƒâ€œå¡ä½äº†â€ï¼ˆä½ é‡åˆ°è¿‡ï¼‰

##### 3ï¸âƒ£ ç»™ tqdm èµ·ä¸€ä¸ªâ€œè§£é‡Šå‹åå­—â€

```python
tqdm(..., desc="Pretokenize chunks")
tqdm(..., desc="BPE merges", unit="merge")
tqdm(..., desc="Counting byte pairs")
```

âŒ ä¸è¦ï¼š
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024

âœ… è¦ï¼š
    Pretokenize chunks:  35%|â–ˆâ–ˆâ–ˆâ–Œ | 350/1000
    desc = ä½ åœ¨ debug æ—¶çš„â€œå¿ƒç†å®‰å…¨æ„Ÿâ€

4ï¸âƒ£ unit æ˜¯ underrated çš„ç¥å™¨
```python
tqdm(..., unit="chunk")
tqdm(..., unit="merge")
tqdm(..., unit="MB")
```

æ¯”å¦‚ BPE mergeï¼š
```python
for _ in trange(num_merges, desc="BPE merges", unit="merge"):
```
ä¼šæ˜¾ç¤ºï¼š
    BPE merges:  1234/10000 merge




##### 4ï¸âƒ£ unit æ˜¯ underrated çš„ç¥å™¨
tqdm(..., unit="chunk")
tqdm(..., unit="merge")
tqdm(..., unit="MB")


æ¯”å¦‚ BPE mergeï¼š

for _ in trange(num_merges, desc="BPE merges", unit="merge"):
    ...


ä¼šæ˜¾ç¤ºï¼š

BPE merges:  1234/10000 merge

##### 5ï¸âƒ£ imap / imap_unordered + tqdmï¼ˆä½ æ­£åœ¨ç”¨çš„ï¼‰

```python
with Pool(num_processes) as pool:
    for sub in tqdm(
        pool.imap_unordered(worker, params, chunksize=1),
        total=len(params),
        desc="Pretokenize chunks",
    ):
        bytes_counts += sub
```


âœ… è¿™æ˜¯å¤šè¿›ç¨‹ + tqdm çš„æœ€ä¼˜è§£ä¹‹ä¸€
    ä¸ºä»€ä¹ˆä¸ç”¨ mapï¼Ÿ
    map ä¼š ç­‰æ‰€æœ‰ worker å®Œæˆæ‰è¿”å›
    tqdm å®Œå…¨æ²¡æ³•æ›´æ–°

#### Map reduce

âœ”ï¸ ç†è®ºä¸Šå†…å­˜å¤Ÿ
âœ”ï¸ ä½†å› ä¸ºåå¤äº§ç”Ÿå³°å€¼
âœ”ï¸ Linux è§¦å‘ swap
âœ”ï¸ swap çš„æ˜¯â€œè¿˜åœ¨ç”¨çš„ Python å¯¹è±¡â€
âœ”ï¸ å¯¼è‡´ page fault + swap å¾ªç¯
âœ”ï¸ ç¨‹åºçœ‹ä¼¼æ²¡æ­»ï¼Œå®åˆ™è¢« IO åæ‰
##### Map éƒ¨åˆ†çš„å°ç»†èŠ‚

1. åˆ›å»ºtmp_dir
##### ä¸€ã€pathlib.Pathï¼šä¸æ˜¯â€œæ›´å¥½çœ‹â€ï¼Œè€Œæ˜¯æ›´å°‘ bug
1ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç”¨ str + os.path äº†ï¼Ÿ
    ä¼ ç»Ÿå†™æ³•çš„é—®é¢˜ä½ è‚¯å®šå·²ç»è¸©è¿‡ï¼š
```python
path = "/tmp" + "/" + "part_0001.pkl"   # å®¹æ˜“é”™
path = os.path.join("/tmp", "part_0001.pkl")  # å¯è¯»æ€§å·®
```


Path çš„æ ¸å¿ƒä»·å€¼ä¸æ˜¯ç®€æ´ï¼Œæ˜¯:  

ğŸ‘‰ æŠŠâ€œè·¯å¾„â€å½“æˆä¸€ä¸ªå¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²

2ï¸âƒ£ Path çš„ä¸‰ä¸ªæœ€å¸¸ç”¨åŠ¨ä½œï¼ˆ90% åœºæ™¯:  
âœ… åˆ›å»º / è¡¨ç¤ºè·¯å¾„ï¼ˆä¸è®¿é—®ç£ç›˜ï¼‰
```python
from pathlib import Path
p = Path("/tmp/bpe")
```

âš ï¸ æ³¨æ„ï¼š 
è¿™ä¸€æ­¥ä¸ä¼šåˆ›å»ºç›®å½•  
åªæ˜¯ä¸€ä¸ªâ€œè·¯å¾„æè¿°å¯¹è±¡â€  

âœ… è·¯å¾„æ‹¼æ¥ï¼ˆæœ€é‡è¦ï¼‰
```python
p = Path("/tmp/bpe")
file = p / "part_0001.pkl"
```
ä½ ç°åœ¨ç”¨çš„æ­£æ˜¯è¿™ä¸ªï¼š  
path = tmp_dir / f"part_{i:04d}.pkl"


è¿™ä¸€æ­¥çš„éšå«å¥½å¤„ï¼š

1. è‡ªåŠ¨å¤„ç† /
2. Windows / Linux é€šç”¨
3. ä¸ä¼šå‡ºç° // æˆ–æ¼ /

âœ… ä¸è€ API äº¤äº’
```python
with open(path, "wb") as f:
open()
pickle.dump()
torch.save()
```
ğŸ‘‰ éƒ½å¤©ç„¶æ”¯æŒ Path

è¿™æ˜¯ Path èƒ½åœ¨å·¥ç¨‹é‡ŒçœŸæ­£â€œè½åœ°â€çš„åŸå› ã€‚

3ï¸âƒ£ Path åœ¨ä½ è¿™æ®µä»£ç é‡Œçš„çœŸå®ä»·å€¼
```python
tmp_dir = Path(tempfile.mkdtemp(...))
path = tmp_dir / f"part_{i:04d}.pkl"
```
ä½ åœ¨åšçš„æ˜¯ï¼š 
æŠŠ MapReduce çš„ä¸­é—´æ–‡ä»¶ï¼Œå½“æˆâ€œç»“æ„åŒ–è·¯å¾„ç©ºé—´â€æ¥ç®¡ç†  
è€Œä¸æ˜¯ï¼š 
åˆ°å¤„æ‹¼å­—ç¬¦ä¸²   
æä¸æ¸…å“ªäº›æ˜¯ç›®å½•ï¼Œå“ªäº›æ˜¯æ–‡ä»¶  
è¿™æ˜¯ infra é£æ ¼ï¼Œä¸æ˜¯è„šæœ¬é£æ ¼ã€‚  

##### äºŒã€tempfile.mkdtempï¼šä½ åœ¨å‘æ“ä½œç³»ç»Ÿâ€œç§Ÿä¸€å—å®‰å…¨ç©ºé—´â€
1ï¸âƒ£ mkdtemp åˆ°åº•å¹²äº†ä»€ä¹ˆï¼Ÿ  
```python
tmp = tempfile.mkdtemp() 
```
æ“ä½œç³»ç»Ÿå±‚é¢è¯­ä¹‰æ˜¯ï¼š
â€œå¸®æˆ‘åˆ›å»ºä¸€ä¸ª å”¯ä¸€ã€ä¸å†²çªã€å·²å­˜åœ¨ çš„ä¸´æ—¶ç›®å½•ï¼Œå¹¶æŠŠè·¯å¾„ç»™æˆ‘â€  
å®ƒä¿è¯äº†ä¸‰ä»¶äº‹ï¼š 
1. ç›®å½•ä¸€å®šå­˜åœ¨
2. åå­—ä¸ä¼šæ’
3. åˆ›å»ºè¿‡ç¨‹æ˜¯åŸå­çš„ï¼ˆçº¿ç¨‹/è¿›ç¨‹å®‰å…¨ï¼‰

2ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç”¨ mkdir("/tmp/xxx")ï¼Ÿ
å› ä¸ºè¿™æ®µä»£ç åœ¨å¹¶è¡Œ / å¤šè¿›ç¨‹ç¯å¢ƒé‡Œï¼š  
```python
os.makedirs("/tmp/bpe_mapreduce")
```
å¯èƒ½ä¼šï¼š
1. è¢«å¦ä¸€ä¸ªè¿›ç¨‹æŠ¢å…ˆåˆ›å»º
2. æŠ›å¼‚å¸¸
3. æˆ– silently è¦†ç›–  

è€Œ mkdtemp çš„è¯­ä¹‰æ˜¯ï¼š  
ğŸ‘‰ è¿™ä¸ªç›®å½•åªå±äºä½ è¿™ä¸€æ¬¡è¿è¡Œ  

3ï¸âƒ£ mkdtemp vs TemporaryDirectoryï¼ˆéå¸¸é‡è¦ï¼‰  
API	ä¼šè‡ªåŠ¨åˆ é™¤å—	é€‚åˆè°  
mkdtemp()	âŒ ä¸ä¼š	MapReduce / pipeline  
TemporaryDirectory()	âœ… with ç»“æŸè‡ªåŠ¨åˆ 	å°è„šæœ¬ / unit test  

ä½ ç°åœ¨ç”¨ mkdtemp æ˜¯å¯¹çš„ï¼Œå› ä¸ºï¼š

1. reduce å¯èƒ½å¾ˆä¹…  
2. å¯èƒ½è¦ checkpoint / debug  
3. ä¸­é€”å´©æºƒä½ è¿˜æƒ³ ls çœ‹æ–‡ä»¶  

4ï¸âƒ£ åœ¨ä½ è¿™ä¸ª MapReduce é‡Œçš„çœŸå®æ„ä¹‰
```python
tmp_dir = Path(tempfile.mkdtemp(prefix="bpe_mapreduce_"))
```  
ä½ å…¶å®æ˜¯åœ¨è¯´ï¼š  
â€œæˆ‘ç°åœ¨è¦å¼€å§‹ä¸€ä¸ª ç‹¬ç«‹çš„ MapReduce jobï¼Œ  
ç»™æˆ‘ä¸€ä¸ª å¹²å‡€çš„ scratch spaceã€‚â€   
è¿™æ˜¯ åˆ†å¸ƒå¼ç³»ç»Ÿæ€ç»´ï¼Œä¸æ˜¯ Python è¯­æ³•ç³–ã€‚   

##### ä¸‰ã€prefixï¼šä¸æ˜¯è£…é¥°ï¼Œæ˜¯â€œè°ƒè¯•ä¸è¿ç»´æ¥å£â€  
1ï¸âƒ£ prefix çš„ä½œç”¨éå¸¸æœ´ç´   
```python
tempfile.mkdtemp(prefix="bpe_mapreduce_")  
```
æœ€ç»ˆè·¯å¾„å¤§æ¦‚æ˜¯ï¼š  
```bash 
/tmp/bpe_mapreduce_8qk4v9n2  
```
prefix åšäº†ä»€ä¹ˆï¼Ÿ  
1. ç»™éšæœºç›®å½•ååŠ ä¸€ä¸ªäººç±»å¯è¯»æ ‡ç­¾  
2. å‰©ä¸‹éƒ¨åˆ†ä¿è¯å”¯ä¸€æ€§  

2ï¸âƒ£ ä¸ºä»€ä¹ˆ prefix åœ¨ infra ä»£ç é‡Œéå¸¸é‡è¦ï¼Ÿ

è®¾æƒ³ä½ åœ¨æœåŠ¡å™¨ä¸Šï¼š
```bash
ls /tmp
```
çœ‹åˆ°ï¼š
```bash
tmpa9f3
tmpz71k
bpe_mapreduce_8qk4v9n2
torch_compile_xxx
```

ä½ ä¸€çœ¼å°±çŸ¥é“ï¼š
1.å“ªä¸ªæ˜¯ä½ çš„ job
2. å“ªä¸ªå¯ä»¥åˆ 
3. å“ªä¸ªä¸æ•¢åŠ¨
ğŸ‘‰ è¿™å°±æ˜¯ è¿ç»´å¯è§‚æµ‹æ€§

3ï¸âƒ£ prefix çš„â€œéšè—å¥½å¤„â€   
âœ… å´©æºƒåå¯è¿½æº¯
ç¨‹åºæŒ‚äº†  
ä½ è¿˜èƒ½ cd /tmp/bpe_mapreduce_*
çœ‹ï¼š part_*.pkl
1. å“ªä¸ª chunk æœ€æ…¢
2. æ–‡ä»¶å¤§å°åˆ†å¸ƒ

âœ… å¤š job å¹¶è¡Œä¸æ··  
åŒæ—¶è·‘ TinyStories + OWT  
prefix ä¸åŒå³å¯åŒºåˆ†  

##### å››ã€enumerate å¹²çš„äº‹æƒ…
enumerate å¹²çš„äº‹æƒ…ï¼ˆéå¸¸é‡è¦ï¼‰
```python
for i, sub in enumerate(iterator):
```
enumerate åšçš„ä¸æ˜¯â€œæ¢å¤åŸ indexâ€ï¼Œè€Œæ˜¯ï¼š  
åœ¨æ¶ˆè´¹ iterator çš„é‚£ä¸€åˆ»ï¼ŒæŒ‰â€œåˆ°è¾¾é¡ºåºâ€åˆ†é…ä¸€ä¸ªæ–° id



### Problem(train_bpe_tinystories):BPETrainingonTinyStories (2points)

#### a  How many hours and memory did training take?What is the longest token in the vocabulary? Does it make sense?
1. Serialize the resulting vocabulary and merges to disk 
    for further inspection

1. éœ€è¦åˆ›å»ºå­˜æ”¾çš„æ–‡ä»¶å¤¹

```python
#å­˜æ”¾çš„ç›®å½• æ˜¯é¡¹ç›®æ–‡ä»¶å¤¹ä¸­çš„
OUT_DIR = "/home/dexterding/projects/assignment1-basics/bpe_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

VOCAB_PATH = os.path.join(OUT_DIR, "tinystories_vocab.pkl")
MERGES_PATH = os.path.join(OUT_DIR, "tinystories_merges.pkl")
PROFILE_PATH = os.path.join(OUT_DIR, "profile.txt")
```

2. å°†è¿”å›çš„vocabå’Œmerges ç”¨pickle.dump(XXXX,f) çš„æ–¹å¼å­˜æ”¾
```python
    # =========================
    # SAVE ARTIFACTS
    # =========================
    with open(VOCAB_PATH, "wb") as f:
        pickle.dump(vocab, f)

    with open(MERGES_PATH, "wb") as f:
        pickle.dump(merges, f)
```


3. å¦‚ä½•æµ‹è¯• æ‰€ç”¨çš„æ—¶é—´

åœ¨è®­ç»ƒçš„å¼€å¤´å’Œç»“å°¾åˆ†åˆ«æ”¾ä¸€ä¸ª 
```python
start_time = time.time()
## training code
end_time = time.time()
    print(f"Training time (this run): {(end_time - start_time)/60:.2f} minutes")

```

4. æµ‹è¯•vocabå’Œ merge å°±ç”¨ len(vocab/merges)

5. longest token 
å› ä¸º vocabçš„ç»“æ„å°±æ˜¯ dict[int,bytes]
5.1 å¯ä»¥åœ¨è·‘è®­ç»ƒçš„ä»£ç é‡Œæ”¾è¿™ä¸ª 
```python 
    longest_token = max(vocab.values(), key=len)
```
5.2 ä¹Ÿå¯ä»¥ç›´æ¥è¯»å–pickeleæ–‡ä»¶
```python
with open(VOCAB_PATH, "rb") as f:
    vocab = pickle.load(f)
tok_id, tok_bytes = max(vocab.items(), key=lambda kv: len(kv[1]))
print("longest_token_id:", tok_id)
print("byte_len:", len(tok_bytes))
print("raw_bytes_repr:", repr(tok_bytes))
print("decoded_utf8 (replace):", tok_bytes.decode("utf-8", errors="replace"))
```



#### b Profile your code. What part of the tokenizer training process takes the most time
```bash        
1           1262.415    1262.415 1766.558   1766.558 /home/dexterding/projects/assignment1-basics/cs336_basics/debug/parallel_train_bpe_cache.py:171(train_bpe)
5094536393  231.086     0.000    231.086    0.000 {built-in method builtins.len}
2255121258  121.755     0.000    121.755    0.000 {method 'append' of 'list' objects}
28286       51.462      0.002    80.908     0.003 {built-in method builtins.max}
9743        0.033       0.000    80.902     0.008 /home/dexterding/projects/assignment1-basics/cs336_basics/debug/parallel_train_bpe_cache.py:127(get_highest_pair)```

å¯è§å¤§å¤šæ•°æ—¶é—´æ˜¯ lenï¼ˆï¼‰å’Œ list.append å æ®å¤§å¤šæ•°æ—¶é—´ å› ä¸ºä¸€ç›´åœ¨å¤§è§„æ¨¡æ„å»º

Training time (this run): 7.15 minutes
Peak RSS (main + children): 7.47 GB
Main RSS delta (end-start): 0.06 GB
Vocab size: 10000 | merges: 9743
Longest token bytes: 13
```

## 2.6 BPE Tokenizer: Encoding and Decoding

### 2.6.1 Encoding text
#### å°ç»†èŠ‚
1.æ³¨æ„æ­¤å¤„çš„@classmethod å’Œ åç»­çš„return cls æœ¬è´¨ä¸Šæ˜¯ è¿”å›ä¸€ä¸ªclass instance

#### æ€è·¯ç†è§£ 
##### Step1: Pre-tokenize.
We first pre-tokenize the sequence and represent each pre-token as a sequence of UTF-8 bytes, just as we did in BPE training.

é¦–å…ˆè¿˜æ˜¯è¦æŠŠtext å˜æˆsequence of bytes 
é—®é¢˜æ˜¯æµç¨‹æ˜¯å¦è¿˜ä¸€æ ·
1. é¦–å…ˆsplit special token
å› ä¸ºæœ‰å¯èƒ½å‡ºç° "You are cat<|speical|>" 
å› ä¸ºspecial tokenå’Œå…¶ä»–è¯è¯­è¿ä¸€å— ä¸å¸Œæœ›tokenizeræŠŠ cat<|speical|>è¿åœ¨ä¸€èµ·
æ‰€ä»¥å°±è¦å…ˆsplitåå†ç”¨PATåˆ†è¯
ç„¶åå†é€ä¸ªencode(utf-8)

We will be merging these bytes within each pre-token into
vocabulary elements, handling each pre-token independently(nomergesacrosspre-tokenboundaries).

##### Step2:Apply the merges.
We then take the sequence of vocabulary element merges created during BPE
training, and apply it to our pre-tokens in the same order of creation.