# 12-14
# 2 Byte-Pair Encoding (BPE) tokenizer
## 2.1 The Unicode Standard

和pdf里写的一样 unicode define了15000+的character   
s 是code point 115
```python
>>> z = ord('s')
115

>>> chr(115)
's'
```
此处可见ord() : 
  convert 一个 single Unicode character 一个unicode的字符
  变成 its integer representation 他的整数表示 
  这个integer 指的就是 Unicode code point
chr():
  把一个 整数表达
  (这个整数必须是一个 valid Unicode code point) 
  转换成对应的 single Unicode character
  (返回值在 Python 里是一个 长度为 1 的字符串)

print(chr(0))
- 输出的是 字符本身
- 该字符是 NULL（不可打印）
- 所以终端 看起来什么都没有

repr(chr(0))
- 输出的是 escaped representation
- 使用的是 十六进制形式 \x00
- 这是为了 让人类和调试器可读

NULL本身连打印 都不占位置
```python
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```
## 2.2 Unicode Encodings

因为直接用unicode不现实
150k 的item 很多都用不到
很多字符及其罕见
vocab 会很稀疏

我们直接用unicode encoding UTF-8
因为
不用细看 但是大概是这样做的 


[[../../理解/utf-8.md]]
UTF-8 的目的不是压缩字符数量，而是把任意 Unicode 字符映射成由有限的 byte（0–255）组成的序列，从而用可变长度编码表示所有字符，并保证向后和向前兼容。


### Problem(unicode2):


(b)
The function incorrectly decodes each byte separately, but UTF-8 characters may span multiple bytes, causing multi-byte characters such as "こ" to be decoded incorrectly.

## 2.4 BPE Tokenizer Training
(总结版 现在不解释)
raw text 
 → special token handling
 → pre-tokenizer (regex)
 → UTF-8 bytes
 → BPE merges
 → token IDs

### Pre-tokenization 

Pre-tokenizer 的作用不是“决定最终 token”，而是把文本切成“合理的小块”，让 BPE 在这些块内部统计和 merge。
```python
>>> # requires `regex` package
>>> import regex as re
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```
可以看出 pre-rtokenization是先将句子变成一个个词和符号和


踩的坑 正则表达式 regex 写了分段
只能一行
具体看 ./理解/pre-tokenizer-regex

了解了如何分词之后

转回到下一步

### Compute BPE merges 计算BPE merges

BPE 算法 
1. iteratively 数每一对bytes 并且 识别最高的一对
2. 统计出现最多的一对 
3. 合并 从 'a' 'b' 变成ab
4. 把合并的加入vocabulary 
5. 最后的vocabulary会是256个 初始的 加上 bpe在训练中融合的
6. 不考虑跨pre-token的边界 
7. 如果出现tie的情况 使用lexicographically larger

```python
max([(b'l', b'o'), (b'o', b'w')]) == (b'o', b'w')
```


### Special tokens 