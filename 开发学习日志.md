# 12-14
# 2 Byte-Pair Encoding (BPE) tokenizer
## 2.1 The Unicode Standard

å’Œpdfé‡Œå†™çš„ä¸€æ · unicode defineäº†15000+çš„character   
s æ˜¯code point 115
```python
>>> z = ord('s')
115

>>> chr(115)
's'
```
æ­¤å¤„å¯è§ord() : 
  convert ä¸€ä¸ª single Unicode character ä¸€ä¸ªunicodeçš„å­—ç¬¦
  å˜æˆ its integer representation ä»–çš„æ•´æ•°è¡¨ç¤º 
  è¿™ä¸ªinteger æŒ‡çš„å°±æ˜¯ Unicode code point
chr():
  æŠŠä¸€ä¸ª æ•´æ•°è¡¨è¾¾
  (è¿™ä¸ªæ•´æ•°å¿…é¡»æ˜¯ä¸€ä¸ª valid Unicode code point) 
  è½¬æ¢æˆå¯¹åº”çš„ single Unicode character
  (è¿”å›å€¼åœ¨ Python é‡Œæ˜¯ä¸€ä¸ª é•¿åº¦ä¸º 1 çš„å­—ç¬¦ä¸²)

print(chr(0))
- è¾“å‡ºçš„æ˜¯ å­—ç¬¦æœ¬èº«
- è¯¥å­—ç¬¦æ˜¯ NULLï¼ˆä¸å¯æ‰“å°ï¼‰
- æ‰€ä»¥ç»ˆç«¯ çœ‹èµ·æ¥ä»€ä¹ˆéƒ½æ²¡æœ‰

repr(chr(0))
- è¾“å‡ºçš„æ˜¯ escaped representation
- ä½¿ç”¨çš„æ˜¯ åå…­è¿›åˆ¶å½¢å¼ \x00
- è¿™æ˜¯ä¸ºäº† è®©äººç±»å’Œè°ƒè¯•å™¨å¯è¯»

NULLæœ¬èº«è¿æ‰“å° éƒ½ä¸å ä½ç½®
```python
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```
## 2.2 Unicode Encodings

å› ä¸ºç›´æ¥ç”¨unicodeä¸ç°å®
150k çš„item å¾ˆå¤šéƒ½ç”¨ä¸åˆ°
å¾ˆå¤šå­—ç¬¦åŠå…¶ç½•è§
vocab ä¼šå¾ˆç¨€ç–

æˆ‘ä»¬ç›´æ¥ç”¨unicode encoding UTF-8
å› ä¸º
ä¸ç”¨ç»†çœ‹ ä½†æ˜¯å¤§æ¦‚æ˜¯è¿™æ ·åšçš„ 


[[../../ç†è§£/utf-8.md]]
UTF-8 çš„ç›®çš„ä¸æ˜¯å‹ç¼©å­—ç¬¦æ•°é‡ï¼Œè€Œæ˜¯æŠŠä»»æ„ Unicode å­—ç¬¦æ˜ å°„æˆç”±æœ‰é™çš„ byteï¼ˆ0â€“255ï¼‰ç»„æˆçš„åºåˆ—ï¼Œä»è€Œç”¨å¯å˜é•¿åº¦ç¼–ç è¡¨ç¤ºæ‰€æœ‰å­—ç¬¦ï¼Œå¹¶ä¿è¯å‘åå’Œå‘å‰å…¼å®¹ã€‚


### Problem(unicode2):


(b)
The function incorrectly decodes each byte separately, but UTF-8 characters may span multiple bytes, causing multi-byte characters such as "ã“" to be decoded incorrectly.

## 2.4 BPE Tokenizer Training
(æ€»ç»“ç‰ˆ ç°åœ¨ä¸è§£é‡Š)
raw text 
 â†’ special token handling
 â†’ pre-tokenizer (regex)
 â†’ UTF-8 bytes
 â†’ BPE merges
 â†’ token IDs

### Pre-tokenization 

Pre-tokenizer çš„ä½œç”¨ä¸æ˜¯â€œå†³å®šæœ€ç»ˆ tokenâ€ï¼Œè€Œæ˜¯æŠŠæ–‡æœ¬åˆ‡æˆâ€œåˆç†çš„å°å—â€ï¼Œè®© BPE åœ¨è¿™äº›å—å†…éƒ¨ç»Ÿè®¡å’Œ mergeã€‚
```python
>>> # requires `regex` package
>>> import regex as re
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```
å¯ä»¥çœ‹å‡º pre-rtokenizationæ˜¯å…ˆå°†å¥å­å˜æˆä¸€ä¸ªä¸ªè¯å’Œç¬¦å·å’Œ


è¸©çš„å‘ æ­£åˆ™è¡¨è¾¾å¼ regex å†™äº†åˆ†æ®µ
åªèƒ½ä¸€è¡Œ
å…·ä½“çœ‹ ./ç†è§£/pre-tokenizer-regex

### ä½¿ç”¨re.finditer è€Œä¸æ˜¯ re.findall
```python
>>> re.findall(PAT, "some text that i'll pre-tokenize")
>>>['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']

for m in re.finditer(PAT2, english_words, flags=re.VERBOSE):
    print(m.group(0))

The original sentence using PAT2 'some text that i'll pre-tokenize'
some
 text
 that
 i
\'ll
 pre
-
tokenize
```
#### re.findall
å¯è§ re.finall(pattern,text)
ä¼šç”Ÿæˆä¸€ä¸ªlist

#### re.finditer
ä½†æ˜¯re.finditer(pattern,text,flag = re.VERBOSE) æ˜¯ä¸€ä¸ªscanner
å¾—ç”¨ for m in ä¸Šè¿°è¿™ä¸ª
ç„¶åæ¯ä¸ªscançš„ éƒ½å¾—ç”¨ m.group(0)

#### åˆ›å»ºå­—å…¸
```python
counts = {}
```
ç›´æ¥ç”Ÿæˆå­—å…¸
å¦‚æœä½¿ç”¨ç›´æ¥æŒ‰ç…§whitespaceåˆ†çš„è¯
æ˜¯text_list = text.split(" ")
æ³¨æ„ç©ºæ ¼ ä»¥åŠè¿”å›çš„æ˜¯list

### 2.4A python é‡Œæ€ä¹ˆåˆ›å»ºå’ŒæŸ¥çœ‹`bytes`
1. é¦–å…ˆbæ˜¯ä¸€ä¸ªbytes å¯¹è±¡ 
  b = b'low'
  bytes æ˜¯ Python çš„ä¸€ä¸ªå†…å»ºç±»å‹

2. å…¶æ¬¡ bytesæ˜¯ä¸€ä¸ª ä¸å¯å˜çš„å­—èŠ‚åºåˆ— 
  æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 0~255 çš„ intï¼ˆ8-bitï¼‰
  
  å› ä¸ºæ˜¯å­—èŠ‚åºåˆ—ï¼ˆsequenceï¼‰
  æ‰€ä»¥å¯ä»¥éå†
  éå†æ—¶è¿”å›çš„ä¹Ÿæ˜¯ int
```python
listï¼ˆbï¼‰
#[108,111,119]  æ¯ä¸ª int æ˜¯è¯¥å­—èŠ‚çš„â€œæ•°å€¼â€ âœ” å–å€¼èŒƒå›´åˆšå¥½æ˜¯ 0~255
    # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 8-bit çš„å­—èŠ‚
    # Python ç”¨ 0~255 çš„ int æ¥è¡¨ç¤ºè¿™ä¸ªå­—èŠ‚çš„å€¼
```

3. bytes(intå€¼) æ˜¯åˆ›å»ºä¸€ä¸ª é•¿åº¦ä¸º n çš„ bytes å¯¹è±¡
    æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯ 0ï¼ˆ\x00ï¼‰

4. ä½†æ˜¯å¦‚æœæ˜¯ bytes([n]) 
    åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ bytes å¯¹è±¡
    ğŸ‘‰ è¿™ä¸ªå­—èŠ‚çš„å€¼æ˜¯ n
    âœ” [n] æ˜¯ä¸€ä¸ª iterable
    âœ” è¡¨ç¤ºâ€œæˆ‘è¦ä¸€ä¸ªå€¼ä¸º n çš„å­—èŠ‚â€
    
5.ç´¢å¼•è¿”å› int åˆ‡ç‰‡è¿”å› bytes


##### 1 `b"..."` literal
```python
b"abc"          # åªèƒ½ç›´æ¥å†™ ASCII å­—ç¬¦
b"\xe4\xb8\xad" # å¯ä»¥ç”¨åå…­è¿›åˆ¶è½¬ä¹‰å†™ä»»æ„å­—èŠ‚
```

##### 2 `str.encode(...)`ï¼šä»æ–‡æœ¬åˆ°å­—èŠ‚
```python
s = "ä¸­æ–‡"
bs = s.encode("utf-8")
# bs æ˜¯ bytes
```
 
##### 3 `bytes.decode(...)` ä»å­—èŠ‚åˆ°æ–‡æœ¬
```python
bs = b"\xe4\xb8\xad\xe6\x96\x87"
s = bs.decode("utf-8")
```


##### 4 `bytes([x,y,z])` ç”¨æ•´æ•°åˆ—è¡¨æ„é€ 
```python
bytes([65, 66, 67])  # b'ABC'
```

##### 5 bytes çš„â€œå…ƒç´ â€æ˜¯ä»€ä¹ˆ
```python
b = b"ABC"
b[0]      # 65ï¼ˆintï¼‰
b[0:2]    # b'AB'ï¼ˆbytesï¼‰
list(b)   # [65, 66, 67]
```



#### 2.4B è¯»å–æ–‡ä»¶
https://chatgpt.com/g/g-p-693f75d2365c8191baf9aaa7038e3595-cs336xiao-xi-jie/c/6948a752-5e40-832e-bb7f-f6299b8b04be
##### 1. open() ä¸ä¸Šä¸‹æ–‡ç®¡ç†å™¨ with

ä¸€å®šä¼˜å…ˆç”¨ with open(...) as f:ï¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€å¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®é‡Šæ”¾èµ„æºã€‚
ä¸ç”¨ with å°±è¦æ‰‹åŠ¨ f.close()ï¼Œå¾ˆå®¹æ˜“å¿˜ã€‚

```python
from pathlib import Path
path = Path("data.txt")
with path.open("r", encoding="utf-8") as f:
    text = f.read()
```




#####


### Compute BPE merges è®¡ç®—BPE merges

BPE ç®—æ³• 
1. iteratively æ•°æ¯ä¸€å¯¹bytes å¹¶ä¸” è¯†åˆ«æœ€é«˜çš„ä¸€å¯¹
2. ç»Ÿè®¡å‡ºç°æœ€å¤šçš„ä¸€å¯¹ 
3. åˆå¹¶ ä» 'a' 'b' å˜æˆab
4. æŠŠåˆå¹¶çš„åŠ å…¥vocabulary 
5. æœ€åçš„vocabularyä¼šæ˜¯256ä¸ª åˆå§‹çš„ åŠ ä¸Š bpeåœ¨è®­ç»ƒä¸­èåˆçš„
6. ä¸è€ƒè™‘è·¨pre-tokençš„è¾¹ç•Œ 
7. å¦‚æœå‡ºç°tieçš„æƒ…å†µ ä½¿ç”¨lexicographically larger
 ï¼ˆä»€ä¹ˆæ˜¯lexicographically larger å¾…è¡¥å®Œ ï¼‰
```python
max([(b'l', b'o'), (b'o', b'w')]) == (b'o', b'w')
```




### Special tokens 
