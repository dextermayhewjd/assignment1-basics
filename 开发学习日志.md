# 12-14
# 2 Byte-Pair Encoding (BPE) tokenizer
## 2.1 The Unicode Standard

å’Œpdfé‡Œå†™çš„ä¸€æ · unicode defineäº†15000+çš„character   
s æ˜¯code point 115
```python
>>> z = ord('s')
115

>>> chr(115)
's'
```
æ­¤å¤„å¯è§ord() : 
  convert ä¸€ä¸ª single Unicode character ä¸€ä¸ªunicodeçš„å­—ç¬¦
  å˜æˆ its integer representation ä»–çš„æ•´æ•°è¡¨ç¤º 
  è¿™ä¸ªinteger æŒ‡çš„å°±æ˜¯ Unicode code point
chr():
  æŠŠä¸€ä¸ª æ•´æ•°è¡¨è¾¾
  (è¿™ä¸ªæ•´æ•°å¿…é¡»æ˜¯ä¸€ä¸ª valid Unicode code point) 
  è½¬æ¢æˆå¯¹åº”çš„ single Unicode character
  (è¿”å›å€¼åœ¨ Python é‡Œæ˜¯ä¸€ä¸ª é•¿åº¦ä¸º 1 çš„å­—ç¬¦ä¸²)

print(chr(0))
- è¾“å‡ºçš„æ˜¯ å­—ç¬¦æœ¬èº«
- è¯¥å­—ç¬¦æ˜¯ NULLï¼ˆä¸å¯æ‰“å°ï¼‰
- æ‰€ä»¥ç»ˆç«¯ çœ‹èµ·æ¥ä»€ä¹ˆéƒ½æ²¡æœ‰

repr(chr(0))
- è¾“å‡ºçš„æ˜¯ escaped representation
- ä½¿ç”¨çš„æ˜¯ åå…­è¿›åˆ¶å½¢å¼ \x00
- è¿™æ˜¯ä¸ºäº† è®©äººç±»å’Œè°ƒè¯•å™¨å¯è¯»

NULLæœ¬èº«è¿æ‰“å° éƒ½ä¸å ä½ç½®
```python
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```
## 2.2 Unicode Encodings

å› ä¸ºç›´æ¥ç”¨unicodeä¸ç°å®
150k çš„item å¾ˆå¤šéƒ½ç”¨ä¸åˆ°
å¾ˆå¤šå­—ç¬¦åŠå…¶ç½•è§
vocab ä¼šå¾ˆç¨€ç–

æˆ‘ä»¬ç›´æ¥ç”¨unicode encoding UTF-8
å› ä¸º
ä¸ç”¨ç»†çœ‹ ä½†æ˜¯å¤§æ¦‚æ˜¯è¿™æ ·åšçš„ 


[[../../ç†è§£/utf-8.md]]
UTF-8 çš„ç›®çš„ä¸æ˜¯å‹ç¼©å­—ç¬¦æ•°é‡ï¼Œè€Œæ˜¯æŠŠä»»æ„ Unicode å­—ç¬¦æ˜ å°„æˆç”±æœ‰é™çš„ byteï¼ˆ0â€“255ï¼‰ç»„æˆçš„åºåˆ—ï¼Œä»è€Œç”¨å¯å˜é•¿åº¦ç¼–ç è¡¨ç¤ºæ‰€æœ‰å­—ç¬¦ï¼Œå¹¶ä¿è¯å‘åå’Œå‘å‰å…¼å®¹ã€‚


### Problem(unicode2):


(b)
The function incorrectly decodes each byte separately, but UTF-8 characters may span multiple bytes, causing multi-byte characters such as "ã“" to be decoded incorrectly.

## 2.4 BPE Tokenizer Training
(æ€»ç»“ç‰ˆ ç°åœ¨ä¸è§£é‡Š)
raw text 
 â†’ special token handling
 â†’ pre-tokenizer (regex)
 â†’ UTF-8 bytes
 â†’ BPE merges
 â†’ token IDs

### Pre-tokenization 

Pre-tokenizer çš„ä½œç”¨ä¸æ˜¯â€œå†³å®šæœ€ç»ˆ tokenâ€ï¼Œè€Œæ˜¯æŠŠæ–‡æœ¬åˆ‡æˆâ€œåˆç†çš„å°å—â€ï¼Œè®© BPE åœ¨è¿™äº›å—å†…éƒ¨ç»Ÿè®¡å’Œ mergeã€‚
```python
>>> # requires `regex` package
>>> import regex as re
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```
å¯ä»¥çœ‹å‡º pre-rtokenizationæ˜¯å…ˆå°†å¥å­å˜æˆä¸€ä¸ªä¸ªè¯å’Œç¬¦å·å’Œ


è¸©çš„å‘ æ­£åˆ™è¡¨è¾¾å¼ regex å†™äº†åˆ†æ®µ
åªèƒ½ä¸€è¡Œ
å…·ä½“çœ‹ ./ç†è§£/pre-tokenizer-regex

### ä½¿ç”¨re.finditer è€Œä¸æ˜¯ re.findall
```python
>>> re.findall(PAT, "some text that i'll pre-tokenize")
>>>['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']

for m in re.finditer(PAT2, english_words, flags=re.VERBOSE):
    print(m.group(0))

The original sentence using PAT2 'some text that i'll pre-tokenize'
some
 text
 that
 i
\'ll
 pre
-
tokenize
```
#### re.findall
å¯è§ re.finall(pattern,text)
ä¼šç”Ÿæˆä¸€ä¸ªlist

#### re.finditer
ä½†æ˜¯re.finditer(pattern,text,flag = re.VERBOSE) æ˜¯ä¸€ä¸ªscanner
å¾—ç”¨ for m in ä¸Šè¿°è¿™ä¸ª
ç„¶åæ¯ä¸ªscançš„ éƒ½å¾—ç”¨ m.group(0)

#### åˆ›å»ºå­—å…¸
```python
counts = {}
```
ç›´æ¥ç”Ÿæˆå­—å…¸
å¦‚æœä½¿ç”¨ç›´æ¥æŒ‰ç…§whitespaceåˆ†çš„è¯
æ˜¯text_list = text.split(" ")
æ³¨æ„ç©ºæ ¼ ä»¥åŠè¿”å›çš„æ˜¯list

### 2.4A python é‡Œæ€ä¹ˆåˆ›å»ºå’ŒæŸ¥çœ‹`bytes`
1. é¦–å…ˆbæ˜¯ä¸€ä¸ªbytes å¯¹è±¡ 
  b = b'low'
  bytes æ˜¯ Python çš„ä¸€ä¸ªå†…å»ºç±»å‹

2. å…¶æ¬¡ bytesæ˜¯ä¸€ä¸ª ä¸å¯å˜çš„å­—èŠ‚åºåˆ— 
  æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 0~255 çš„ intï¼ˆ8-bitï¼‰
  
  å› ä¸ºæ˜¯å­—èŠ‚åºåˆ—ï¼ˆsequenceï¼‰
  æ‰€ä»¥å¯ä»¥éå†
  éå†æ—¶è¿”å›çš„ä¹Ÿæ˜¯ int
```python
listï¼ˆbï¼‰
#[108,111,119]  æ¯ä¸ª int æ˜¯è¯¥å­—èŠ‚çš„â€œæ•°å€¼â€ âœ” å–å€¼èŒƒå›´åˆšå¥½æ˜¯ 0~255
    # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 8-bit çš„å­—èŠ‚
    # Python ç”¨ 0~255 çš„ int æ¥è¡¨ç¤ºè¿™ä¸ªå­—èŠ‚çš„å€¼
```

3. bytes(intå€¼) æ˜¯åˆ›å»ºä¸€ä¸ª é•¿åº¦ä¸º n çš„ bytes å¯¹è±¡
    æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯ 0ï¼ˆ\x00ï¼‰

4. ä½†æ˜¯å¦‚æœæ˜¯ bytes([n]) 
    åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ bytes å¯¹è±¡
    ğŸ‘‰ è¿™ä¸ªå­—èŠ‚çš„å€¼æ˜¯ n
    âœ” [n] æ˜¯ä¸€ä¸ª iterable
    âœ” è¡¨ç¤ºâ€œæˆ‘è¦ä¸€ä¸ªå€¼ä¸º n çš„å­—èŠ‚â€

5.ç´¢å¼•è¿”å› int åˆ‡ç‰‡è¿”å› bytes


##### 1 `b"..."` literal
```python
b"abc"          # åªèƒ½ç›´æ¥å†™ ASCII å­—ç¬¦
b"\xe4\xb8\xad" # å¯ä»¥ç”¨åå…­è¿›åˆ¶è½¬ä¹‰å†™ä»»æ„å­—èŠ‚
```

##### 2 `str.encode(...)`ï¼šä»æ–‡æœ¬åˆ°å­—èŠ‚
```python
s = "ä¸­æ–‡"
bs = s.encode("utf-8")
# bs æ˜¯ bytes
```
 
##### 3 `bytes.decode(...)` ä»å­—èŠ‚åˆ°æ–‡æœ¬
```python
bs = b"\xe4\xb8\xad\xe6\x96\x87"
s = bs.decode("utf-8")
```


##### 4 `bytes([x,y,z])` ç”¨æ•´æ•°åˆ—è¡¨æ„é€ 
```python
bytes([65, 66, 67])  # b'ABC'
```

##### 5 bytes çš„â€œå…ƒç´ â€æ˜¯ä»€ä¹ˆ
```python
b = b"ABC"
b[0]      # 65ï¼ˆintï¼‰
b[0:2]    # b'AB'ï¼ˆbytesï¼‰
list(b)   # [65, 66, 67]
```



#### 2.4B è¯»å–æ–‡ä»¶
https://chatgpt.com/g/g-p-693f75d2365c8191baf9aaa7038e3595-cs336xiao-xi-jie/c/6948a752-5e40-832e-bb7f-f6299b8b04be
##### 1. open() ä¸ä¸Šä¸‹æ–‡ç®¡ç†å™¨ with

ä¸€å®šä¼˜å…ˆç”¨ with open(...) as f:ï¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€å¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®é‡Šæ”¾èµ„æºã€‚
ä¸ç”¨ with å°±è¦æ‰‹åŠ¨ f.close()ï¼Œå¾ˆå®¹æ˜“å¿˜ã€‚

```python
from pathlib import Path
path = Path("data.txt")
with path.open("r", encoding="utf-8") as f:
    text = f.read()
```

1ï¸âƒ£ ä½ è¦è§£å†³çš„å®é™…é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ BPE è®­ç»ƒå‰ï¼Œè¦æ±‚ï¼š
special tokensï¼ˆå¦‚ <|endoftext|>ï¼‰ä¸èƒ½å‚ä¸ pre-tokenization

ä¸èƒ½è·¨ special token åš merge
æ‰€ä»¥æµç¨‹æ˜¯ï¼š

å…ˆæŒ‰ special tokens åˆ‡æ–‡æœ¬ â†’ æ¯ä¸€æ®µå•ç‹¬åš regex pre-tokenization

1. ä¸ç”¨æ­£åˆ™ï¼ˆå¯¹æ¯”ç”¨ï¼‰
  ```python
    text = "A<X>B<X>C"
    print(text.split("<X>"))
  ```

2.  å¿…é¡»ã€Œè¿æ¥ä¸¤ä¸ªå¯åŒ¹é…çš„ä¸œè¥¿ã€

    æ­£åˆ™é‡Œçš„åŸºæœ¬å½¢å¼æ˜¯ï¼š
    A | B
    æ„æ€æ˜¯ï¼š
    åŒ¹é… A æˆ– B
    æ‰€ä»¥ä½ å¿…é¡»ç»™å®ƒå·¦æ“ä½œæ•°å’Œå³æ“ä½œæ•°ï¼š
    <X> | <Y>

3. join åšçš„äº‹ï¼ˆéå¸¸ç²¾ç¡®ï¼‰
    "|".join(["<X>", "<Y>", "<Z>"])
    ç»“æœï¼š
    "<X>|<Y>|<Z>"
    join éœ€è¦ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ æ‰€ä»¥generator expressionå’Œ list comprehensionéƒ½è¡Œ
```python
"|".join(re.escape(tok) for tok in special_tokens)
"|".join([re.escape(tok) for tok in special_tokens])
```
```python
(re.escape(tok) for tok in special_tokens)
```
å®ƒäº§ç”Ÿçš„æ˜¯ï¼š
ä¸€ä¸ª æƒ°æ€§ iterable
æ¯æ¬¡ join éœ€è¦ä¸‹ä¸€ä¸ªå…ƒç´ æ—¶æ‰è®¡ç®—

4. re.escape
    æ˜¯æŠŠç‰¹æ®Šçš„ç¬¦å· ä¾‹å¦‚ 
    special token é€šå¸¸é•¿è¿™æ ·ï¼š
    <|endoftext|>
    ä½†åœ¨æ­£åˆ™é‡Œï¼š
    |
    <
    >
    éƒ½æœ‰ç‰¹æ®Šå«ä¹‰
    ğŸ‘‰ å¦‚æœä½ ç›´æ¥ç”¨ï¼Œä¼šè¢« regex è¯¯è§£
    âŒ é”™è¯¯ç¤ºä¾‹
    ```python
    pattern = "|".join(["<|endoftext|>"])
    ```

### 2.4C sum
`sum(iterable, start=0) `
ç­‰ä»·äºï¼š

```python
total = start
for x in iterable:
    total = total + x
return total
```

1) æ±‚æ•°å­—å’Œï¼ˆæœ€å¸¸è§ï¼‰

```python
nums = [1, 2, 3]
total = sum(nums)          # 6
total2 = sum(nums, 10)     # 16  (ä» 10 å¼€å§‹åŠ )
```

2) å¯¹ç”Ÿæˆå™¨æ±‚å’Œï¼ˆçœå†…å­˜ï¼‰
```python
total = sum(i*i for i in range(10))
```

3) åˆå¹¶â€œæ”¯æŒ + çš„å¯¹è±¡â€ï¼Œç”¨ start æŒ‡å®šåˆå§‹å€¼
```python
from collections import Counter
counters = [Counter("ab"), Counter("bc")]
merged = sum(counters, Counter())
```

#####

### 2.4D Counter 
1) ç›´æ¥ç»Ÿè®¡é¢‘æ¬¡
```python
from collections import Counter
cnt = Counter("banana")
# Counter({'a': 3, 'n': 2, 'b': 1})
```

2) é€æ­¥ç´¯åŠ ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
cnt = Counter()
for tok in tokens:
    cnt[tok] += 1
```

3) æ‰¹é‡æ›´æ–°
```python
cnt.update(tokens)              # tokens æ˜¯ iterable
cnt.update({"a": 2, "b": 1})    # ä¹Ÿå¯ä»¥æ˜¯ mapping
```

4) top-k é«˜é¢‘
```python
cnt.most_common(5)
```

5) åˆå¹¶/åŠ å‡ï¼ˆè®¡æ•°è¯­ä¹‰ï¼‰
```python
c1 = Counter(a=2, b=1)
c2 = Counter(a=1, b=5, c=1)

c1 + c2   # è®¡æ•°ç›¸åŠ 
c1 - c2   # è®¡æ•°ç›¸å‡ï¼ˆä¼šè¿‡æ»¤ <=0ï¼‰
c1 & c2   # æ¯ä¸ªé”®å– min
c1 | c2   # æ¯ä¸ªé”®å– max
```
6) ä¸å­˜åœ¨çš„é”®é»˜è®¤æ˜¯ 0
```python
cnt = Counter()
cnt["missing"]    # 0
```

### 2.5E Counter vs dictï¼šå¸¸è§ç”¨æ³•å¯¹æ¯”ï¼ˆä½ å†™ä½œä¸šæœ€å¸¸ç¢°åˆ°ï¼‰
A) è®¡æ•°ï¼ˆtoken -> æ¬¡æ•°ï¼‰

âœ… ç”¨ Counter

```python
cnt = Counter()
cnt[tok] += 1
```

ç”¨ dict ä¹Ÿèƒ½åšï¼Œä½†æ›´å•°å—¦ï¼š
```python
d = {}
d[tok] = d.get(tok, 0) + 1
```
B) æ˜ å°„/ç´¢å¼•ï¼ˆtoken -> id, id -> tokenï¼‰

âœ… ç”¨ dictï¼ˆå³ä½¿ value æ˜¯æ•°å­—ä¹Ÿä¸€æ ·ï¼‰

```python
token2id = {"<pad>": 0, "<unk>": 1}
id2token = {v: k for k, v in token2id.items()}
```

C) åˆå¹¶å¤šä¸ªç»Ÿè®¡ç»“æœ

âœ… Counter.update / +=ï¼ˆæ¨èï¼‰

```python
total = Counter()
for c in sub_counters:
    total.update(c)     # æˆ– total += c
```

ä¹Ÿå¯ä»¥ç”¨ sum(sub_counters, Counter())ï¼ˆèƒ½ç”¨ä½†é€šå¸¸æ…¢ç‚¹ï¼‰

D) â€œä¸å­˜åœ¨â€è¦ä¸è¦ç®— 0

è¦å½“ 0ï¼šCounter æ›´è‡ªç„¶

è¦ä¸¥æ ¼åŒºåˆ†ä¸å­˜åœ¨ vs å€¼ä¸º 0ï¼šdict æ›´æ˜ç¡®



### 2.4E å¹¶è¡Œ multiprocessing

ä¸¤ä¸ªç»„æˆå…ƒç´ 
ä¸€ä¸ªæ˜¯worker function
    æ‹¿åˆ°çš„æ˜¯y ä½œä¸ºiterableä¸­çš„ä¸€ä»½ä½œä¸ºå‚æ•°
    

ä¸€ä¸ªæ˜¯å¹¶è¡Œçš„caller
    pool.map()
      ä¼šæŠŠpool.map(x,y)
      yä¸­æ¯ä¸€ä¸ªå…ƒç´  
      ä½œä¸ºå®å‚ä¼ ç»™x 
      å³taskï¼ˆpiece of yï¼‰
      
```python
def task1(x:str)->list[str]:
    # print(x) # æ‹¿åˆ°çš„æ˜¯ string
    token_list = x.split(" ")
    # print(token_list) # è¿”å›çš„æ˜¯list
    return token_list


if __name__ == "__main__":
    with Pool(4) as pool:
        results = pool.map(task1, list_text)
    # print(results)

# æ­¤å¤„å¯è§è¿™é‡Œè¿”å›çš„æ˜¯ä¸€ä¸ªlist
```
### Compute BPE merges è®¡ç®—BPE merges

BPE ç®—æ³• 
1. iteratively æ•°æ¯ä¸€å¯¹bytes å¹¶ä¸” è¯†åˆ«æœ€é«˜çš„ä¸€å¯¹
2. ç»Ÿè®¡å‡ºç°æœ€å¤šçš„ä¸€å¯¹ 
3. åˆå¹¶ ä» 'a' 'b' å˜æˆab
4. æŠŠåˆå¹¶çš„åŠ å…¥vocabulary 
5. æœ€åçš„vocabularyä¼šæ˜¯256ä¸ª åˆå§‹çš„ åŠ ä¸Š bpeåœ¨è®­ç»ƒä¸­èåˆçš„
6. ä¸è€ƒè™‘è·¨pre-tokençš„è¾¹ç•Œ 
7. å¦‚æœå‡ºç°tieçš„æƒ…å†µ ä½¿ç”¨lexicographically larger
 ï¼ˆä»€ä¹ˆæ˜¯lexicographically larger å¾…è¡¥å®Œ ï¼‰
```python
max([(b'l', b'o'), (b'o', b'w')]) == (b'o', b'w')
```



