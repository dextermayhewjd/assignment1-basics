# 12-14
# 2 Byte-Pair Encoding (BPE) tokenizer
## 2.1 The Unicode Standard

å’Œpdfé‡Œå†™çš„ä¸€æ · unicode defineäº†15000+çš„character   
s æ˜¯code point 115
```python
>>> z = ord('s')
115

>>> chr(115)
's'
```
æ­¤å¤„å¯è§ord() : 
  convert ä¸€ä¸ª single Unicode character ä¸€ä¸ªunicodeçš„å­—ç¬¦
  å˜æˆ its integer representation ä»–çš„æ•´æ•°è¡¨ç¤º 
  è¿™ä¸ªinteger æŒ‡çš„å°±æ˜¯ Unicode code point
chr():
  æŠŠä¸€ä¸ª æ•´æ•°è¡¨è¾¾
  (è¿™ä¸ªæ•´æ•°å¿…é¡»æ˜¯ä¸€ä¸ª valid Unicode code point) 
  è½¬æ¢æˆå¯¹åº”çš„ single Unicode character
  (è¿”å›å€¼åœ¨ Python é‡Œæ˜¯ä¸€ä¸ª é•¿åº¦ä¸º 1 çš„å­—ç¬¦ä¸²)

print(chr(0))
- è¾“å‡ºçš„æ˜¯ å­—ç¬¦æœ¬èº«
- è¯¥å­—ç¬¦æ˜¯ NULLï¼ˆä¸å¯æ‰“å°ï¼‰
- æ‰€ä»¥ç»ˆç«¯ çœ‹èµ·æ¥ä»€ä¹ˆéƒ½æ²¡æœ‰

repr(chr(0))
- è¾“å‡ºçš„æ˜¯ escaped representation
- ä½¿ç”¨çš„æ˜¯ åå…­è¿›åˆ¶å½¢å¼ \x00
- è¿™æ˜¯ä¸ºäº† è®©äººç±»å’Œè°ƒè¯•å™¨å¯è¯»

NULLæœ¬èº«è¿æ‰“å° éƒ½ä¸å ä½ç½®
```python
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```
## 2.2 Unicode Encodings

å› ä¸ºç›´æ¥ç”¨unicodeä¸ç°å®
150k çš„item å¾ˆå¤šéƒ½ç”¨ä¸åˆ°
å¾ˆå¤šå­—ç¬¦åŠå…¶ç½•è§
vocab ä¼šå¾ˆç¨€ç–

æˆ‘ä»¬ç›´æ¥ç”¨unicode encoding UTF-8
å› ä¸º
ä¸ç”¨ç»†çœ‹ ä½†æ˜¯å¤§æ¦‚æ˜¯è¿™æ ·åšçš„ 


[[../../ç†è§£/utf-8.md]]
UTF-8 çš„ç›®çš„ä¸æ˜¯å‹ç¼©å­—ç¬¦æ•°é‡ï¼Œè€Œæ˜¯æŠŠä»»æ„ Unicode å­—ç¬¦æ˜ å°„æˆç”±æœ‰é™çš„ byteï¼ˆ0â€“255ï¼‰ç»„æˆçš„åºåˆ—ï¼Œä»è€Œç”¨å¯å˜é•¿åº¦ç¼–ç è¡¨ç¤ºæ‰€æœ‰å­—ç¬¦ï¼Œå¹¶ä¿è¯å‘åå’Œå‘å‰å…¼å®¹ã€‚


### Problem(unicode2):


(b)
The function incorrectly decodes each byte separately, but UTF-8 characters may span multiple bytes, causing multi-byte characters such as "ã“" to be decoded incorrectly.

## 2.4 BPE Tokenizer Training
(æ€»ç»“ç‰ˆ ç°åœ¨ä¸è§£é‡Š)
raw text 
 â†’ special token handling
 â†’ pre-tokenizer (regex)
 â†’ UTF-8 bytes
 â†’ BPE merges
 â†’ token IDs

### Pre-tokenization 

Pre-tokenizer çš„ä½œç”¨ä¸æ˜¯â€œå†³å®šæœ€ç»ˆ tokenâ€ï¼Œè€Œæ˜¯æŠŠæ–‡æœ¬åˆ‡æˆâ€œåˆç†çš„å°å—â€ï¼Œè®© BPE åœ¨è¿™äº›å—å†…éƒ¨ç»Ÿè®¡å’Œ mergeã€‚
```python
>>> # requires `regex` package
>>> import regex as re
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```
å¯ä»¥çœ‹å‡º pre-rtokenizationæ˜¯å…ˆå°†å¥å­å˜æˆä¸€ä¸ªä¸ªè¯å’Œç¬¦å·å’Œ


è¸©çš„å‘ æ­£åˆ™è¡¨è¾¾å¼ regex å†™äº†åˆ†æ®µ
åªèƒ½ä¸€è¡Œ
å…·ä½“çœ‹ ./ç†è§£/pre-tokenizer-regex

### ä½¿ç”¨re.finditer è€Œä¸æ˜¯ re.findall
```python
>>> re.findall(PAT, "some text that i'll pre-tokenize")
>>>['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']

for m in re.finditer(PAT2, english_words, flags=re.VERBOSE):
    print(m.group(0))

The original sentence using PAT2 'some text that i'll pre-tokenize'
some
 text
 that
 i
\'ll
 pre
-
tokenize
```
#### re.findall
å¯è§ re.finall(pattern,text)
ä¼šç”Ÿæˆä¸€ä¸ªlist

#### re.finditer
ä½†æ˜¯re.finditer(pattern,text,flag = re.VERBOSE) æ˜¯ä¸€ä¸ªscanner
å¾—ç”¨ for m in ä¸Šè¿°è¿™ä¸ª
ç„¶åæ¯ä¸ªscançš„ éƒ½å¾—ç”¨ m.group(0)

#### åˆ›å»ºå­—å…¸
```python
counts = {}
```
ç›´æ¥ç”Ÿæˆå­—å…¸
å¦‚æœä½¿ç”¨ç›´æ¥æŒ‰ç…§whitespaceåˆ†çš„è¯
æ˜¯text_list = text.split(" ")
æ³¨æ„ç©ºæ ¼ ä»¥åŠè¿”å›çš„æ˜¯list

### 2.4A python é‡Œæ€ä¹ˆåˆ›å»ºå’ŒæŸ¥çœ‹`bytes`
1. é¦–å…ˆbæ˜¯ä¸€ä¸ªbytes å¯¹è±¡ 
  b = b'low'
  bytes æ˜¯ Python çš„ä¸€ä¸ªå†…å»ºç±»å‹

2. å…¶æ¬¡ bytesæ˜¯ä¸€ä¸ª ä¸å¯å˜çš„å­—èŠ‚åºåˆ— 
  æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 0~255 çš„ intï¼ˆ8-bitï¼‰
  
  å› ä¸ºæ˜¯å­—èŠ‚åºåˆ—ï¼ˆsequenceï¼‰
  æ‰€ä»¥å¯ä»¥éå†
  éå†æ—¶è¿”å›çš„ä¹Ÿæ˜¯ int
```python
listï¼ˆbï¼‰
#[108,111,119]  æ¯ä¸ª int æ˜¯è¯¥å­—èŠ‚çš„â€œæ•°å€¼â€ âœ” å–å€¼èŒƒå›´åˆšå¥½æ˜¯ 0~255
    # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 8-bit çš„å­—èŠ‚
    # Python ç”¨ 0~255 çš„ int æ¥è¡¨ç¤ºè¿™ä¸ªå­—èŠ‚çš„å€¼
```

3. bytes(intå€¼) æ˜¯åˆ›å»ºä¸€ä¸ª é•¿åº¦ä¸º n çš„ bytes å¯¹è±¡
    æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯ 0ï¼ˆ\x00ï¼‰

4. ä½†æ˜¯å¦‚æœæ˜¯ bytes([n]) 
    åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ bytes å¯¹è±¡
    ğŸ‘‰ è¿™ä¸ªå­—èŠ‚çš„å€¼æ˜¯ n
    âœ” [n] æ˜¯ä¸€ä¸ª iterable
    âœ” è¡¨ç¤ºâ€œæˆ‘è¦ä¸€ä¸ªå€¼ä¸º n çš„å­—èŠ‚â€

5.ç´¢å¼•è¿”å› int åˆ‡ç‰‡è¿”å› bytes


##### 1 `b"..."` literal
```python
b"abc"          # åªèƒ½ç›´æ¥å†™ ASCII å­—ç¬¦
b"\xe4\xb8\xad" # å¯ä»¥ç”¨åå…­è¿›åˆ¶è½¬ä¹‰å†™ä»»æ„å­—èŠ‚
```

##### 2 `str.encode(...)`ï¼šä»æ–‡æœ¬åˆ°å­—èŠ‚
```python
s = "ä¸­æ–‡"
bs = s.encode("utf-8")
# bs æ˜¯ bytes
```
 
##### 3 `bytes.decode(...)` ä»å­—èŠ‚åˆ°æ–‡æœ¬
```python
bs = b"\xe4\xb8\xad\xe6\x96\x87"
s = bs.decode("utf-8")
```


##### 4 `bytes([x,y,z])` ç”¨æ•´æ•°åˆ—è¡¨æ„é€ 
```python
bytes([65, 66, 67])  # b'ABC'
```

##### 5 bytes çš„â€œå…ƒç´ â€æ˜¯ä»€ä¹ˆ
```python
b = b"ABC"
b[0]      # 65ï¼ˆintï¼‰
b[0:2]    # b'AB'ï¼ˆbytesï¼‰
list(b)   # [65, 66, 67]
```



#### 2.4B è¯»å–æ–‡ä»¶
https://chatgpt.com/g/g-p-693f75d2365c8191baf9aaa7038e3595-cs336xiao-xi-jie/c/6948a752-5e40-832e-bb7f-f6299b8b04be
##### 1. open() ä¸ä¸Šä¸‹æ–‡ç®¡ç†å™¨ with

ä¸€å®šä¼˜å…ˆç”¨ with open(...) as f:ï¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€å¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®é‡Šæ”¾èµ„æºã€‚
ä¸ç”¨ with å°±è¦æ‰‹åŠ¨ f.close()ï¼Œå¾ˆå®¹æ˜“å¿˜ã€‚

```python
from pathlib import Path
path = Path("data.txt")
with path.open("r", encoding="utf-8") as f:
    text = f.read()
```

1ï¸âƒ£ ä½ è¦è§£å†³çš„å®é™…é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ BPE è®­ç»ƒå‰ï¼Œè¦æ±‚ï¼š
special tokensï¼ˆå¦‚ <|endoftext|>ï¼‰ä¸èƒ½å‚ä¸ pre-tokenization

ä¸èƒ½è·¨ special token åš merge
æ‰€ä»¥æµç¨‹æ˜¯ï¼š

å…ˆæŒ‰ special tokens åˆ‡æ–‡æœ¬ â†’ æ¯ä¸€æ®µå•ç‹¬åš regex pre-tokenization

1. ä¸ç”¨æ­£åˆ™ï¼ˆå¯¹æ¯”ç”¨ï¼‰
  ```python
    text = "A<X>B<X>C"
    print(text.split("<X>"))
  ```

2.  å¿…é¡»ã€Œè¿æ¥ä¸¤ä¸ªå¯åŒ¹é…çš„ä¸œè¥¿ã€

    æ­£åˆ™é‡Œçš„åŸºæœ¬å½¢å¼æ˜¯ï¼š
    A | B
    æ„æ€æ˜¯ï¼š
    åŒ¹é… A æˆ– B
    æ‰€ä»¥ä½ å¿…é¡»ç»™å®ƒå·¦æ“ä½œæ•°å’Œå³æ“ä½œæ•°ï¼š
    <X> | <Y>

3. join åšçš„äº‹ï¼ˆéå¸¸ç²¾ç¡®ï¼‰
    "|".join(["<X>", "<Y>", "<Z>"])
    ç»“æœï¼š
    "<X>|<Y>|<Z>"
    join éœ€è¦ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ æ‰€ä»¥generator expressionå’Œ list comprehensionéƒ½è¡Œ
```python
"|".join(re.escape(tok) for tok in special_tokens)
"|".join([re.escape(tok) for tok in special_tokens])
```
```python
(re.escape(tok) for tok in special_tokens)
```
å®ƒäº§ç”Ÿçš„æ˜¯ï¼š
ä¸€ä¸ª æƒ°æ€§ iterable
æ¯æ¬¡ join éœ€è¦ä¸‹ä¸€ä¸ªå…ƒç´ æ—¶æ‰è®¡ç®—

4. re.escape
    æ˜¯æŠŠç‰¹æ®Šçš„ç¬¦å· ä¾‹å¦‚ 
    special token é€šå¸¸é•¿è¿™æ ·ï¼š
    <|endoftext|>
    ä½†åœ¨æ­£åˆ™é‡Œï¼š
    |
    <
    >
    éƒ½æœ‰ç‰¹æ®Šå«ä¹‰
    ğŸ‘‰ å¦‚æœä½ ç›´æ¥ç”¨ï¼Œä¼šè¢« regex è¯¯è§£
    âŒ é”™è¯¯ç¤ºä¾‹
    ```python
    pattern = "|".join(["<|endoftext|>"])
    ```

### 2.4C sum
`sum(iterable, start=0) `
ç­‰ä»·äºï¼š

```python
total = start
for x in iterable:
    total = total + x
return total
```

1) æ±‚æ•°å­—å’Œï¼ˆæœ€å¸¸è§ï¼‰

```python
nums = [1, 2, 3]
total = sum(nums)          # 6
total2 = sum(nums, 10)     # 16  (ä» 10 å¼€å§‹åŠ )
```

2) å¯¹ç”Ÿæˆå™¨æ±‚å’Œï¼ˆçœå†…å­˜ï¼‰
```python
total = sum(i*i for i in range(10))
```

3) åˆå¹¶â€œæ”¯æŒ + çš„å¯¹è±¡â€ï¼Œç”¨ start æŒ‡å®šåˆå§‹å€¼
```python
from collections import Counter
counters = [Counter("ab"), Counter("bc")]
merged = sum(counters, Counter())
```

#####

### 2.4D Counter 
1) ç›´æ¥ç»Ÿè®¡é¢‘æ¬¡
```python
from collections import Counter
cnt = Counter("banana")
# Counter({'a': 3, 'n': 2, 'b': 1})
```

2) é€æ­¥ç´¯åŠ ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
cnt = Counter()
for tok in tokens:
    cnt[tok] += 1
```

3) æ‰¹é‡æ›´æ–°
```python
cnt.update(tokens)              # tokens æ˜¯ iterable
cnt.update({"a": 2, "b": 1})    # ä¹Ÿå¯ä»¥æ˜¯ mapping
```

4) top-k é«˜é¢‘
```python
cnt.most_common(5)
```

5) åˆå¹¶/åŠ å‡ï¼ˆè®¡æ•°è¯­ä¹‰ï¼‰
```python
c1 = Counter(a=2, b=1)
c2 = Counter(a=1, b=5, c=1)

c1 + c2   # è®¡æ•°ç›¸åŠ 
c1 - c2   # è®¡æ•°ç›¸å‡ï¼ˆä¼šè¿‡æ»¤ <=0ï¼‰
c1 & c2   # æ¯ä¸ªé”®å– min
c1 | c2   # æ¯ä¸ªé”®å– max
```
6) ä¸å­˜åœ¨çš„é”®é»˜è®¤æ˜¯ 0
```python
cnt = Counter()
cnt["missing"]    # 0
```

### 2.5E Counter vs dictï¼šå¸¸è§ç”¨æ³•å¯¹æ¯”ï¼ˆä½ å†™ä½œä¸šæœ€å¸¸ç¢°åˆ°ï¼‰
A) è®¡æ•°ï¼ˆtoken -> æ¬¡æ•°ï¼‰

âœ… ç”¨ Counter

```python
cnt = Counter()
cnt[tok] += 1
```

ç”¨ dict ä¹Ÿèƒ½åšï¼Œä½†æ›´å•°å—¦ï¼š
```python
d = {}
d[tok] = d.get(tok, 0) + 1
```
B) æ˜ å°„/ç´¢å¼•ï¼ˆtoken -> id, id -> tokenï¼‰

âœ… ç”¨ dictï¼ˆå³ä½¿ value æ˜¯æ•°å­—ä¹Ÿä¸€æ ·ï¼‰

```python
token2id = {"<pad>": 0, "<unk>": 1}
id2token = {v: k for k, v in token2id.items()}
```

C) åˆå¹¶å¤šä¸ªç»Ÿè®¡ç»“æœ

âœ… Counter.update / +=ï¼ˆæ¨èï¼‰

```python
total = Counter()
for c in sub_counters:
    total.update(c)     # æˆ– total += c
```

ä¹Ÿå¯ä»¥ç”¨ sum(sub_counters, Counter())ï¼ˆèƒ½ç”¨ä½†é€šå¸¸æ…¢ç‚¹ï¼‰

D) â€œä¸å­˜åœ¨â€è¦ä¸è¦ç®— 0

è¦å½“ 0ï¼šCounter æ›´è‡ªç„¶

è¦ä¸¥æ ¼åŒºåˆ†ä¸å­˜åœ¨ vs å€¼ä¸º 0ï¼šdict æ›´æ˜ç¡®



### 2.4E å¹¶è¡Œ multiprocessing

#### map + sum  

```python
with Pool(num_processes) as pool:
sub_counters = pool.map(parallel_worker, worker_parameters)
```

ä¸¤ä¸ªç»„æˆå…ƒç´ 
ä¸€ä¸ªæ˜¯worker function
    æ‹¿åˆ°çš„æ˜¯y ä½œä¸ºiterableä¸­çš„ä¸€ä»½ä½œä¸ºå‚æ•°
    

ä¸€ä¸ªæ˜¯å¹¶è¡Œçš„caller
    pool.map()
      ä¼šæŠŠpool.map(x,y)
      yä¸­æ¯ä¸€ä¸ªå…ƒç´  
      ä½œä¸ºå®å‚ä¼ ç»™x 
      å³taskï¼ˆpiece of yï¼‰
      
```python
def task1(x:str)->list[str]:
    # print(x) # æ‹¿åˆ°çš„æ˜¯ string
    token_list = x.split(" ")
    # print(token_list) # è¿”å›çš„æ˜¯list
    return token_list


if __name__ == "__main__":

    with Pool(4) as pool:
        results = pool.map(task1, list_text)# è¿™ä¸ªresultsæ˜¯ä¸€ä¸ªlist

    bytes_counts = sum(sub_counters, Counter()) # è¿™é‡Œå¯è§ sum æŠŠè¿”å›çš„è¿™ä¸ªiterable ç»™ä¸€æ¬¡æ€§æ‰«æå½’å¹¶
```
##### åœ¨åšä»€ä¹ˆ
map ä¼š ç­‰æ‰€æœ‰ worker éƒ½å®Œæˆ
æ‰€æœ‰ Counter ä¸€æ¬¡æ€§è¿”å›æˆä¸€ä¸ª list
æœ€åç”¨ sum æŠŠå®ƒä»¬åˆå¹¶

##### ä¼˜ç‚¹
âœ… ä»£ç æç®€
å¾ˆâ€œå‡½æ•°å¼â€ï¼Œä¸€çœ¼å°±æ‡‚
âœ… åˆå¹¶é€»è¾‘å°‘
sum å†…éƒ¨æ˜¯çº¿æ€§ mergeï¼Œæ¬¡æ•° = len(sub_counters)

##### è‡´å‘½ç¼ºç‚¹ï¼ˆåœ¨ BPE é‡Œéå¸¸æ˜æ˜¾ï¼‰
âŒå†…å­˜å³°å€¼çˆ†ç‚¸
    åŒæ—¶å­˜åœ¨ï¼š
    æ‰€æœ‰ worker çš„ Counter
    æœ€ç»ˆåˆå¹¶ç”¨çš„ Counter
    åœ¨ TinyStories / OWT ä¸Šï¼š
    â— è¿™æ˜¯æœ€å®¹æ˜“ OOM / swap / å¡æ­» çš„å†™æ³•

âŒ æ²¡æœ‰æµå¼åé¦ˆ
    tqdm åªèƒ½åŒ… mapï¼Œä½†ç»“æœæ²¡å›æ¥ä¹‹å‰ä½ å•¥ä¹Ÿå¹²ä¸äº†
    çœ‹èµ·æ¥åƒâ€œå¡ä½äº†â€

#### imap_unordered + å¢é‡ç´¯åŠ ï¼ˆæµå¼ï¼‰
from multiprocessing import Pool

```python

bytes_counts = Counter()
with Pool(num_processes) as pool:
    for sub in pool.imap_unordered(
        parallel_worker,
        worker_parameters,
        chunksize=1,
    ):
        bytes_counts += sub
'''
chunk å¾ˆé‡ â†’ chunksize å°
chunk å¾ˆè½» â†’ chunksize å¤§
'''

```
##### åœ¨åšä»€ä¹ˆ
parallel_worker ä¸€ä¸ªä»»åŠ¡ â†’ ä¸€ä¸ª Counter
worker ç®—å®Œå°±ç«‹åˆ»è¿”å›
ä¸»è¿›ç¨‹ è¾¹æ”¶åˆ°ã€è¾¹ç´¯åŠ  åˆ° bytes_counts
é¡ºåº ä¸ä¿è¯ï¼ˆunorderedï¼‰

##### ä¼˜ç‚¹
âœ… å†…å­˜å ç”¨ä½ï¼ˆå…³é”®ï¼‰
    ä¸»è¿›ç¨‹é‡Œæ°¸è¿œåªå¤šä¸€ä¸ª sub Counter
    éå¸¸é€‚åˆ TinyStories / OWT è¿™ç§ Counter æå¤§ çš„åœºæ™¯
âœ… å»¶è¿Ÿä½
    ç¬¬ä¸€ä¸ª worker ä¸€ç®—å®Œä½ å°±å¼€å§‹ç´¯åŠ 
    tqdm èƒ½å®æ—¶åŠ¨ï¼Œæ–¹ä¾¿åˆ¤æ–­â€œæ˜¯ä¸æ˜¯å¡æ­»äº†â€
âœ… æ›´ç¨³
    å¦‚æœæŸä¸ª chunk ç‰¹åˆ«å¤§ï¼Œä¸ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ç»“æœå †åœ¨å†…å­˜é‡Œ

##### ç¼ºç‚¹
âŒ Python å±‚ += æ¬¡æ•°å¤š
    æ¯ä¸ª sub éƒ½è¦åšä¸€æ¬¡ Counter merge
    å¦‚æœ worker æ•°é‡ Ã— chunk æ•°é‡ç‰¹åˆ«å¤§ï¼Œä¼šæœ‰ä¸€äº› Python å¼€é”€


### Compute BPE merges è®¡ç®—BPE merges

BPE ç®—æ³• 
1. iteratively æ•°æ¯ä¸€å¯¹bytes å¹¶ä¸” è¯†åˆ«æœ€é«˜çš„ä¸€å¯¹
2. ç»Ÿè®¡å‡ºç°æœ€å¤šçš„ä¸€å¯¹ 
3. åˆå¹¶ ä» 'a' 'b' å˜æˆab
4. æŠŠåˆå¹¶çš„åŠ å…¥vocabulary 
5. æœ€åçš„vocabularyä¼šæ˜¯256ä¸ª åˆå§‹çš„ åŠ ä¸Š bpeåœ¨è®­ç»ƒä¸­èåˆçš„
6. ä¸è€ƒè™‘è·¨pre-tokençš„è¾¹ç•Œ 
7. å¦‚æœå‡ºç°tieçš„æƒ…å†µ ä½¿ç”¨lexicographically larger
 ï¼ˆä»€ä¹ˆæ˜¯lexicographically larger å¾…è¡¥å®Œ ï¼‰
```python
max([(b'l', b'o'), (b'o', b'w')]) == (b'o', b'w')
```

#### ä½¿ç”¨tqdm
##### 1ï¸âƒ£ åŒ…è£¹ä»»ä½•å¯è¿­ä»£å¯¹è±¡

```python
from tqdm import tqdm
for x in tqdm(data):
    ...
```

ğŸ‘‰ åŸåˆ™ï¼šå‡¡æ˜¯â€œäººä¼šç­‰â€çš„å¾ªç¯ï¼Œå°±è¯¥æœ‰ tqdm
    ï¼ˆtokenize / merge / train / scan file / pool.imapï¼‰

##### 2ï¸âƒ£ æ˜ç¡® totalï¼ˆéå¸¸é‡è¦ï¼‰
```python
for sub in tqdm(pool.imap_unordered(fn, items), total=len(items)):
```

å¦‚æœæ²¡æœ‰ totalï¼š
    ETA ä¸å‡†
    ç™¾åˆ†æ¯”ä¸æ˜¾ç¤º
    çœ‹èµ·æ¥åƒâ€œå¡ä½äº†â€ï¼ˆä½ é‡åˆ°è¿‡ï¼‰

##### 3ï¸âƒ£ ç»™ tqdm èµ·ä¸€ä¸ªâ€œè§£é‡Šå‹åå­—â€

```python
tqdm(..., desc="Pretokenize chunks")
tqdm(..., desc="BPE merges", unit="merge")
tqdm(..., desc="Counting byte pairs")
```

âŒ ä¸è¦ï¼š
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024

âœ… è¦ï¼š
    Pretokenize chunks:  35%|â–ˆâ–ˆâ–ˆâ–Œ | 350/1000
    desc = ä½ åœ¨ debug æ—¶çš„â€œå¿ƒç†å®‰å…¨æ„Ÿâ€

4ï¸âƒ£ unit æ˜¯ underrated çš„ç¥å™¨
```python
tqdm(..., unit="chunk")
tqdm(..., unit="merge")
tqdm(..., unit="MB")
```

æ¯”å¦‚ BPE mergeï¼š
```python
for _ in trange(num_merges, desc="BPE merges", unit="merge"):
```
ä¼šæ˜¾ç¤ºï¼š
    BPE merges:  1234/10000 merge




##### 4ï¸âƒ£ unit æ˜¯ underrated çš„ç¥å™¨
tqdm(..., unit="chunk")
tqdm(..., unit="merge")
tqdm(..., unit="MB")


æ¯”å¦‚ BPE mergeï¼š

for _ in trange(num_merges, desc="BPE merges", unit="merge"):
    ...


ä¼šæ˜¾ç¤ºï¼š

BPE merges:  1234/10000 merge

##### 5ï¸âƒ£ imap / imap_unordered + tqdmï¼ˆä½ æ­£åœ¨ç”¨çš„ï¼‰

```python
with Pool(num_processes) as pool:
    for sub in tqdm(
        pool.imap_unordered(worker, params, chunksize=1),
        total=len(params),
        desc="Pretokenize chunks",
    ):
        bytes_counts += sub
```


âœ… è¿™æ˜¯å¤šè¿›ç¨‹ + tqdm çš„æœ€ä¼˜è§£ä¹‹ä¸€
    ä¸ºä»€ä¹ˆä¸ç”¨ mapï¼Ÿ
    map ä¼š ç­‰æ‰€æœ‰ worker å®Œæˆæ‰è¿”å›
    tqdm å®Œå…¨æ²¡æ³•æ›´æ–°

#### Map reduce

âœ”ï¸ ç†è®ºä¸Šå†…å­˜å¤Ÿ
âœ”ï¸ ä½†å› ä¸ºåå¤äº§ç”Ÿå³°å€¼
âœ”ï¸ Linux è§¦å‘ swap
âœ”ï¸ swap çš„æ˜¯â€œè¿˜åœ¨ç”¨çš„ Python å¯¹è±¡â€
âœ”ï¸ å¯¼è‡´ page fault + swap å¾ªç¯
âœ”ï¸ ç¨‹åºçœ‹ä¼¼æ²¡æ­»ï¼Œå®åˆ™è¢« IO åæ‰
##### Map éƒ¨åˆ†çš„å°ç»†èŠ‚

1. åˆ›å»ºtmp_dir
##### ä¸€ã€pathlib.Pathï¼šä¸æ˜¯â€œæ›´å¥½çœ‹â€ï¼Œè€Œæ˜¯æ›´å°‘ bug
1ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç”¨ str + os.path äº†ï¼Ÿ
    ä¼ ç»Ÿå†™æ³•çš„é—®é¢˜ä½ è‚¯å®šå·²ç»è¸©è¿‡ï¼š
```python
path = "/tmp" + "/" + "part_0001.pkl"   # å®¹æ˜“é”™
path = os.path.join("/tmp", "part_0001.pkl")  # å¯è¯»æ€§å·®
```


Path çš„æ ¸å¿ƒä»·å€¼ä¸æ˜¯ç®€æ´ï¼Œæ˜¯:  

ğŸ‘‰ æŠŠâ€œè·¯å¾„â€å½“æˆä¸€ä¸ªå¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²

2ï¸âƒ£ Path çš„ä¸‰ä¸ªæœ€å¸¸ç”¨åŠ¨ä½œï¼ˆ90% åœºæ™¯:  
âœ… åˆ›å»º / è¡¨ç¤ºè·¯å¾„ï¼ˆä¸è®¿é—®ç£ç›˜ï¼‰
```python
from pathlib import Path
p = Path("/tmp/bpe")
```

âš ï¸ æ³¨æ„ï¼š 
è¿™ä¸€æ­¥ä¸ä¼šåˆ›å»ºç›®å½•  
åªæ˜¯ä¸€ä¸ªâ€œè·¯å¾„æè¿°å¯¹è±¡â€  

âœ… è·¯å¾„æ‹¼æ¥ï¼ˆæœ€é‡è¦ï¼‰
```python
p = Path("/tmp/bpe")
file = p / "part_0001.pkl"
```
ä½ ç°åœ¨ç”¨çš„æ­£æ˜¯è¿™ä¸ªï¼š  
path = tmp_dir / f"part_{i:04d}.pkl"


è¿™ä¸€æ­¥çš„éšå«å¥½å¤„ï¼š

1. è‡ªåŠ¨å¤„ç† /
2. Windows / Linux é€šç”¨
3. ä¸ä¼šå‡ºç° // æˆ–æ¼ /

âœ… ä¸è€ API äº¤äº’
```python
with open(path, "wb") as f:
open()
pickle.dump()
torch.save()
```
ğŸ‘‰ éƒ½å¤©ç„¶æ”¯æŒ Path

è¿™æ˜¯ Path èƒ½åœ¨å·¥ç¨‹é‡ŒçœŸæ­£â€œè½åœ°â€çš„åŸå› ã€‚

3ï¸âƒ£ Path åœ¨ä½ è¿™æ®µä»£ç é‡Œçš„çœŸå®ä»·å€¼
```python
tmp_dir = Path(tempfile.mkdtemp(...))
path = tmp_dir / f"part_{i:04d}.pkl"
```
ä½ åœ¨åšçš„æ˜¯ï¼š 
æŠŠ MapReduce çš„ä¸­é—´æ–‡ä»¶ï¼Œå½“æˆâ€œç»“æ„åŒ–è·¯å¾„ç©ºé—´â€æ¥ç®¡ç†  
è€Œä¸æ˜¯ï¼š 
åˆ°å¤„æ‹¼å­—ç¬¦ä¸²   
æä¸æ¸…å“ªäº›æ˜¯ç›®å½•ï¼Œå“ªäº›æ˜¯æ–‡ä»¶  
è¿™æ˜¯ infra é£æ ¼ï¼Œä¸æ˜¯è„šæœ¬é£æ ¼ã€‚  

##### äºŒã€tempfile.mkdtempï¼šä½ åœ¨å‘æ“ä½œç³»ç»Ÿâ€œç§Ÿä¸€å—å®‰å…¨ç©ºé—´â€
1ï¸âƒ£ mkdtemp åˆ°åº•å¹²äº†ä»€ä¹ˆï¼Ÿ  
```python
tmp = tempfile.mkdtemp() 
```
æ“ä½œç³»ç»Ÿå±‚é¢è¯­ä¹‰æ˜¯ï¼š
â€œå¸®æˆ‘åˆ›å»ºä¸€ä¸ª å”¯ä¸€ã€ä¸å†²çªã€å·²å­˜åœ¨ çš„ä¸´æ—¶ç›®å½•ï¼Œå¹¶æŠŠè·¯å¾„ç»™æˆ‘â€  
å®ƒä¿è¯äº†ä¸‰ä»¶äº‹ï¼š 
1. ç›®å½•ä¸€å®šå­˜åœ¨
2. åå­—ä¸ä¼šæ’
3. åˆ›å»ºè¿‡ç¨‹æ˜¯åŸå­çš„ï¼ˆçº¿ç¨‹/è¿›ç¨‹å®‰å…¨ï¼‰

2ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç”¨ mkdir("/tmp/xxx")ï¼Ÿ
å› ä¸ºè¿™æ®µä»£ç åœ¨å¹¶è¡Œ / å¤šè¿›ç¨‹ç¯å¢ƒé‡Œï¼š  
```python
os.makedirs("/tmp/bpe_mapreduce")
```
å¯èƒ½ä¼šï¼š
1. è¢«å¦ä¸€ä¸ªè¿›ç¨‹æŠ¢å…ˆåˆ›å»º
2. æŠ›å¼‚å¸¸
3. æˆ– silently è¦†ç›–  

è€Œ mkdtemp çš„è¯­ä¹‰æ˜¯ï¼š  
ğŸ‘‰ è¿™ä¸ªç›®å½•åªå±äºä½ è¿™ä¸€æ¬¡è¿è¡Œ  

3ï¸âƒ£ mkdtemp vs TemporaryDirectoryï¼ˆéå¸¸é‡è¦ï¼‰  
API	ä¼šè‡ªåŠ¨åˆ é™¤å—	é€‚åˆè°  
mkdtemp()	âŒ ä¸ä¼š	MapReduce / pipeline  
TemporaryDirectory()	âœ… with ç»“æŸè‡ªåŠ¨åˆ 	å°è„šæœ¬ / unit test  

ä½ ç°åœ¨ç”¨ mkdtemp æ˜¯å¯¹çš„ï¼Œå› ä¸ºï¼š

1. reduce å¯èƒ½å¾ˆä¹…  
2. å¯èƒ½è¦ checkpoint / debug  
3. ä¸­é€”å´©æºƒä½ è¿˜æƒ³ ls çœ‹æ–‡ä»¶  

4ï¸âƒ£ åœ¨ä½ è¿™ä¸ª MapReduce é‡Œçš„çœŸå®æ„ä¹‰
```python
tmp_dir = Path(tempfile.mkdtemp(prefix="bpe_mapreduce_"))
```  
ä½ å…¶å®æ˜¯åœ¨è¯´ï¼š  
â€œæˆ‘ç°åœ¨è¦å¼€å§‹ä¸€ä¸ª ç‹¬ç«‹çš„ MapReduce jobï¼Œ  
ç»™æˆ‘ä¸€ä¸ª å¹²å‡€çš„ scratch spaceã€‚â€   
è¿™æ˜¯ åˆ†å¸ƒå¼ç³»ç»Ÿæ€ç»´ï¼Œä¸æ˜¯ Python è¯­æ³•ç³–ã€‚   

##### ä¸‰ã€prefixï¼šä¸æ˜¯è£…é¥°ï¼Œæ˜¯â€œè°ƒè¯•ä¸è¿ç»´æ¥å£â€  
1ï¸âƒ£ prefix çš„ä½œç”¨éå¸¸æœ´ç´   
```python
tempfile.mkdtemp(prefix="bpe_mapreduce_")  
```
æœ€ç»ˆè·¯å¾„å¤§æ¦‚æ˜¯ï¼š  
```bash 
/tmp/bpe_mapreduce_8qk4v9n2  
```
prefix åšäº†ä»€ä¹ˆï¼Ÿ  
1. ç»™éšæœºç›®å½•ååŠ ä¸€ä¸ªäººç±»å¯è¯»æ ‡ç­¾  
2. å‰©ä¸‹éƒ¨åˆ†ä¿è¯å”¯ä¸€æ€§  

2ï¸âƒ£ ä¸ºä»€ä¹ˆ prefix åœ¨ infra ä»£ç é‡Œéå¸¸é‡è¦ï¼Ÿ

è®¾æƒ³ä½ åœ¨æœåŠ¡å™¨ä¸Šï¼š
```bash
ls /tmp
```
çœ‹åˆ°ï¼š
```bash
tmpa9f3
tmpz71k
bpe_mapreduce_8qk4v9n2
torch_compile_xxx
```

ä½ ä¸€çœ¼å°±çŸ¥é“ï¼š
1.å“ªä¸ªæ˜¯ä½ çš„ job
2. å“ªä¸ªå¯ä»¥åˆ 
3. å“ªä¸ªä¸æ•¢åŠ¨
ğŸ‘‰ è¿™å°±æ˜¯ è¿ç»´å¯è§‚æµ‹æ€§

3ï¸âƒ£ prefix çš„â€œéšè—å¥½å¤„â€   
âœ… å´©æºƒåå¯è¿½æº¯
ç¨‹åºæŒ‚äº†  
ä½ è¿˜èƒ½ cd /tmp/bpe_mapreduce_*
çœ‹ï¼š part_*.pkl
1. å“ªä¸ª chunk æœ€æ…¢
2. æ–‡ä»¶å¤§å°åˆ†å¸ƒ

âœ… å¤š job å¹¶è¡Œä¸æ··  
åŒæ—¶è·‘ TinyStories + OWT  
prefix ä¸åŒå³å¯åŒºåˆ†  

##### å››ã€enumerate å¹²çš„äº‹æƒ…
enumerate å¹²çš„äº‹æƒ…ï¼ˆéå¸¸é‡è¦ï¼‰
```python
for i, sub in enumerate(iterator):
```
enumerate åšçš„ä¸æ˜¯â€œæ¢å¤åŸ indexâ€ï¼Œè€Œæ˜¯ï¼š  
åœ¨æ¶ˆè´¹ iterator çš„é‚£ä¸€åˆ»ï¼ŒæŒ‰â€œåˆ°è¾¾é¡ºåºâ€åˆ†é…ä¸€ä¸ªæ–° id



### Problem(train_bpe_tinystories):BPETrainingonTinyStories (2points)

#### a  How many hours and memory did training take?What is the longest token in the vocabulary? Does it make sense?
1. Serialize the resulting vocabulary and merges to disk 
    for further inspection

1. éœ€è¦åˆ›å»ºå­˜æ”¾çš„æ–‡ä»¶å¤¹

```python
#å­˜æ”¾çš„ç›®å½• æ˜¯é¡¹ç›®æ–‡ä»¶å¤¹ä¸­çš„
OUT_DIR = "/home/dexterding/projects/assignment1-basics/bpe_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

VOCAB_PATH = os.path.join(OUT_DIR, "tinystories_vocab.pkl")
MERGES_PATH = os.path.join(OUT_DIR, "tinystories_merges.pkl")
PROFILE_PATH = os.path.join(OUT_DIR, "profile.txt")
```

2. å°†è¿”å›çš„vocabå’Œmerges ç”¨pickle.dump(XXXX,f) çš„æ–¹å¼å­˜æ”¾
```python
    # =========================
    # SAVE ARTIFACTS
    # =========================
    with open(VOCAB_PATH, "wb") as f:
        pickle.dump(vocab, f)

    with open(MERGES_PATH, "wb") as f:
        pickle.dump(merges, f)
```


3. å¦‚ä½•æµ‹è¯• æ‰€ç”¨çš„æ—¶é—´

åœ¨è®­ç»ƒçš„å¼€å¤´å’Œç»“å°¾åˆ†åˆ«æ”¾ä¸€ä¸ª 
```python
start_time = time.time()
## training code
end_time = time.time()
    print(f"Training time (this run): {(end_time - start_time)/60:.2f} minutes")

```

4. æµ‹è¯•vocabå’Œ merge å°±ç”¨ len(vocab/merges)

5. longest token 
å› ä¸º vocabçš„ç»“æ„å°±æ˜¯ dict[int,bytes]
5.1 å¯ä»¥åœ¨è·‘è®­ç»ƒçš„ä»£ç é‡Œæ”¾è¿™ä¸ª 
```python 
    longest_token = max(vocab.values(), key=len)
```
5.2 ä¹Ÿå¯ä»¥ç›´æ¥è¯»å–pickeleæ–‡ä»¶
```python
with open(VOCAB_PATH, "rb") as f:
    vocab = pickle.load(f)
tok_id, tok_bytes = max(vocab.items(), key=lambda kv: len(kv[1]))
print("longest_token_id:", tok_id)
print("byte_len:", len(tok_bytes))
print("raw_bytes_repr:", repr(tok_bytes))
print("decoded_utf8 (replace):", tok_bytes.decode("utf-8", errors="replace"))
```



#### b Profile your code. What part of the tokenizer training process takes the most time
```bash        
1           1262.415    1262.415 1766.558   1766.558 /home/dexterding/projects/assignment1-basics/cs336_basics/debug/parallel_train_bpe_cache.py:171(train_bpe)
5094536393  231.086     0.000    231.086    0.000 {built-in method builtins.len}
2255121258  121.755     0.000    121.755    0.000 {method 'append' of 'list' objects}
28286       51.462      0.002    80.908     0.003 {built-in method builtins.max}
9743        0.033       0.000    80.902     0.008 /home/dexterding/projects/assignment1-basics/cs336_basics/debug/parallel_train_bpe_cache.py:127(get_highest_pair)```

å¯è§å¤§å¤šæ•°æ—¶é—´æ˜¯ lenï¼ˆï¼‰å’Œ list.append å æ®å¤§å¤šæ•°æ—¶é—´ å› ä¸ºä¸€ç›´åœ¨å¤§è§„æ¨¡æ„å»º

Training time (this run): 7.15 minutes
Peak RSS (main + children): 7.47 GB
Main RSS delta (end-start): 0.06 GB
Vocab size: 10000 | merges: 9743
Longest token bytes: 13
```

## 2.6 BPE Tokenizer: Encoding and Decoding

### 2.6.1 Encoding text
#### å°ç»†èŠ‚
1.æ³¨æ„æ­¤å¤„çš„@classmethod å’Œ åç»­çš„return cls æœ¬è´¨ä¸Šæ˜¯ è¿”å›ä¸€ä¸ªclass instance

#### æ€è·¯ç†è§£ 
##### Step1: Pre-tokenize.
We first pre-tokenize the sequence and represent each pre-token as a sequence of UTF-8 bytes, just as we did in BPE training.

é¦–å…ˆè¿˜æ˜¯è¦æŠŠtext å˜æˆsequence of bytes 
é—®é¢˜æ˜¯æµç¨‹æ˜¯å¦è¿˜ä¸€æ ·
1. é¦–å…ˆsplit special token
å› ä¸ºæœ‰å¯èƒ½å‡ºç° "You are cat<|speical|>" 
å› ä¸ºspecial tokenå’Œå…¶ä»–è¯è¯­è¿ä¸€å— ä¸å¸Œæœ›tokenizeræŠŠ cat<|speical|>è¿åœ¨ä¸€èµ·
æ‰€ä»¥å°±è¦å…ˆsplitåå†ç”¨PATåˆ†è¯
ç„¶åå†é€ä¸ªencode(utf-8)

We will be merging these bytes within each pre-token into
vocabulary elements, handling each pre-token independently(nomergesacrosspre-tokenboundaries).

##### Step2:Apply the merges.
We then take the sequence of vocabulary element merges created during BPE
training, and apply it to our pre-tokens in the same order of creation.


 é˜²å¾¡æ€§ç¼–ç¨‹ encodeå®Œæˆ 
 é€»è¾‘æ˜¯greedyåŠ æ”¹å¾ªç¯å˜é‡ 
 é€šè¿‡æ›´æ”¹tokens 
 æ¥æ›´æ–°å¾ªç¯å˜é‡æ¡ä»¶ 
 ç”¨å­—å…¸ç”Ÿæˆå¼åè½¬



 # 3 Transformer Language Model Architecture

è¯­è¨€æ¨¡å‹çš„
è¾“å…¥æ˜¯ä¸€æ‰¹ï¼ˆbatchedï¼‰æ•´æ•°å½¢å¼çš„ token ID åºåˆ—  
ï¼ˆå³å½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ torch.Tensorï¼‰ï¼Œ

è¾“å‡ºæ˜¯ä¸€æ‰¹åœ¨è¯è¡¨ä¸Šçš„å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ  
ï¼ˆå³å½¢çŠ¶ä¸º `(batch_size, sequence_length, vocab_size)` çš„ PyTorch Tensorï¼‰ï¼Œ

å…¶ä¸­æ¯ä¸€ä¸ªä½ç½®ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œéƒ½æ˜¯åœ¨é¢„æµ‹â€œä¸‹ä¸€ä¸ª tokenâ€ã€‚

`åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶`   
æˆ‘ä»¬ä½¿ç”¨è¿™äº›â€œä¸‹ä¸€ä¸ªè¯â€çš„é¢„æµ‹predictionsç»“æœï¼Œ  
æ¥è®¡ç®—çœŸå®ä¸‹ä¸€ä¸ªè¯ä¸æ¨¡å‹é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µæŸå¤± cross-entropy lossã€‚

`åœ¨æ¨ç†ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰æ—¶`  
æˆ‘ä»¬åªå–åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„â€œä¸‹ä¸€ä¸ªè¯æ¦‚ç‡åˆ†å¸ƒâ€ï¼Œ  
ç”¨å®ƒæ¥ç”Ÿæˆä¸‹ä¸€ä¸ª token  
ï¼ˆä¾‹å¦‚ï¼šå–æœ€å¤§æ¦‚ç‡çš„ tokenï¼Œæˆ–ä»åˆ†å¸ƒä¸­é‡‡æ ·ï¼‰ï¼Œ  
ç„¶åæŠŠç”Ÿæˆçš„ token è¿½åŠ åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼Œ  
å¹¶ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚  

## 3.1 Transformer è¯­è¨€æ¨¡å‹ 

ç»™å®šä¸€æ®µ token ID åºåˆ—ï¼Œ
Transformer è¯­è¨€æ¨¡å‹ä¼šï¼š

1. ä½¿ç”¨è¾“å…¥ embeddingæŠŠ token ID è½¬æˆç¨ å¯†å‘é‡  

2. å°†è¿™äº›å‘é‡é€å…¥ num_layers ä¸ª Transformer block

3. å†é€šè¿‡ä¸€ä¸ªå­¦ä¹ å¾—åˆ°çš„çº¿æ€§æŠ•å½±  
ï¼ˆä¹Ÿå« output embedding æˆ– LM headï¼‰  
æ¥äº§ç”Ÿé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„ logits  
<img src="069ecc5a291e290743f71d6dcb6fccb.png" width="40%">

### 3.1.1 Token Embeddingsï¼ˆè¯å…ƒåµŒå…¥ï¼‰
åœ¨æœ€å¼€å§‹çš„ä¸€æ­¥ä¸­ï¼ŒTransformer ä¼šæŠŠï¼ˆæŒ‰ batch ç»„ç»‡çš„ï¼‰token ID åºåˆ—åµŒå…¥æˆä¸€ä¸²å‘é‡ï¼Œ  
è¿™äº›å‘é‡åŒ…å«äº† token èº«ä»½çš„ä¿¡æ¯ï¼ˆè§å›¾ 1 ä¸­çš„çº¢è‰²æ–¹å—ï¼‰ã€‚   

æ›´å…·ä½“åœ°è¯´ï¼Œç»™å®šä¸€ä¸ª token ID çš„åºåˆ—ï¼ŒTransformer è¯­è¨€æ¨¡å‹ä½¿ç”¨ä¸€ä¸ª token embedding å±‚ æ¥ç”Ÿæˆä¸€ç»„å‘é‡ã€‚  
è¿™ä¸ª embedding å±‚æ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º  
(batch_size, sequence_length) çš„æ•´æ•°å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œ
å¹¶è¾“å‡ºä¸€ä¸ªå½¢çŠ¶ä¸º  
(batch_size, sequence_length, d_model) çš„å‘é‡å¼ é‡ã€‚  

#### ç†è§£  
Embedding â‰  Linear  
æœ¬è´¨æ˜¯ï¼š  
embedding_matrix[token_id]  
è€Œä¸æ˜¯ x @ W  
æ‰€ä»¥åœ¨å®ç°ä¸­ä½ åªèƒ½ç”¨ç´¢å¼•ï¼Œä¸æ˜¯çŸ©é˜µä¹˜æ³•  

### 3.1.2 Pre-norm Transformer Block
åœ¨å®Œæˆ embedding ä¹‹åï¼Œæ¿€æ´»å€¼ä¼šè¢«é€å…¥å¤šä¸ªç»“æ„å®Œå…¨ç›¸åŒçš„ç¥ç»ç½‘ç»œå±‚ä¸­å¤„ç†ã€‚  
ä¸€ä¸ªæ ‡å‡†çš„ decoder-only Transformer è¯­è¨€æ¨¡å‹ ç”± num_layers ä¸ªå®Œå…¨ç›¸åŒçš„å±‚ç»„æˆï¼ˆé€šå¸¸ç§°ä¸º Transformer â€œblocksâ€ï¼‰ã€‚

æ¯ä¸ª block ä¼šï¼š
é€šè¿‡ è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰ åœ¨åºåˆ—ç»´åº¦ä¸Šèšåˆä¿¡æ¯
é€šè¿‡ å‰é¦ˆç½‘ç»œï¼ˆfeed-forward layersï¼‰ å¯¹ä¿¡æ¯è¿›è¡Œéçº¿æ€§å˜æ¢

<img src = "69ca137f59e992d3b43fd3ef257f83c.png" width="40%">

1. â€œidentically structuredâ€ æ˜¯å…³é”®  
æ‰€æœ‰ blockï¼š
ç»“æ„ä¸€æ ·  
å‚æ•°ä¸åŒ  
  ç±»ä¼¼ CNN é‡Œâ€œé‡å¤å †å å·ç§¯å±‚â€

2. decoder-only  
- åªæœ‰ self-attention  
- æ²¡æœ‰ encoder / cross-attention
- é…åˆ causal mask

3. ä¸ºä»€ä¹ˆè¾“å…¥è¾“å‡ºå½¢çŠ¶å¿…é¡»ä¸€æ ·
- å› ä¸ºæœ‰ residualï¼š
```python
x + f(x)
```

4. æ€»ç»“è¿™ä¸ª block
- â€œå…ˆçœ‹å…¨å±€ï¼ˆattentionï¼‰ï¼Œå†åšé€ä½ç½®çš„éçº¿æ€§å˜æ¢ï¼ˆFFNï¼‰â€

## 3.2 Output Normalization and Embedding

è¾“å‡ºå½’ä¸€åŒ–ä¸è¾“å‡ºåµŒå…¥

åœ¨ç»è¿‡ num_layers ä¸ª Transformer block ä¹‹åï¼Œæˆ‘ä»¬ä¼šå–æœ€ç»ˆçš„æ¿€æ´»å€¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªå¯¹æ•´ä¸ªè¯è¡¨çš„æ¦‚ç‡åˆ†å¸ƒã€‚  

æˆ‘ä»¬å°†å®ç° pre-norm Transformer blockï¼ˆè¯¦è§ Â§3.5ï¼‰ã€‚  
è¿™ç§ç»“æ„è¿˜è¦æ±‚åœ¨æœ€åä¸€ä¸ª Transformer block ä¹‹åï¼Œ  
å†é¢å¤–ä½¿ç”¨ä¸€æ¬¡ layer normalizationï¼Œä»¥ç¡®ä¿è¾“å‡ºçš„å°ºåº¦æ˜¯åˆé€‚çš„ã€‚  

åœ¨å®Œæˆå½’ä¸€åŒ–ä¹‹åï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ä¸€ä¸ªæ ‡å‡†çš„ã€å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼Œ  
å°† Transformer block çš„è¾“å‡ºè½¬æ¢ä¸ºé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„ logits  
ï¼ˆä¾‹å¦‚è§ Radford et al. [2018] çš„å…¬å¼ 2ï¼‰ã€‚


## 3.3 Remark: Batching, Einsum and Efficient Computation

åœ¨æ•´ä¸ª Transformer ä¸­ï¼Œæˆ‘ä»¬ä¼šåå¤å¯¹è®¸å¤šâ€œç±»ä¼¼ batch çš„è¾“å…¥â€æ‰§è¡Œç›¸åŒçš„è®¡ç®—ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š

1. Batch çš„å…ƒç´ ï¼šå¯¹ batch ä¸­çš„æ¯ä¸ªelement åº”ç”¨ç›¸åŒçš„ Transformer å‰å‘è®¡ç®—

2. åºåˆ—é•¿åº¦ç»´åº¦ï¼šåƒ RMSNorm å’Œå‰é¦ˆç½‘ç»œè¿™æ ·çš„â€œé€ä½ç½®ï¼ˆposition-wiseï¼‰â€æ“ä½œï¼Œå¯¹åºåˆ—ä¸­æ¯ä¸ªä½ç½®åšç›¸åŒçš„å¤„ç†

3. æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰ï¼šå¤šå¤´æ³¨æ„åŠ›ä¸­çš„ attention æ“ä½œï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨ head ç»´åº¦ä¸Šåš batch


```python
channels_last = torch.randn(64, 32, 32, 3) # (batch, height, width, channel)
B = torch.randn(32*32, 32*32)
## Rearrange an image tensor for mixing across all pixels
channels_last_flat = channels_last.view(-1, channels_last.size(1) * channels_last.size(2), channels_last.size(3)
)
'''
æ­¤å¤„ æ³¨æ„æ˜¯ channels_last.size(1) * channels_last.size(2)
'''
channels_first_flat = channels_last_flat.transpose(1, 2)
channels_first_flat_transformed = channels_first_flat @ B.T
channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)
channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)
'''
æ­¤å¤„ æ³¨æ„æ˜¯ ä½¿ç”¨çš„æ‹†åŒ… ç›´æ¥æ‹†å‡ºæ¥B H W C 
'''
Instead, using einops:
height = width = 32
## Rearrange replaces clunky torch view + transpose
channels_first = rearrange(
channels_last,
"batch height width channel-> batch channel (height width)"
)
channels_first_transformed = einsum(
channels_first, B,
"batch channel pixel_in, pixel_out pixel_in-> batch channel pixel_out"
)
channels_last_transformed = rearrange(
channels_first_transformed,
"batch channel (height width)-> batch height width channel",
height=height, width=width
)
```
### 3.3.1 æ•°å­¦è®°å·ä¸å†…å­˜é¡ºåºï¼ˆMathematical Notation and Memory Orderingï¼‰
è®¸å¤šæœºå™¨å­¦ä¹ è®ºæ–‡åœ¨æ•°å­¦è®°å·ä¸­ä½¿ç”¨è¡Œå‘é‡ï¼ˆrow vectorsï¼‰ï¼Œè¿™ç§è¡¨ç¤ºæ–¹å¼ä¸ NumPy å’Œ PyTorch é»˜è®¤ä½¿ç”¨çš„è¡Œä¸»åºï¼ˆrow-majorï¼‰å†…å­˜å¸ƒå±€éå¸¸å¥‘åˆã€‚

åœ¨ä½¿ç”¨è¡Œå‘é‡çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªçº¿æ€§å˜æ¢å¯ä»¥å†™æˆï¼š
$$ 
 y = x W^{T}
$$

å…¶ä¸­ï¼š
$$ 
ğ‘Šâˆˆğ‘…^{ğ‘‘_{out}Ã—ğ‘‘_{in}} 
$$
$$
xâˆˆğ‘…^{1Ã—d_{m}}
$$	â€‹

---
è€Œåœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œæ›´å¸¸è§çš„æ˜¯ä½¿ç”¨åˆ—å‘é‡ï¼ˆcolumn vectorsï¼‰ã€‚æ­¤æ—¶ï¼Œçº¿æ€§å˜æ¢å†™æˆï¼š
$$ 
y = Wx
$$ 
$$ 
ğ‘Šâˆˆğ‘…^{ğ‘‘_{out}Ã—ğ‘‘_{in}} 
$$
$$
xâˆˆğ‘…^{d_{in}}
$$	
 æ˜¯ä¸€ä¸ªåˆ—å‘é‡ column vector
 åœ¨æ•°å­¦æ¨å¯¼ä¸­ç»Ÿä¸€ä½¿ç”¨åˆ—å‘é‡è®°å·ï¼Œå› ä¸ºè¿™ç§æ–¹å¼åœ¨æ•°å­¦ä¸Šæ›´å®¹æ˜“ç†è§£ã€‚

ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼š
ğŸ‘‰ å¦‚æœä½ åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨çŸ©é˜µä¹˜æ³•ï¼ˆå¦‚ @ï¼‰ï¼Œå°±å¿…é¡»éµå¾ª è¡Œå‘é‡çº¦å®šï¼Œå› ä¸º PyTorch ä½¿ç”¨çš„æ˜¯è¡Œä¸»åºå†…å­˜å¸ƒå±€ã€‚

å¦‚æœä½ ä½¿ç”¨ einsum æ¥åšçŸ©é˜µè¿ç®—ï¼Œé‚£ä¹ˆè¿™ä¸ªé—®é¢˜åŸºæœ¬å¯ä»¥å¿½ç•¥ã€‚
## 3.4 åŸºæœ¬æ¨¡å—ï¼šLinear ä¸ Embedding
### 3.4.1 å‚æ•°åˆå§‹åŒ–
æœ‰æ•ˆè®­ç»ƒç¥ç»ç½‘ç»œé€šå¸¸éœ€è¦ç²¾å¿ƒè®¾è®¡çš„å‚æ•°åˆå§‹åŒ–ã€‚ç³Ÿç³•çš„åˆå§‹åŒ–ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚  
è™½ç„¶ Pre-norm Transformer å¯¹åˆå§‹åŒ–ç›¸å¯¹é²æ£’ï¼Œä½†åˆå§‹åŒ–æ–¹å¼ä»ä¼šæ˜¾è‘—å½±å“è®­ç»ƒé€Ÿåº¦å’Œæ”¶æ•›æ€§ã€‚   
åœ¨æœ¬æ¬¡ä½œä¸šä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹è¿‘ä¼¼åˆå§‹åŒ–æ–¹æ¡ˆï¼š  
å…¶ä»–çš„ç•™åˆ°assignment3 é‡Œ

#### Linear æƒé‡åˆå§‹åŒ–
$$
W \sim \mathcal{N}
\left(
0,\;
\frac{2}{d_{\text{in}} + d_{\text{out}}}
\right),
\quad W \in [-3\sigma, 3\sigma]
$$

â‘  æ­£æ€åˆ†å¸ƒï¼ˆGaussian / Normalï¼‰

$$
\mathcal N(\mu, \sigma^2)
$$

- å‡å€¼ï¼š
$$\mu = 0\$$
- æ–¹å·®ï¼š
$$\sigma^2 = \frac{2}{d_{\text{in}} + d_{\text{out}}}\$$

è¿™æ˜¯ä¸€ç§ **Xavier / Glorot åˆå§‹åŒ–çš„å˜ä½“**ã€‚

---

#### â‘¡ ä¸ºä»€ä¹ˆæ–¹å·®æ˜¯ 
$sigma^2 = \frac{2}{d_{\text{in}} + d_{\text{out}}}$

- $d_{\text{in}}$ï¼šè¾“å…¥ç»´åº¦  
- $d_{\text{out}}$ï¼šè¾“å‡ºç»´åº¦  

ç›´è§‰ç†è§£ï¼š

- æ¯ä¸€å±‚çš„è¾“å‡ºæ˜¯å¾ˆå¤šæƒé‡ Ã— è¾“å…¥çš„**ç´¯åŠ **
- å¦‚æœæƒé‡å¤ªå¤§ â†’ æ¿€æ´»å€¼ / æ¢¯åº¦çˆ†ç‚¸
- å¦‚æœæƒé‡å¤ªå° â†’ æ¿€æ´»å€¼ / æ¢¯åº¦æ¶ˆå¤±
- è¿™ä¸ªæ–¹å·®è®©å‰å‘å’Œåå‘ä¼ æ’­çš„å°ºåº¦ä¿æŒç¨³å®š

ä¸€å¥è¯æ€»ç»“ï¼š

> **å±‚è¶Šå®½ï¼Œæ¯ä¸ªæƒé‡å°±åº”è¯¥è¶Šå°**

---

#### â‘¢ truncated at $[-3\sigma, 3\sigma]$ æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

è¿™æ˜¯ **æˆªæ–­æ­£æ€åˆ†å¸ƒï¼ˆTruncated Normalï¼‰**ï¼š

- å…ˆæŒ‰æ­£æ€åˆ†å¸ƒé‡‡æ ·
- å¦‚æœå€¼è¶…å‡º \([-3\sigma, 3\sigma]\)ï¼Œå°±ä¸¢å¼ƒå¹¶é‡æ–°é‡‡æ ·

åŸå› ï¼š

- é«˜æ–¯åˆ†å¸ƒæ˜¯æ— ç•Œçš„ï¼Œæå°æ¦‚ç‡ä¼šé‡‡åˆ°ç‰¹åˆ«å¤§çš„å€¼
- Transformer å¯¹è¿™ç§ outlier éå¸¸æ•æ„Ÿ
- Â±3Ïƒ è¦†ç›–çº¦ **99.7%** çš„æ¦‚ç‡è´¨é‡
- å‡ ä¹ä¸æ”¹å˜åˆ†å¸ƒå½¢çŠ¶ï¼Œä½†æ˜¾è‘—æé«˜ç¨³å®šæ€§

---


### Embedding åˆå§‹åŒ–
$$
E \sim \mathcal{N}(0, 1), \quad E \in [-3, 3]
$$
Embedding çš„æœ¬è´¨æ˜¯ï¼š

- **æŸ¥è¡¨ï¼ˆlookupï¼‰**
- ä¸åšçŸ©é˜µä¹˜æ³•
- ä¸å‘ç”Ÿå¤§è§„æ¨¡ç´¯åŠ 

å› æ­¤ï¼š

- ä¸å­˜åœ¨è¾“å…¥ç»´åº¦å¯¼è‡´çš„æ•°å€¼æ”¾å¤§é—®é¢˜
- ä½¿ç”¨å•ä½æ–¹å·®$\sigma^2 = 1$å°±è¶³å¤Ÿ
- åŒæ ·ç”¨æˆªæ–­é˜²æ­¢æç«¯å€¼
### RMSNorm åˆå§‹åŒ–

$$
g_i = 1
$$

| æ¨¡å—        | åˆå§‹åŒ–æ–¹å¼                               |
| --------- | ----------------------------------- |
| Linear    | `torch.nn.init.trunc_normal_`       |
| Embedding | `torch.nn.init.trunc_normal_`       |
| RMSNorm   | `nn.Parameter(torch.ones(d_model))` |

### 3.4.2 Linear æ¨¡å—

åŸæ–‡ç¿»è¯‘

Linear å±‚æ˜¯ Transformer å’Œç¥ç»ç½‘ç»œä¸­æœ€åŸºæœ¬çš„æ„ä»¶ä¹‹ä¸€ã€‚

ä½ éœ€è¦å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ Linear ç±»ï¼ˆç»§æ‰¿ torch.nn.Moduleï¼‰ï¼Œå®Œæˆå¦‚ä¸‹çº¿æ€§å˜æ¢ï¼š

çº¿æ€§å˜æ¢å®šä¹‰ä¸ºï¼š
$$
\mathbf{y} = \mathbf{W} \mathbf{x}
$$
å…¶ä¸­ï¼š
$$
\mathbf{W} \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}
$$
æ³¨æ„ï¼š
ä¸åŒ…å« bias é¡¹ï¼ˆè¿™æ˜¯ç°ä»£å¤§æ¨¡å‹çš„å¸¸è§åšæ³•ï¼‰ã€‚


###ä½œä¸šé¡»çŸ¥

1. é¦–å…ˆç»§æ‰¿class
ä¸åªæ˜¯
```python 
class Linear(nn.Module)
è€Œä¸”ä¸èƒ½å¿˜äº† 
super().__inti__()
```
2. åˆ›å»ºç©ºçš„torch ä¸ç”¨ones ä¸ç”¨zeros
```python
ç”¨
torch.empty(x,y,device=device,dtype=dtype)
```

3. åˆ›å»ºé«˜æ–¯åˆ†å¸ƒ

è¿™ä¸ªtrunc_normalçš„æ“ä½œæ—¶in placeçš„
```python
torch.nn.init.trunc_normal_(
    tensor=W,
    mean=mean,
    std=std,
    a=-3*std,
    b=3*std
)
```

4. pytrochè‡ªåŠ¨è®°å½•å‚æ•° éœ€è¦ç”¨nn.Parameter
```python
self.W = nn.Parameter(W)
```

5. adapter
```python
embedding.load_state_dict({"weights":weights})
```
å‰é¢çš„weights æ˜¯class ä¸­çš„instance attribute
åé¢çš„weightsæ˜¯è¯»å–çš„å€¼

## 3.5 Pre-Norm Transformer Blockï¼ˆé¢„å½’ä¸€åŒ– Transformer å—ï¼‰

æ¯ä¸ª Transformer block åŒ…å«ä¸¤ä¸ªå­å±‚ï¼ˆsub-layerï¼‰ï¼š
å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰  
é€ä½ç½®å‰é¦ˆç½‘ç»œï¼ˆPosition-wise Feed-Forward Networkï¼‰  
åœ¨æœ€åˆçš„ Transformerï¼ˆVaswani et al., 2017ï¼‰ä¸­ï¼Œæ¯ä¸ªå­å±‚çš„ç»“æ„æ˜¯ï¼š  

å­å±‚ â†’ æ®‹å·®è¿æ¥ â†’ LayerNorm

è¿™ç§ç»“æ„è¢«ç§°ä¸º Post-Norm Transformerï¼ˆåå½’ä¸€åŒ–ï¼‰ï¼Œå› ä¸º LayerNorm ä½œç”¨åœ¨å­å±‚è¾“å‡ºä¸Šã€‚

åç»­ç ”ç©¶å‘ç°ï¼Œå¦‚æœå°† LayerNorm ä»å­å±‚è¾“å‡ºç§»åŠ¨åˆ°å­å±‚è¾“å…¥ï¼Œå¹¶åœ¨æœ€åä¸€ä¸ª Transformer block ä¹‹åå†åŠ ä¸€æ¬¡å½’ä¸€åŒ–ï¼Œå¯ä»¥æ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ï¼ˆNguyen & Salazar, 2019ï¼›Xiong et al., 2020ï¼‰ã€‚è¿™ç§ç»“æ„è¢«ç§°ä¸º Pre-Norm Transformerï¼ˆé¢„å½’ä¸€åŒ–ï¼‰ã€‚

åœ¨ Pre-Norm ç»“æ„ä¸­ï¼š

æ¯ä¸ªå­å±‚ å…ˆåšå½’ä¸€åŒ–  
å†è¿›å…¥å­å±‚è®¡ç®—  
æœ€åé€šè¿‡ æ®‹å·®è¿æ¥ï¼ˆresidual connectionï¼‰ ä¸è¾“å…¥ç›¸åŠ 

ç›´è§‚ç†è§£æ˜¯ï¼š

ä»è¾“å…¥ embedding åˆ°æœ€ç»ˆè¾“å‡ºï¼Œæœ‰ä¸€æ¡**â€œå¹²å‡€çš„æ®‹å·®æµï¼ˆresidual streamï¼‰â€**ï¼Œä¸­é—´æ²¡æœ‰è¢«å½’ä¸€åŒ–æ‰“æ–­ï¼Œè¿™æœ‰åŠ©äº æ¢¯åº¦ä¼ æ’­æ›´ç¨³å®šã€‚  
ç›®å‰å‡ ä¹æ‰€æœ‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-3ã€LLaMAã€PaLMï¼‰éƒ½é‡‡ç”¨ Pre-Norm Transformerï¼Œå› æ­¤æœ¬ä½œä¸šä¹Ÿå®ç°è¿™ä¸€ç‰ˆæœ¬ã€‚

Pre-Norm Transformer çš„æ•°å­¦å½¢å¼

è®¾è¾“å…¥ä¸º
$$
ğ‘¥âˆˆğ‘…^{ğ‘‘_{model}} 
$$
ä¸€ä¸ª Transformer block çš„ä¸¤å±‚å¯å†™ä¸ºï¼š
ï¼ˆ1ï¼‰è‡ªæ³¨æ„åŠ›å­å±‚
$$
z = x + \mathrm{MHA}(\mathrm{Norm}(x))
$$
ï¼ˆ2ï¼‰å‰é¦ˆç½‘ç»œå­å±‚

$$
y = z + \mathrm{FFN}(\mathrm{Norm}(z))
$$
å…¶ä¸­ï¼š
- Norm æ˜¯ RMSNormï¼ˆæœ¬ä½œä¸šä½¿ç”¨ï¼‰
- MHA æ˜¯å¤šå¤´è‡ªæ³¨æ„åŠ›
- FFN æ˜¯é€ä½ç½®å‰é¦ˆç½‘ç»œ
- +æ˜¯æ®‹å·®è¿æ¥

åŸå§‹ Transformer ä½¿ç”¨çš„æ˜¯ Layer Normalizationï¼ˆLayerNormï¼‰ã€‚
æœ¬ä½œä¸šä¸­ï¼Œæˆ‘ä»¬æŒ‰ç…§ Touvron et al. (2023) çš„åšæ³•ï¼Œæ”¹ç”¨ RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰ã€‚

ç»™å®šä¸€ä¸ªæ¿€æ´»å‘é‡ï¼š
$$
\mathbf{a} = (a_1, a_2, \dots, a_{d_{\text{model}}})
\in \mathbb{R}^{d_{\text{model}}}
$$

RMSNorm will rescale each activation ai as follows

$$
\mathrm{RMSNorm}(a_i)
=
\frac{a_i}{\mathrm{RMS}(\mathbf{a})}
\cdot g_i
$$
where RMS(a)
$$
\mathrm{RMS}(\mathbf{a})
=
\sqrt{
\frac{1}{d_{\text{model}}}
\sum_{i=1}^{d_{\text{model}}} a_i^2
+ \varepsilon
}
$$

giæ˜¯ å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼ˆgainï¼‰ 

ä¸€å…±æœ‰ $ğ‘‘_{model}ä¸ª ğ‘”_{i}$

Îµ æ˜¯æ•°å€¼ç¨³å®šé¡¹ï¼Œé€šå¸¸è®¾ä¸º 1leâˆ’5

#### ä½œä¸šå®æµ‹ linear + embedding + RMSNorm
1. è¿™é‡Œè¦nn parameterï¼ˆtorch ones/zeros/emptyï¼‰å¥½åƒåŒºåˆ«éƒ½ä¸å¤§  å› ä¸ºéƒ½ä¼š 
```python
rmsnorm.load_state_dict({"weights":weights})
```

2. åœ¨tensorçš„è®¡ç®—è¿‡ç¨‹ä¸­ è®°å¾— sum æˆ–è€…meançš„ dimension
3. math.sqrt ä¸èƒ½å¤„ç†tensor çš„sqrt 
4. keep dimsension æ‰èƒ½broadcast
5. è¿™é‡Œè½¬æ¢typeçš„æ—¶å€™ å¯ä»¥åˆ©ç”¨ 

```python
'''
è¿™é‡Œ è®°å¾—è®°ä½in_dtype
ç„¶åä½¿ç”¨x.to(torch.float32)è¿™ç§ 
'''
in_dtype = x.dtype
x = x.to(torch.float32)
# Your code here performing RMSNorm
...
result = ...
# Return the result in the original dtype
return result.to(in_dtype)
```
6. å¦‚ä½•ä½¿ç”¨einops 
```python
from einops import einsum
from einops import reduce
```
7. einsumæ„Ÿå— 
åœ¨æ ‡æ³¨çš„åŒæ—¶ å®Œæˆäº† rearrange å¹¶ä¸”åŠ ä¸ŠçŸ©é˜µä¹˜æ³•  
```python
#/home/dexterding/projects/assignment1-basics/cs336_basics/transformer_modules/linear_module.py
forward(self,x:torch.Tensor)->torch.Tensor:
        return x@self.W.T
        
        return einsum(x,self.W,'batch sequence in_f, out_f in_f -> batch sequence out_f')
```
è¿™é‡Œæ˜¯ä¸¤ç§ æˆ‘è§‰å¾—éƒ½å†™å¯ä»¥åŒä¿®

8. reduceæ„Ÿå—
ååˆ†æ¸…æ¥šçš„æ ‡æ³¨äº†è§£äº† å°‘æ‰çš„ æ˜¯å“ªä¸€ä¸ªdimensionä»¥åŠå…¶å«ä¹‰  
ä¸‰è€…ç­‰ä»·
```python
x.pow(2).mean(dim=-1,keepdim=True)

x.pow(2).sum(dim=-1,keepdim= True)/self.d_model

mean_square = reduce(x.pow(2), "batch sequence d_model -> batch sequence 1", "mean")
```

### 3.5.2 Position-Wise Feed-Forward Network
åŸæ–‡

åœ¨æœ€åˆçš„ Transformer è®ºæ–‡ï¼ˆVaswani ç­‰ï¼Œ2017ï¼Œç¬¬ 3.3 èŠ‚ï¼‰ä¸­ï¼Œ**å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰**ç”±ä¸¤å±‚çº¿æ€§å˜æ¢ç»„æˆï¼Œä¸­é—´ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ï¼ˆReLU(x) = max(0, x)ï¼‰ã€‚å…¶ä¸­ï¼Œä¸­é—´éšè—å±‚çš„ç»´åº¦é€šå¸¸æ˜¯è¾“å…¥ç»´åº¦çš„ 4 å€ã€‚

ç„¶è€Œï¼Œç°ä»£è¯­è¨€æ¨¡å‹ç›¸è¾ƒäºè¿™ä¸€åŸå§‹è®¾è®¡ï¼Œé€šå¸¸å¼•å…¥äº†`ä¸¤`ä¸ªä¸»è¦å˜åŒ–ï¼š  
ä½¿ç”¨ä¸åŒäº ReLU çš„æ¿€æ´»å‡½æ•°ï¼›

å¼•å…¥ä¸€ç§ é—¨æ§æœºåˆ¶ï¼ˆ`gating mechanism`ï¼‰ã€‚
å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ç§åä¸º `SwiGLU` çš„æ¿€æ´»å‡½æ•°ï¼Œè¯¥å‡½æ•°å·²è¢«è¯¸å¦‚ Llama 3 å’Œ Qwen 2.5 ç­‰å¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨ã€‚

`SwiGLU` å°† `SiLU`ï¼ˆåˆç§° Swishï¼‰æ¿€æ´»å‡½æ•° ä¸ä¸€ç§ç§°ä¸º `Gated Linear Unitï¼ˆGLUï¼‰` çš„é—¨æ§ç»“æ„ç»“åˆèµ·æ¥ã€‚

æ­¤å¤–ï¼Œéµå¾ªè‡ª PaLM ä¸ LLaMA ä»¥æ¥çš„å¤§å¤šæ•°ç°ä»£ LLM çš„å®è·µï¼Œæˆ‘ä»¬åœ¨è¿™äº›çº¿æ€§å±‚ä¸­ çœç•¥ bias é¡¹ã€‚

1ï¸âƒ£ SiLU / Swish æ¿€æ´»å‡½æ•°

SiLUï¼ˆæˆ– Swishï¼‰æ¿€æ´»å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
$$
\mathrm{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

å®ƒä¸ ReLU ç±»ä¼¼ï¼Œä½†åœ¨ 0 é™„è¿‘æ˜¯ å¹³æ»‘çš„ï¼ˆsmoothï¼‰ï¼Œè€Œä¸æ˜¯æœ‰æŠ˜ç‚¹ã€‚

2ï¸âƒ£ Gated Linear Unitï¼ˆGLUï¼‰  

GLU æœ€æ—©ç”± Dauphin ç­‰äººåœ¨ 2017 å¹´æå‡ºï¼Œå…¶å®šä¹‰ä¸ºï¼š  
ä¸€ä¸ªçº¿æ€§å˜æ¢ç»è¿‡ sigmoid åï¼Œä¸å¦ä¸€ä¸ªçº¿æ€§å˜æ¢è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
$$
\mathrm{GLU}(x, W_1, W_2)
= \sigma(W_1 x) \odot (W_2 x)
$$

GLU è¢«è®¤ä¸ºå¯ä»¥ï¼š  
é€šè¿‡æä¾›ä¸€æ¡â€œçº¿æ€§æ¢¯åº¦é€šè·¯â€ï¼Œåœ¨ä¿ç•™éçº¿æ€§èƒ½åŠ›çš„åŒæ—¶ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

3ï¸âƒ£ SwiGLUï¼šSiLU + GLU çš„ç»„åˆ

å°† SiLUï¼ˆSwishï¼‰å’Œ GLU ç»“åˆï¼Œå°±å¾—åˆ°äº† SwiGLUï¼Œæˆ‘ä»¬å°†åœ¨ Transformer çš„å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨å®ƒï¼š
$$
\mathrm{FFN}(x)
= \mathrm{SwiGLU}(x, W_1, W_2, W_3)
= W_2 \left( \mathrm{SiLU}(W_1 x) \odot (W_3 x) \right)
$$

ä¸€ä¸ªçº¿æ€§æŠ•å½±W1è´Ÿè´£ éçº¿æ€§å˜æ¢ï¼ˆSiLUï¼‰  
å¦ä¸€ä¸ªçº¿æ€§æŠ•å½±W3è´Ÿè´£ é—¨æ§

ä¸¤è€…é€å…ƒç´ ç›¸ä¹˜åï¼Œå†ç»è¿‡è¾“å‡ºçº¿æ€§å±‚W2

åœ¨æ ‡å‡†è®¾ç½®ä¸­ï¼Œå‰é¦ˆå±‚çš„éšè—ç»´åº¦æ»¡è¶³ï¼š
$$
x \in \mathbb{R}^{d_{\text{model}}}
$$

$$
W_1, W_3 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}
$$

$$
W_2 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}
$$

$$
d_{\text{ff}} = \frac{8}{3} d_{\text{model}}
$$


4ï¸âƒ£ ç»éªŒè§†è§’ï¼ˆEmpirical Perspectiveï¼‰

Shazeerï¼ˆ2020ï¼‰é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æå‡ºå°† SiLU/Swish ä¸ GLU ç»“åˆï¼Œå¹¶é€šè¿‡å®éªŒè¡¨æ˜ï¼š

SwiGLU åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ä¼˜äº ReLU

ä¹Ÿä¼˜äº ä¸å¸¦é—¨æ§çš„ SiLU

å°½ç®¡æœ‰ä¸€äº›å¯å‘å¼è§£é‡Šï¼Œä½†ä½œè€…æœ¬äººä¿æŒäº†éå¸¸å…‹åˆ¶ã€ç»éªŒä¸»ä¹‰çš„æ€åº¦ï¼Œå¹¶ç•™ä¸‹äº†ä¸€å¥éå¸¸è‘—åçš„è¯ï¼š

æˆ‘ä»¬å¹¶ä¸è§£é‡Šä¸ºä»€ä¹ˆè¿™äº›ç»“æ„æœ‰æ•ˆï¼›
æˆ‘ä»¬å°†å®ƒä»¬çš„æˆåŠŸï¼Œå’Œå…¶ä»–ä¸€åˆ‡ä¸€æ ·ï¼Œå½’å› äºç¥çš„ä»æ…ˆã€‚